{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparametertuning mittels RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from Splitting_Scaling_Function import Split_Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "data = Get_data(0,1800,0)\n",
    "X_train, X_test, Y_train, Y_test = Split_Scaling(data, size=0.2, Train_Test_Split=2, Datengröße=1800, random=42)\n",
    "\n",
    "# Anzahl an Bäume\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Anzahl an Features bei jedem Split\n",
    "max_features = ['log2', 'sqrt', 10, 0.5]\n",
    "# Maximale tiefe der Bäume\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum von Samples einen Baum zu splitten\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum um ein Blattknoten zu splitten\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Erzeuge das Random grid mit den festgelegten Hyperparametern\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# Lege den RFR fest\n",
    "rf_op=RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, 3 Folds pro Suche\n",
    "# Suche 50 Kombinationen und validiere jede 3 mal \n",
    "rf_random = RandomizedSearchCV(estimator = rf_op, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Modell fitten um Suche auszuführen\n",
    "rf_random.fit(X_train, Y_train)\n",
    "\n",
    "# Printe die besten Hyperparameter\n",
    "print(\"Beste Parameter:\", rf_random.best_params_)\n",
    "\n",
    "# Speichern der besten Parameter\n",
    "best_params = rf_random.best_params_\n",
    "Pfad = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\14_Modelle_Hyperparameter\\Konventionell'\n",
    "hyperparameters_pfad = os.path.join(Pfad, 'best_hyperparameters_RandomForest.json')\n",
    "with open(hyperparameters_pfad, 'w') as json_file:\n",
    "    json.dump(best_params, json_file)\n",
    "\n",
    "# Speichern des besten Modells\n",
    "model_pfad = os.path.join(Pfad, 'best_random_forest_model.pkl')\n",
    "joblib.dump(rf_random.best_estimator_, model_pfad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}\n",
      "MAE für die Trainingsdaten des best fits: [0. 0. 0.]\n",
      "MAE für die Testdaten des best fits: [0.35       1.25611607 1.5771875 ]\n",
      "R2-Score für die Trainingsdaten des best fits: [1. 1. 1.]\n",
      "R2-Score für die Testdaten des best fits: [0.32324344 0.57946661 0.39636909]\n"
     ]
    }
   ],
   "source": [
    "# Validieren des besten Modells\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Beste Parameter und bestes Modell anzeigen\n",
    "print(\"Beste Parameter:\", rf_random.best_params_)\n",
    "\n",
    "# Predicte die Variablen \n",
    "Y_pred_best_train = rf_random.predict(X_train)\n",
    "Y_pred_best_test = rf_random.predict(X_test)\n",
    "\n",
    "# MAEs berechnen für die predicteten Values mit den besten Hyperparameter\n",
    "MAE_best_train = mean_absolute_error(Y_train, Y_pred_best_train, multioutput='raw_values')\n",
    "MAE_best_test = mean_absolute_error(Y_test, Y_pred_best_test, multioutput='raw_values')\n",
    "\n",
    "# R2 Score\n",
    "r2__best_train = r2_score(Y_train, Y_pred_best_train, multioutput='raw_values')\n",
    "r2_best_test = r2_score(Y_test, Y_pred_best_test, multioutput='raw_values')\n",
    "\n",
    "#Print des MAEs\n",
    "print(f'MAE für die Trainingsdaten des best fits: {MAE_best_train}')\n",
    "print(f'MAE für die Testdaten des best fits: {MAE_best_test}')\n",
    "\n",
    "#Print des R2\n",
    "print(f'R2-Score für die Trainingsdaten des best fits: {r2__best_train}')\n",
    "print(f'R2-Score für die Testdaten des best fits: {r2_best_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Zeilen im finalen DataFrame: 255600\n",
      "None\n",
      "[41, 19, 83, 75, 70, 48, 39, 152, 35, 121, 21, 20, 36, 68]\n",
      "25200\n",
      "25200\n",
      "<class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Beste Parameter: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan', 'algorithm': 'kd_tree'}\n"
     ]
    }
   ],
   "source": [
    "# Kneighbor Hyperparametertuning mittels RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from Splitting_Scaling_Function import Split_Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "# Reinladen der Daten\n",
    "data = Get_data(0,1800)\n",
    "X_train, X_test, Y_train, Y_test = Split_Scaling(data, size=0.2, Train_Test_Split=2, Datengröße=1800, random=42, Validation_Data=0)\n",
    "\n",
    "# Definition des Hyperparameterraums\n",
    "random_grid = {'n_neighbors': [3, 5, 7,9,11,13,15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'metric' :['minkowski','euclidean','manhattan']\n",
    "    }\n",
    "\n",
    "#Initialisere KNR\n",
    "KNR = KNeighborsRegressor()\n",
    "\n",
    "# Definieren Sie die Scoring-Funktion\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Durchführung der Hyperparametersuche\n",
    "KNR_opt = RandomizedSearchCV(estimator=KNR, param_distributions= random_grid, n_iter=50, cv=3, scoring=mae_scorer, verbose=2, n_jobs=-1)\n",
    "KNR_opt.fit(X_train, Y_train)\n",
    "\n",
    "# Beste Parameter und bestes Modell anzeigen\n",
    "print(\"Beste Parameter:\", KNR_opt.best_params_)\n",
    "\n",
    "# Speichern der besten Parameter\n",
    "best_params = KNR_opt.best_params_\n",
    "Pfad = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\14_Modelle_Hyperparameter\\Konventionell'\n",
    "hyperparameters_pfad = os.path.join(Pfad, 'best_hyperparameters_KNR_30Trials.json')\n",
    "with open(hyperparameters_pfad, 'w') as json_file:\n",
    "    json.dump(best_params, json_file)\n",
    "\n",
    "# Speichern des besten Modells\n",
    "model_pfad = os.path.join(Pfad, 'best_KNR_model.pkl')\n",
    "joblib.dump(KNR_opt.best_params_, model_pfad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE für die Trainingsdaten des best fits: [0. 0. 0.]\n",
      "MAE für die Testdaten des best fits: [0.28571429 0.66160549 1.85424797]\n",
      "R2-Score für die Trainingsdaten des best fits: [1. 1. 1.]\n",
      "R2-Score für die Testdaten des best fits: [ 0.54098361  0.88411528 -0.25882954]\n"
     ]
    }
   ],
   "source": [
    "# Validieren des besten Modells des KNRs\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Predicte die Values\n",
    "Y_pred_best_train = KNR_opt.predict(X_train)\n",
    "Y_pred_best_test = KNR_opt.predict(X_test)\n",
    "\n",
    "# MAE der Vorhersagen\n",
    "MAE_best_train = mean_absolute_error(Y_train, Y_pred_best_train, multioutput='raw_values')\n",
    "MAE_best_test = mean_absolute_error(Y_test, Y_pred_best_test, multioutput='raw_values')\n",
    "\n",
    "# R2 Score\n",
    "r2__best_train = r2_score(Y_train, Y_pred_best_train, multioutput='raw_values')\n",
    "r2_best_test = r2_score(Y_test, Y_pred_best_test, multioutput='raw_values')\n",
    "\n",
    "#Printe MAEs\n",
    "print(f'MAE für die Trainingsdaten des best fits: {MAE_best_train}')\n",
    "print(f'MAE für die Testdaten des best fits: {MAE_best_test}')\n",
    "\n",
    "# Printe R2 Score\n",
    "print(f'R2-Score für die Trainingsdaten des best fits: {r2__best_train}')\n",
    "print(f'R2-Score für die Testdaten des best fits: {r2_best_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor Hyperparametersuche\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from Splitting_Scaling_Function import Split_Scaling\n",
    "\n",
    "#Reinladen der Daten\n",
    "data = Get_data(0,1800,0)\n",
    "X_train, X_test, Y_train, Y_test = Split_Scaling(data, size=0.2, Train_Test_Split=2, Datengröße=1800, random=42)\n",
    "\n",
    "# Definition des Modells\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Definition des Parameterraums\n",
    "param_grid = {\n",
    "    'criterion': ['absolute_error', 'friedman_mse', 'squared_error'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Durchführung der Rastersuche mit MAE als Bewertungsmetrik\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Beste Parameter und bestes Modell anzeigen\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "\n",
    "Y_pred_best_train = KNR_opt.predict(X_train)\n",
    "Y_pred_best_test = KNR_opt.predict(X_test)\n",
    "\n",
    "MAE_best_train = mean_absolute_error(Y_train, Y_pred_best_train, multioutput='raw_values')\n",
    "MAE_best_test = mean_absolute_error(Y_test, Y_pred_best_test, multioutput='raw_values')\n",
    "\n",
    "r2__best_train = r2_score(Y_train, Y_pred_best_train, multioutput='raw_values')\n",
    "r2_best_test = r2_score(Y_test, Y_pred_best_test, multioutput='raw_values')\n",
    "\n",
    "print(f'MAE für die Trainingsdaten des best fits: {MAE_best_train}')\n",
    "print(f'MAE für die Testdaten des best fits: {MAE_best_test}')\n",
    "\n",
    "print(f'R2-Score für die Trainingsdaten des best fits: {r2__best_train}')\n",
    "print(f'R2-Score für die Testdaten des best fits: {r2_best_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR Hyperparametertuning mittels RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from Splitting_Scaling_Function import Split_Scaling\n",
    "\n",
    "#Reinladen der Daten \n",
    "data = Get_data(0,1800)\n",
    "X_train, X_test, Y_train, Y_test = Split_Scaling(data, size=0.2, Train_Test_Split=2, Datengröße=1800, random=42)\n",
    "\n",
    "# Cross validation grid search (beste Parameters) \n",
    "c_range = np.logspace(-0, 4, 8)\n",
    "gamma_range = np.logspace(-4, 0, 8)\n",
    "epsilon = [0.1,0.2,0.3]\n",
    "kernel= ['rbf','linear','sigmoid']\n",
    "\n",
    "# Erzeuge das Random Grid\n",
    "random_grid = {'C': c_range,\n",
    "               'gamma': gamma_range,\n",
    "               'kernel': kernel,\n",
    "               'epsilon': epsilon\n",
    "               }\n",
    "\n",
    "print(random_grid)\n",
    "print(Y_train['Y_Opt-Y_ist'])\n",
    "\n",
    "# Initalisiere Modelle\n",
    "svr = SVR()\n",
    "\n",
    "# SVR muss als MultiOutputRegressor definiert werden\n",
    "SVR_op = MultiOutputRegressor(svr)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "SVR_random = RandomizedSearchCV(svr , param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42)\n",
    "\n",
    "# Fit the random search model\n",
    "SVR_random.fit(X_train, Y_train)\n",
    "\n",
    "# Validiere das Modell mit den Testdaten und dem entsprechenden MAE und R2 Score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "print(SVR_random.best_params_)\n",
    "\n",
    "Y_pred_best_train_SVR = SVR_random.predict(X_train)\n",
    "Y_pred_best_test_SVR = SVR_random.predict(X_test)\n",
    "\n",
    "MAE_best_train = mean_absolute_error(Y_train['X_opt-X-Ist'], Y_pred_best_train_SVR, multioutput='raw_values')\n",
    "MAE_best_test = mean_absolute_error(Y_test['X_opt-X-Ist'], Y_pred_best_test_SVR, multioutput='raw_values')\n",
    "\n",
    "r2__best_train = r2_score(Y_train['Y_Opt-Y_ist'], Y_pred_best_train_SVR, multioutput='raw_values')\n",
    "r2_best_test = r2_score(Y_test['Y_Opt-Y_ist'], Y_pred_best_test_SVR, multioutput='raw_values')\n",
    "\n",
    "print(f'MAE für die Trainingsdaten des best fits: {MAE_best_train}')\n",
    "print(f'MAE für die Testdaten des best fits: {MAE_best_test}')\n",
    "\n",
    "print(f'R2-Score für die Trainingsdaten des best fits: {r2__best_train}')\n",
    "print(f'R2-Score für die Testdaten des best fits: {r2_best_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
