{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_1 --> Existiert bereits\n",
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_2 --> Existiert bereits\n",
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_3 --> Existiert bereits\n",
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_4 --> Existiert bereits\n",
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_5 --> Existiert bereits\n",
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_6 --> Existiert bereits\n",
      "C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\15_Plots\\Plots_KurzesBlech_7 --> Existiert bereits\n"
     ]
    }
   ],
   "source": [
    "# Zum Speichern der Plots des kurzen Bleches aus Rohdaten, zentrierten Daten und stationären Bereich\n",
    "from PreProcessing_SingleBlech_Function_KurzesBlech import Preprocessing\n",
    "\n",
    "for i in range (1,8):\n",
    "    Data_stat, Data_zentriert, BlechNummer = Preprocessing(i,0,1400,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinladen der Daten für das beste CNN Modell im nächsten Code Abschnitt\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "\n",
    "# Hier werden die interpolierten Daten reingeladen\n",
    "df_Int, Interpoliertes_df = Get_data(0,1800,1,2)\n",
    "\n",
    "# Skalierung und Fensterung der Daten\n",
    "from Fensterung_Scaling_DeepLearning import Fensterung_Scale\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels, X_test_scaled_int, Y_test_interpolation, Blechnummern_Test_Int = Fensterung_Scale(df_Int, interpoliertesdf=Interpoliertes_df, Validation_data=1, random=1, Train_Test_Split=2, window_size=10, Interpolation=1, size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 15ms/step - Verstellweg_Phi_mae: 0.3729 - Verstellweg_X_mae: 0.0240 - Verstellweg_Y_mae: 0.3259 - loss: 1.3836 - val_Verstellweg_Phi_mae: 0.2428 - val_Verstellweg_X_mae: 0.0084 - val_Verstellweg_Y_mae: 0.2906 - val_loss: 0.7375\n",
      "Epoch 2/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 16ms/step - Verstellweg_Phi_mae: 0.1912 - Verstellweg_X_mae: 0.0049 - Verstellweg_Y_mae: 0.1690 - loss: 0.5505 - val_Verstellweg_Phi_mae: 0.2290 - val_Verstellweg_X_mae: 0.0087 - val_Verstellweg_Y_mae: 0.2900 - val_loss: 0.6922\n",
      "Epoch 3/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 16ms/step - Verstellweg_Phi_mae: 0.1738 - Verstellweg_X_mae: 0.0045 - Verstellweg_Y_mae: 0.1549 - loss: 0.4933 - val_Verstellweg_Phi_mae: 0.2530 - val_Verstellweg_X_mae: 0.0159 - val_Verstellweg_Y_mae: 0.2975 - val_loss: 0.7165\n",
      "Epoch 4/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 15ms/step - Verstellweg_Phi_mae: 0.1633 - Verstellweg_X_mae: 0.0042 - Verstellweg_Y_mae: 0.1485 - loss: 0.4630 - val_Verstellweg_Phi_mae: 0.1517 - val_Verstellweg_X_mae: 0.0211 - val_Verstellweg_Y_mae: 0.3560 - val_loss: 0.6692\n",
      "Epoch 5/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 18ms/step - Verstellweg_Phi_mae: 0.1576 - Verstellweg_X_mae: 0.0041 - Verstellweg_Y_mae: 0.1462 - loss: 0.4461 - val_Verstellweg_Phi_mae: 0.2305 - val_Verstellweg_X_mae: 0.0099 - val_Verstellweg_Y_mae: 0.2747 - val_loss: 0.6485\n",
      "Epoch 6/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - Verstellweg_Phi_mae: 0.1545 - Verstellweg_X_mae: 0.0041 - Verstellweg_Y_mae: 0.1445 - loss: 0.4351 - val_Verstellweg_Phi_mae: 0.1525 - val_Verstellweg_X_mae: 0.0040 - val_Verstellweg_Y_mae: 0.3357 - val_loss: 0.6201\n",
      "Epoch 7/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 12ms/step - Verstellweg_Phi_mae: 0.1504 - Verstellweg_X_mae: 0.0040 - Verstellweg_Y_mae: 0.1425 - loss: 0.4237 - val_Verstellweg_Phi_mae: 0.2117 - val_Verstellweg_X_mae: 0.0059 - val_Verstellweg_Y_mae: 0.3309 - val_loss: 0.6718\n",
      "Epoch 8/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 12ms/step - Verstellweg_Phi_mae: 0.1491 - Verstellweg_X_mae: 0.0043 - Verstellweg_Y_mae: 0.1419 - loss: 0.4178 - val_Verstellweg_Phi_mae: 0.1548 - val_Verstellweg_X_mae: 0.0032 - val_Verstellweg_Y_mae: 0.3264 - val_loss: 0.6036\n",
      "Epoch 9/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 12ms/step - Verstellweg_Phi_mae: 0.1489 - Verstellweg_X_mae: 0.0043 - Verstellweg_Y_mae: 0.1383 - loss: 0.4106 - val_Verstellweg_Phi_mae: 0.1464 - val_Verstellweg_X_mae: 0.0077 - val_Verstellweg_Y_mae: 0.3221 - val_loss: 0.5928\n",
      "Epoch 10/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 12ms/step - Verstellweg_Phi_mae: 0.1479 - Verstellweg_X_mae: 0.0042 - Verstellweg_Y_mae: 0.1363 - loss: 0.4047 - val_Verstellweg_Phi_mae: 0.1515 - val_Verstellweg_X_mae: 0.0101 - val_Verstellweg_Y_mae: 0.3148 - val_loss: 0.5906\n",
      "Epoch 11/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 12ms/step - Verstellweg_Phi_mae: 0.1417 - Verstellweg_X_mae: 0.0042 - Verstellweg_Y_mae: 0.1350 - loss: 0.3967 - val_Verstellweg_Phi_mae: 0.1685 - val_Verstellweg_X_mae: 0.0037 - val_Verstellweg_Y_mae: 0.3344 - val_loss: 0.6213\n",
      "Epoch 12/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 13ms/step - Verstellweg_Phi_mae: 0.1393 - Verstellweg_X_mae: 0.0042 - Verstellweg_Y_mae: 0.1342 - loss: 0.3916 - val_Verstellweg_Phi_mae: 0.1834 - val_Verstellweg_X_mae: 0.0077 - val_Verstellweg_Y_mae: 0.3428 - val_loss: 0.6465\n",
      "Epoch 13/30\n",
      "\u001b[1m19746/19746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 13ms/step - Verstellweg_Phi_mae: 0.1384 - Verstellweg_X_mae: 0.0043 - Verstellweg_Y_mae: 0.1357 - loss: 0.3901 - val_Verstellweg_Phi_mae: 0.1644 - val_Verstellweg_X_mae: 0.0055 - val_Verstellweg_Y_mae: 0.3169 - val_loss: 0.5962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1119/1119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step\n",
      "Mean Absolute Error for Verstellweg_X_ alleDaten: 0.009507456149995222\n",
      "Mean Absolute Error for Verstellweg_Y_alleDaten: 0.26674271942657424\n",
      "Mean Absolute Error for Verstellweg_Phi_alleDaten: 0.13997678431677085\n"
     ]
    }
   ],
   "source": [
    "# Modellbildung bestes CNN, hier kann das Modell mit den vorher definierten Zufallsvariablen und Größen gespeichert werden\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from Load_Data_for_Modelling_Function import Data_for_Model\n",
    "from Splitting_Scaling_Function import Split_Scaling\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import gc\n",
    "\n",
    "Y_train = np.squeeze(Y_train)\n",
    "Y_test = np.squeeze(Y_test)\n",
    "Y_val =np.squeeze(Y_val)\n",
    "Y_train_scaled = np.squeeze(Y_train_scaled)\n",
    "Y_val_scaled = np.squeeze(Y_val_scaled)\n",
    "\n",
    "# TensorFlow-Sitzung zurücksetzen\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# CNN Modell definieren\n",
    "input_layer = layers.Input(shape=(10,11))\n",
    "\n",
    "\n",
    "#Hyperparametersuche 2 60 Trials\n",
    "conv_1 = layers.Conv1D(filters=160, kernel_size=2, activation='relu', padding='same', kernel_regularizer=keras.regularizers.l2(0.01))(input_layer)\n",
    "pool_1 = layers.MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = layers.Conv1D(filters=480, kernel_size=4, activation='relu', padding='same', strides=1, kernel_regularizer=keras.regularizers.l2(0.01))(pool_1)\n",
    "pool_2 = layers.MaxPooling1D(pool_size=2)(conv_2)\n",
    "    \n",
    "    \n",
    "flatten = layers.Flatten()(pool_2)\n",
    "\n",
    "#Hyperparametersuche 3\n",
    "dense_layer = layers.Dense(64, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(flatten)\n",
    "\n",
    "# Output Layers definieren\n",
    "X_output = layers.Dense(1, activation='linear', name='Verstellweg_X')(dense_layer)\n",
    "Y_output = layers.Dense(1, activation='linear', name='Verstellweg_Y')(dense_layer)\n",
    "Phi_output = layers.Dense(1, activation='linear', name='Verstellweg_Phi')(dense_layer)\n",
    "\n",
    "# Liste erstellen für alle Outputs\n",
    "outputs = [X_output, Y_output, Phi_output]\n",
    "\n",
    "# Modell definieren \n",
    "model = keras.Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "# Hyperparameter 3 60 Trials\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.0003255639325303961), \n",
    "            loss=['mean_absolute_error', 'mean_absolute_error', 'mean_absolute_error'], \n",
    "            metrics={'Verstellweg_X': 'mae', 'Verstellweg_Y': 'mae', 'Verstellweg_Phi': 'mae'})\n",
    "\n",
    "# Modell zusammenfassen\n",
    "#model.summary()\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "CNN = model.fit(X_train_scaled, [Y_train[:,0], Y_train[:,1], Y_train[:,2]],\n",
    "                        epochs=30,\n",
    "                        validation_data=(X_val_scaled, [Y_val[:, 0], Y_val[:, 1], Y_val[:, 2]]),\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "# Speichern des Modells\n",
    "Pfad = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\14_Modelle_Hyperparameter'\n",
    "model_pfad = os.path.join(Pfad, 'Best_Modell_CNN_Interpolationsfaktor2_Random1.h5')\n",
    "model.save(model_pfad)\n",
    "\n",
    "# Vorhersagen für den Testdatensatz erstellen\n",
    "predictions1 = model.predict(X_test_scaled)\n",
    "X_p, Y_p, Phi_p = predictions1\n",
    "\n",
    "# predictions_int = model.predict(X_test_scaled_int)\n",
    "# X_int, Y_int, Phi_int = predictions_int\n",
    "\n",
    " # MAE für den Testdatensatz berechnen\n",
    "mae_X = mean_absolute_error(Y_test[:,0], X_p)\n",
    "mae_Y = mean_absolute_error(Y_test[:,1], Y_p)\n",
    "mae_Phi = mean_absolute_error(Y_test[:,2], Phi_p)\n",
    "\n",
    "print(f\"Mean Absolute Error for Verstellweg_X_ alleDaten: {mae_X}\")\n",
    "print(f\"Mean Absolute Error for Verstellweg_Y_alleDaten: {mae_Y}\")\n",
    "print(f\"Mean Absolute Error for Verstellweg_Phi_alleDaten: {mae_Phi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Zeilen im finalen DataFrame: 4200\n",
      "None\n",
      "      1-OW-OS Lateral Force  1-OW-OS Axial Force  2-OW-RS Lateral Force  \\\n",
      "0               2311.959854            16.310887            6104.752182   \n",
      "1               2250.925952             0.542466            6030.260740   \n",
      "2               2314.176390             2.909735            6033.506382   \n",
      "3               2299.452258            11.415856            6035.168784   \n",
      "4               2307.051810            22.690478            6033.427220   \n",
      "...                     ...                  ...                    ...   \n",
      "4195            1889.701038            23.787647            5314.785876   \n",
      "4196            1853.919814             7.417377            5357.850004   \n",
      "4197            1855.898864             9.824770            5291.749734   \n",
      "4198            1852.019926            23.346292            5243.856724   \n",
      "4199            1867.931488            31.611673            5332.043192   \n",
      "\n",
      "      2-OW-RS Axial Force  3-UW-RS Lateral Force  3-UW-RS Axial Force  \\\n",
      "0             -762.084989            3455.075362          1002.922010   \n",
      "1             -760.551625            3422.856428          1032.983863   \n",
      "2             -760.849779            3445.259274          1017.928009   \n",
      "3             -750.244010            3379.317328          1021.567338   \n",
      "4             -750.627351            3427.447824          1019.772601   \n",
      "...                   ...                    ...                  ...   \n",
      "4195          -694.693419            3419.415256           953.351858   \n",
      "4196          -683.832090            3387.671294           964.967798   \n",
      "4197          -680.850548            3467.862400           983.962103   \n",
      "4198          -690.604448            3492.323458           973.193678   \n",
      "4199          -680.339427            3456.621396           974.888708   \n",
      "\n",
      "      4-UW-OS Lateral Force  4-UW-OS Axial Force  X-Ist  Y-Ist  phi-Ist  \\\n",
      "0              -2315.845521           -43.698350   10.0   -3.0      0.0   \n",
      "1              -2284.497369            13.735106   10.0   -3.0      0.0   \n",
      "2              -2330.332167            18.695682   10.0   -3.0      0.0   \n",
      "3              -2343.314735            14.190205   10.0   -3.0      0.0   \n",
      "4              -2286.634743            -0.646013   10.0   -3.0      0.0   \n",
      "...                     ...                  ...    ...    ...      ...   \n",
      "4195           -2214.180535           116.577400   10.0   -8.0      0.0   \n",
      "4196           -2198.585621           149.708586   10.0   -8.0      0.0   \n",
      "4197           -2225.579863           154.032023   10.0   -8.0      0.0   \n",
      "4198           -2207.689251           132.187286   10.0   -8.0      0.0   \n",
      "4199           -2200.960481           154.259573   10.0   -8.0      0.0   \n",
      "\n",
      "      X_opt-X-Ist  Y_Opt-Y_ist  phi_Opt-phi_ist  \n",
      "0             0.0          0.0              0.0  \n",
      "1             0.0          0.0              0.0  \n",
      "2             0.0          0.0              0.0  \n",
      "3             0.0          0.0              0.0  \n",
      "4             0.0          0.0              0.0  \n",
      "...           ...          ...              ...  \n",
      "4195          0.0          5.0              0.0  \n",
      "4196          0.0          5.0              0.0  \n",
      "4197          0.0          5.0              0.0  \n",
      "4198          0.0          5.0              0.0  \n",
      "4199          0.0          5.0              0.0  \n",
      "\n",
      "[4200 rows x 14 columns]\n",
      "[7, 11, 10, 46, 21, 94, 85, 39, 32, 77]\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Anzahl an Bleche in der Gesamtmatrix 3\n",
      "(42, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (4128, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (42, 1, 3)\n",
      "(42, 10, 11)\n",
      "(42, 10, 11)\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from Load_Data_for_Modelling_Function_KurzesBlech import Data_for_Model\n",
    "from Fensterung_Scaling_DeepLearning import Fensterung_Scale\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Bestes CNN Modell (siehe Code Abschnitt zuvor) laden\n",
    "Pfad = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\14_Modelle_Hyperparameter\\CNN'\n",
    "model_pfad = os.path.join(Pfad, 'Best_Modell_CNN_Interpolationsfaktor2_Random1.h5')\n",
    "model = load_model(model_pfad)\n",
    "\n",
    "# Reinladen der Daten des kurzen Bleches\n",
    "data = Data_for_Model(0,1400)\n",
    "print(data)\n",
    "# Generieren einer Liste von 10 eindeutigen zufälligen Ganzzahlen zwischen 0 und 100\n",
    "random.seed(2)\n",
    "Random_numbers = random.sample(range(101), 10)\n",
    "print(Random_numbers)\n",
    "\n",
    "# Leere Liste für die Fehler\n",
    "Liste_MAEs_Blechsplit = []\n",
    "Liste_Fehler_Blechsplit = []\n",
    "\n",
    "# 10 fache Validierung für das kurze Blech (Erfolgt hierbei egal nach welchem Split)\n",
    "for i in Random_numbers:\n",
    "    # Skaliere und Fenster die Daten entsprechend des gewünschten Zufallsfaktors\n",
    "    X_train_scaled, X_test_scaled, Y_train_scaled, Y_test_scaled, Y_train, Y_test, scalers_features, scaler_labels, Blechnummern_Test = Fensterung_Scale(data, Validation_data=0, random=i, Train_Test_Split=1, window_size=10, size=0.01, Datengröße=1400)\n",
    "\n",
    "    print(X_test_scaled.shape)\n",
    "    print(X_test_scaled.shape)\n",
    "\n",
    "    # Squeeze damit kein Fehler beim Vergleich der Daten auftritt\n",
    "    Y_train = np.squeeze(Y_train)\n",
    "    Y_test = np.squeeze(Y_test)\n",
    "\n",
    "    # Vorhersagen für den Testdatensatz erstellen\n",
    "    # Hier wird mit Train die Daten predicted weil die Size sehr klein gewählt wird beim SPlitten sodass möglichst alle Daten genutzt werden und die Testdaten eben sehr klein sind \n",
    "    # Somit werden über 99% der Daten für den Test des kurzen Bleches hier verwendet\n",
    "    predictions1 = model.predict(X_train_scaled)\n",
    "    X_p, Y_p, Phi_p = predictions1\n",
    "\n",
    "    # predictions_int = model.predict(X_test_scaled_int)\n",
    "    # X_int, Y_int, Phi_int = predictions_int\n",
    "\n",
    "    # MAE für den Testdatensatz berechnen\n",
    "    mae_X = mean_absolute_error(Y_train[:,0], X_p)\n",
    "    mae_Y = mean_absolute_error(Y_train[:,1], Y_p)\n",
    "    mae_Phi = mean_absolute_error(Y_train[:,2], Phi_p)\n",
    "\n",
    "    # print(f\"Mean Absolute Error for Verstellweg_X_ alleDaten: {mae_X}\")\n",
    "    # print(f\"Mean Absolute Error for Verstellweg_Y_alleDaten: {mae_Y}\")\n",
    "    # print(f\"Mean Absolute Error for Verstellweg_Phi_alleDaten: {mae_Phi}\")\n",
    "    \n",
    "    # Hinzufügen der MAEs zum Dataframe\n",
    "    MAE_BlechSplit_df = pd.DataFrame([{'CV':i, 'Error' : 'MAE', 'X': mae_X, 'Y': mae_Y, 'Phi': mae_Phi}])\n",
    "    Liste_MAEs_Blechsplit.append(MAE_BlechSplit_df)\n",
    "    \n",
    "    # Berechnen der einzelnen Fehler aus Label und Prädiktion\n",
    "    Fehler_X = Y_train[:,0]-X_p[:,0]\n",
    "    Fehler_Y = Y_train[:,1]-Y_p[:,0]\n",
    "    Fehler_Phi = Y_train[:,2]-Phi_p[:,0]\n",
    "    \n",
    "    # Fehler in einen DataFrame konvertieren\n",
    "    df_Fehler = pd.DataFrame({\n",
    "        'Label_X': Y_train[:,0],\n",
    "        'Label_Y': Y_train[:,1],\n",
    "        'Label_Phi': Y_train[:,2],\n",
    "        'Fehler_X': Fehler_X,\n",
    "        'Fehler_Y': Fehler_Y,\n",
    "        'Fehler_Phi': Fehler_Phi})\n",
    "    \n",
    "    # Appende die Splitmethode und den Durchlauf zur Fehlerliste\n",
    "    df_Fehler.insert(loc=0, column='SplitMethode', value='Blechsplit')\n",
    "    df_Fehler.insert(loc=1, column='CV', value=i)\n",
    "    Liste_Fehler_Blechsplit.append(df_Fehler)\n",
    "\n",
    "# Füge die Listen zusammen\n",
    "MAE_Blechsplit = pd.concat(Liste_MAEs_Blechsplit, ignore_index=True)\n",
    "Fehler_Blechsplit_df = pd.concat(Liste_Fehler_Blechsplit, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_27164\\1360701161.py:15: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  MAE_BlechSplit_comp[Column] = MAE_BlechSplit_comp[Column].astype(str).str.replace('.', ',')\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_27164\\1360701161.py:18: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  Fehler_Blechsplit_df[Column] = Fehler_Blechsplit_df[Column].astype(str).str.replace('.', ',')\n"
     ]
    }
   ],
   "source": [
    "# Hier können die zuvor erstellten Listen gespeichert werden in einer Excel\n",
    "\n",
    "# Sicherungsordner hier angeben\n",
    "Ordner = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\13_ExcelvonDaten_Code\\Evaluation_kurzesBlech'\n",
    "\n",
    "# print(Fehler_Blechsplit_df)\n",
    "# print(MAE_Blechsplit)\n",
    "# Berechnung der Standardabweichung und des Mittelwertes\n",
    "Mean = MAE_Blechsplit[['X','Y','Phi']].mean() \n",
    "Std = MAE_Blechsplit[['X','Y','Phi']].std()\n",
    "\n",
    "# Zusammenfügen der Mittelwerte und Std\n",
    "MAE_BlechSplit = pd.concat([MAE_Blechsplit, pd.DataFrame([{'CV': 'Mittelwert', 'Error' : 'MAE', 'X': Mean[0], 'Y': Mean[1], 'Phi': Mean[2]}])], ignore_index=True)\n",
    "MAE_BlechSplit_comp = pd.concat([MAE_BlechSplit, pd.DataFrame([{'CV': 'Standardabweichung', 'Error' : 'MAE', 'X': Std[0], 'Y': Std[1], 'Phi': Std[2]}])], ignore_index=True)\n",
    "\n",
    "# print(MAE_BlechSplit)\n",
    "# print(MAE_StandardSplit)\n",
    "\n",
    "# Umwandeln von Flaot in String für die richtige Darstellung in der CSV\n",
    "for Column in MAE_BlechSplit_comp.columns:\n",
    "         MAE_BlechSplit_comp[Column] = MAE_BlechSplit_comp[Column].astype(str).str.replace('.', ',')\n",
    "        \n",
    "for Column in Fehler_Blechsplit_df:\n",
    "        Fehler_Blechsplit_df[Column] = Fehler_Blechsplit_df[Column].astype(str).str.replace('.', ',')\n",
    "\n",
    "# Speichern der Fehlerliste und der MAEs im entsprechenden Ordner\n",
    "Fehler_Blechsplit_df.to_csv(f'{Ordner}\\Fehler_CNNmitIntfaktor2_neuesBlech.csv', index=True, sep=';')\n",
    "\n",
    "MAE_BlechSplit_comp.to_csv(f'{Ordner}\\MAE_CNNmitIntfaktor2_neuesBlech.csv', index=True, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
