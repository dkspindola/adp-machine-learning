{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Load_Data_for_Modelling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reinladen der Daten aus den Excel Dateien inklusive Daten Preprocessing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLoad_Data_for_Modelling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Get_data\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_csv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Data = Get_data('assets/data.csv', 0,1800,0,1)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Load_Data_for_Modelling'"
     ]
    }
   ],
   "source": [
    "# Reinladen der Daten aus den Excel Dateien inklusive Daten Preprocessing\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from pandas import read_csv\n",
    "\n",
    "#Data = Get_data('assets/data.csv', 0,1800,0,1)\n",
    "Data = read_csv('assets/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "254180\n",
      "142.0\n",
      "Richtige Blechnummern umgerechnet der Validierungsdaten [108, 52, 128, 26, 32, 38, 119, 28, 158, 68, 23, 36, 138, 134]\n",
      "Richtige Blechnummern umgerechnet der Testdaten [22, 44, 25, 84, 160, 21, 132, 98, 29, 42, 106, 144, 100, 149]\n",
      "Blechnummern für Validierungsdaten, abgezählt vom Array nicht die Originaldaten [83, 39, 102, 13, 19, 25, 94, 15, 130, 55, 10, 23, 112, 108]\n",
      "Blechnummern für Testdaten, abgezählt vom Array nicht die Originaldaten[9, 31, 12, 71, 132, 8, 106, 73, 16, 29, 81, 117, 75, 122]\n",
      "25060\n",
      "Shape nach dem Random Sampling des Arrays von X_val: (25060, 10, 11)\n",
      "Shape nach dem Random Sampling des Arrays von X_train: (204060, 10, 11)\n",
      "Shape nach dem Random Sampling des Arrays von Y_train: (204060, 1, 3)\n",
      "(25060, 1, 3)\n",
      "Shape für die Features der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (204060, 10, 11)\n",
      "Shape für die Features der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (25060, 10, 11) \n",
      "Shape für die Labels der gesamten Trainingsdaten, also im Falle einer Interpolation mit allen Daten (204060, 1, 3)\n",
      "Shape für die Labels der gesamten Testdaten, also im Falle einer Interpolation mit allen Daten (25060, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Hier werden die Daten mit der Fensterung_Scale Funktion für die Hyperparametersuche vorbereitet\n",
    "# Fensterung durchführen, Split der Daten und Skalierung der Features\n",
    "from Fensterung_Scaling_DeepLearning import Fensterung_Scale\n",
    "\n",
    "# Hier muss auf die richtige Anzahl geachtet werden: Bei Interpolation gibt es zwei zusätzliche Ausgaben aus der Datei\n",
    "# Für genaue Anzahl siehe die Datei Fensterung_Scaling_DeepLearning, je nachdem was returned wird\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels, Angepasste_Blechnummern_test = Fensterung_Scale(Data, Validation_data=1, random=7, Train_Test_Split=2, window_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bevor die Daten für die Hyperparametersuche verwendet werden müssen die Labels \"gesqueezed\" werden für ein eindimensionales Array\n",
    "import numpy as np\n",
    "Y_train = np.squeeze(Y_train)\n",
    "Y_test = np.squeeze(Y_test)\n",
    "Y_val =np.squeeze(Y_val)\n",
    "Y_train_scaled = np.squeeze(Y_train_scaled)\n",
    "Y_val_scaled = np.squeeze(Y_val_scaled)\n",
    "#Y_test_interpolation =np.squeeze(Y_test_interpolation)\n",
    "# Y_train_int = np.squeeze(Y_train_int)\n",
    "# Y_test_int = np.squeeze(Y_test_int)\n",
    "# Y_val_int =np.squeeze(Y_val_int)\n",
    "# Y_train_scaled_int = np.squeeze(Y_train_scaled_int)\n",
    "# Y_val_scaled_int = np.squeeze(Y_val_scaled_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 Complete [04h 54m 00s]\n",
      "val_loss: 0.7724713087081909\n",
      "\n",
      "Best val_loss So Far: 0.51645827293396\n",
      "Total elapsed time: 1d 03h 44m 26s\n",
      "\n",
      "Search: Running Trial #15\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |3                 |num_layers_conv\n",
      "0.0015337         |3.6478e-05        |learning_rate\n",
      "288               |480               |units_conv0\n",
      "tanh              |tanh              |activation_conv0\n",
      "5                 |2                 |kernel_0\n",
      "0                 |0                 |l2_conv0\n",
      "5                 |1                 |num_layers_fully\n",
      "32                |160               |units_conv1\n",
      "tanh              |relu              |activation_conv1\n",
      "5                 |5                 |kernel_1\n",
      "0.002             |0                 |l2_conv1\n",
      "448               |512               |units_conv2\n",
      "relu              |relu              |activation_conv2\n",
      "5                 |5                 |kernel_2\n",
      "0.004             |0.005             |l2_conv2\n",
      "96                |96                |units_conv3\n",
      "relu              |tanh              |activation_conv3\n",
      "2                 |3                 |kernel_3\n",
      "0.006             |0.004             |l2_conv3\n",
      "256               |160               |units_dense0\n",
      "tanh              |tanh              |activation_dense0\n",
      "0.008             |0.007             |l2_dense0\n",
      "512               |512               |units_dense1\n",
      "tanh              |relu              |activation_dense1\n",
      "0.004             |0.003             |l2_dense1\n",
      "160               |96                |units_dense2\n",
      "tanh              |relu              |activation_dense2\n",
      "0.008             |0.008             |l2_dense2\n",
      "64                |None              |units_dense3\n",
      "tanh              |None              |activation_dense3\n",
      "0.007             |None              |l2_dense3\n",
      "160               |None              |units_conv4\n",
      "tanh              |None              |activation_conv4\n",
      "5                 |None              |kernel_4\n",
      "0.007             |None              |l2_conv4\n",
      "\n",
      "Anzahl an Conv Layers: 3\n",
      "Kernel Size 0 ist: 5\n",
      "Kernel Size 1 ist: 5\n",
      "Kernel Size 2 ist: 5\n",
      "Anzahl an Fully Connected Layers: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,128</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">46,112</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">72,128</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,080</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Verstellweg_X       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Verstellweg_Y       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Verstellweg_Phi     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m11\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m288\u001b[0m)   │     \u001b[38;5;34m16,128\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │     \u001b[38;5;34m46,112\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │     \u001b[38;5;34m72,128\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m114,944\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │     \u001b[38;5;34m82,080\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m10,304\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Verstellweg_X       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Verstellweg_Y       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Verstellweg_Phi     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">473,475</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m473,475\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">473,475</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m473,475\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m17565/17565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2202s\u001b[0m 125ms/step - Verstellweg_Phi_mae: 0.4490 - Verstellweg_X_mae: 0.0597 - Verstellweg_Y_mae: 0.4481 - loss: 1.4415 - val_Verstellweg_Phi_mae: 0.3747 - val_Verstellweg_X_mae: 0.0616 - val_Verstellweg_Y_mae: 0.5040 - val_loss: 1.1584\n",
      "Epoch 2/30\n",
      "\u001b[1m17565/17565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - Verstellweg_Phi_mae: 0.1992 - Verstellweg_X_mae: 0.0288 - Verstellweg_Y_mae: 0.2259 - loss: 0.6550"
     ]
    }
   ],
   "source": [
    "# Bayesian Hyperparametersuche \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from Load_Data_for_Modelling_Function import Data_for_Model\n",
    "from Splitting_Scaling_Function import Split_Scaling\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Liste leeren, um Suche zu beschleunigen\n",
    "results = list()\n",
    "\n",
    "# Vorherige Tuner löschen um Leistung freizugeben\n",
    "tuner_directory = 'my_dir'\n",
    "if os.path.exists(tuner_directory):\n",
    "    shutil.rmtree(tuner_directory)\n",
    "\n",
    "# Keras Modell definieren\n",
    "def build_model(hp):\n",
    "    \n",
    "    # CInputs Layer definieren (10er Window Size, 11 Features), wenn Window Size angepasst wird, hier auch anpassen\n",
    "    input_layer = layers.Input(shape=(10,11))\n",
    "    \n",
    "    # Hyperparameter für die Anzahl an Conv Layer\n",
    "    num_layers_conv = hp.Int('num_layers_conv', 0, 6)\n",
    "    print(f'Anzahl an Conv Layers: {num_layers_conv}')\n",
    "    \n",
    "    # Hyperparameter für die Lernrate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
    "    \n",
    "    # Übergebe den Input Layer als x für die folgende for Schleife\n",
    "    x = input_layer\n",
    "    \n",
    "    # Iteriere über die gewählte Anzahl an Conv Schichten\n",
    "    for i in range(num_layers_conv):\n",
    "        # Definition weiterer Hyperparameter innerhalb der Schleife für jede Schicht\n",
    "        units_conv = hp.Int(f'units_conv{i}', min_value=32, max_value=512, step=32)\n",
    "        activation_layer = hp.Choice(f'activation_conv{i}', values =['relu','tanh'])\n",
    "        kernel_size = hp.Choice(f'kernel_{i}', values = [2,3,4,5])\n",
    "        l2_regulizer =hp.Float(f'l2_conv{i}', min_value=0.0, max_value=0.01, step=0.001)\n",
    "        print(f'Kernel Size {i} ist: {kernel_size}')\n",
    "        \n",
    "        # Nur die letzten 3 Pooling Schichten dürfen eine Größe größer 1 haben, damit bei der Window Size 10 und einer größeren Anzhal an Conv Layer kein Fehler auftritt\n",
    "        if i >= num_layers_conv - 3:  # Letzte drei Pooling-Schichten\n",
    "            pool_size = 2\n",
    "        else:\n",
    "            pool_size = 1\n",
    "\n",
    "        # Conv Schicht mit Parametern\n",
    "        x = layers.Conv1D(filters=units_conv, kernel_size=kernel_size, activation=activation_layer, padding='same', kernel_regularizer=keras.regularizers.l2(l2_regulizer))(x)\n",
    "        #Max Pooling mit Parametern\n",
    "        x = layers.MaxPooling1D(pool_size=pool_size)(x)\n",
    "        \n",
    "    # Flatten Schicht für Fully Connected Layer\n",
    "    flatten = layers.Flatten()(x)\n",
    "\n",
    "    # Fully Connected Part (MLP) / Dense Schichten\n",
    "    \n",
    "    # Definition der Anzahl an Dense Layers\n",
    "    num_layers_fully = hp.Int('num_layers_fully', 0,6)\n",
    "    print(f'Anzahl an Fully Connected Layers: {num_layers_fully}')\n",
    "\n",
    "    # Übergabe der Flatten Schicht\n",
    "    y = flatten\n",
    "    \n",
    "    for i in range(num_layers_fully):\n",
    "        # Weitere Hyperparameter\n",
    "        units_dense = hp.Int(f'units_dense{i}', min_value=32, max_value=512, step=32)\n",
    "        activation_layer_dense = hp.Choice(f'activation_dense{i}', values =['relu','tanh'])\n",
    "        l2_dense_x = hp.Float(f'l2_dense{i}', min_value=0.0, max_value=0.01, step=0.001)\n",
    "        \n",
    "        #Dense Schicht \n",
    "        y= layers.Dense(units_dense, activation=activation_layer_dense, kernel_regularizer=keras.regularizers.l2(l2_dense_x) )(y)\n",
    "        \n",
    "\n",
    "    # Output Layers definieren\n",
    "    X_output = layers.Dense(1, activation='linear', name='Verstellweg_X')(y)\n",
    "    Y_output = layers.Dense(1, activation='linear', name='Verstellweg_Y')(y)\n",
    "    Phi_output = layers.Dense(1, activation='linear', name='Verstellweg_Phi')(y)\n",
    "\n",
    "    # Liste erstellen für alle Outputs\n",
    "    outputs = [X_output, Y_output, Phi_output]\n",
    "\n",
    "    # Modell definieren \n",
    "    model = keras.Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "    # Kompilieren des Modells\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate), \n",
    "                loss=['mean_absolute_error', 'mean_absolute_error', 'mean_absolute_error'], \n",
    "                metrics={'Verstellweg_X': 'mae', 'Verstellweg_Y': 'mae', 'Verstellweg_Phi': 'mae'})\n",
    "\n",
    "    # Modell zusammenfassen\n",
    "    model.summary()\n",
    "    \n",
    "    return model \n",
    "\n",
    "# Keras Tuner initialisieren\n",
    "tuner = kt.BayesianOptimization(build_model,\n",
    "                                objective='val_loss',\n",
    "                                max_trials=30,\n",
    "                                executions_per_trial=1,\n",
    "                                directory=tuner_directory,\n",
    "                                project_name='CNN_Hyperparametertuning_BayesianOptimization')\n",
    "\n",
    "# Custom Callbacks: Falls Val Loss drei mal in Folge über 2.5 ist wird zur nächsten Hyperparameterkombination gesprungen\n",
    "class EarlyStopOnHighValLoss(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold, patience=3):\n",
    "        super(EarlyStopOnHighValLoss, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss > self.threshold:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "        else:\n",
    "            self.wait = 0\n",
    "\n",
    "\n",
    "# 2. Callback: Falls der Val Loss 3 mal in Folge keine Verbesserung zeigt, wird die aktuelle Suche unterbrochen und die nächsten Parameter ausgewählt\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Aufrufen des ersten Callbacks, der zuvor definiert wurde: Grenzwert und Anzahl an Epochen in FOlge können hier festgelegt werden\n",
    "early_stop_on_high_val_loss = EarlyStopOnHighValLoss(threshold=2.5, patience=3)  \n",
    "\n",
    "# Hyperparametersuche durchführen mit Validationsdaten validieren\n",
    "tuner.search(X_train_scaled, [Y_train[:, 0], Y_train[:, 1], Y_train[:, 2]],\n",
    "             epochs=30,\n",
    "             validation_data=(X_val_scaled, [Y_val[:, 0], Y_val[:, 1], Y_val[:, 2]]),\n",
    "             callbacks=[early_stopping,early_stop_on_high_val_loss])\n",
    "\n",
    "# Optimale Hyperparameter zurückgeben lassen\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Speichere Hyperparameter in einer JSON Datei\n",
    "best_hyperparameters = best_hps.values\n",
    "Pfad = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\14_Modelle_Hyperparameter'\n",
    "hyperparameters_pfad = os.path.join(Pfad, 'best_hyperparameters_CNN_Bayesian_30Trials_Interpolation.json')\n",
    "with open(hyperparameters_pfad, 'w') as json_file:\n",
    "    json.dump(best_hyperparameters, json_file)\n",
    "\n",
    "# Printe alle Hyperparameters\n",
    "print(\"All available hyperparameters:\")\n",
    "print(best_hps.values)\n",
    "\n",
    "# Printe die Anzhal an Layers\n",
    "print(f\"Best number of convolutional layers: {best_hps.get('num_layers_conv')}\")\n",
    "print(f\"Best number of fully connected layers: {best_hps.get('num_layers_fully')}\")\n",
    "\n",
    "# Printe die Hyperparameter in den einzelnen Schichten hier CONV\n",
    "for i in range(best_hps.get('num_layers_conv')):\n",
    "    print(f\"Best units_conv{i}: {best_hps.get(f'units_conv{i}')}\")\n",
    "    print(f\"Best activation_conv{i}: {best_hps.get(f'activation_conv{i}')}\")\n",
    "    print(f\"Best kernel_{i} size: {best_hps.get(f'kernel_{i}')}\")\n",
    "    \n",
    "# Hier Fully\n",
    "for i in range(best_hps.get('num_layers_fully')):\n",
    "    print(f\"Best units_dense{i}: {best_hps.get(f'units_dense{i}')}\")\n",
    "    print(f\"Best activation_dense{i}: {best_hps.get(f'activation_dense{i}')}\")\n",
    "\n",
    "# Modell mit den besten Hyperparametern aufbauen trainieren und testen\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train_scaled, [Y_train[:, 0], Y_train[:, 1], Y_train[:, 2]],\n",
    "                         epochs=50,\n",
    "                         validation_data=(X_test_scaled,[Y_test[:, 0], Y_test[:, 1], Y_test[:, 2]]),\n",
    "                         callbacks=[early_stopping])\n",
    "\n",
    "# Speichern des Modells\n",
    "model_pfad = os.path.join(Pfad, 'best_model_CNN_60Trials_Interpolation.h5')\n",
    "best_model.save(model_pfad)\n",
    "\n",
    "# Plotte die losses in der Trainings- und Testkurve\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['val_Verstellweg_X_mae'], label='Validation Loss - Verstellweg_X')\n",
    "plt.plot(history.history['val_Verstellweg_Y_mae'], label='Validation Loss - Verstellweg_Y')\n",
    "plt.plot(history.history['val_Verstellweg_Phi_mae'], label='Validation Loss - Verstellweg_Phi')\n",
    "plt.title('Validation Loss for Each Output')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_20828\\3526222180.py:11: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "# Aktuell bestes Modell aufbauen \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Aufrufen des besten Modells um Validationen durchzuführen (Standard Split, Blech Split, mit RandomSeed2 10 mal)\n",
    "def bestes_model(X_train_scaled, X_val_scaled, X_test_scaled, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels):\n",
    "    \n",
    "    # Umwandeln der Labels in eindimensionalen Array\n",
    "    Y_train = np.squeeze(Y_train)\n",
    "    Y_test = np.squeeze(Y_test)\n",
    "    Y_val =np.squeeze(Y_val)\n",
    "    Y_train_scaled = np.squeeze(Y_train_scaled)\n",
    "    Y_val_scaled = np.squeeze(Y_val_scaled)\n",
    "    \n",
    "    # TensorFlow-Sitzung zurücksetzen\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # CNN Modell definieren\n",
    "    input_layer = layers.Input(shape=(10,11))\n",
    "\n",
    "    #Struktur bestes Modell\n",
    "    conv_1 = layers.Conv1D(filters=160, kernel_size=2, activation='relu', padding='same', kernel_regularizer=keras.regularizers.l2(0.01))(input_layer)\n",
    "    pool_1 = layers.MaxPooling1D(pool_size=2)(conv_1)\n",
    "    conv_2 = layers.Conv1D(filters=480, kernel_size=4, activation='relu', padding='same', strides=1, kernel_regularizer=keras.regularizers.l2(0.01))(pool_1)\n",
    "    pool_2 = layers.MaxPooling1D(pool_size=2)(conv_2)\n",
    "\n",
    "    # Flatten Schicht\n",
    "    flatten = layers.Flatten()(pool_2)\n",
    "  \n",
    "    #FUlly Connected Schicht\n",
    "    dense_layer = layers.Dense(64, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(flatten)\n",
    "\n",
    "    # Output Layers definieren\n",
    "    X_output = layers.Dense(1, activation='linear', name='Verstellweg_X')(dense_layer)\n",
    "    Y_output = layers.Dense(1, activation='linear', name='Verstellweg_Y')(dense_layer)\n",
    "    Phi_output = layers.Dense(1, activation='linear', name='Verstellweg_Phi')(dense_layer)\n",
    "\n",
    "    # Liste erstellen für alle Outputs\n",
    "    outputs = [X_output, Y_output, Phi_output]\n",
    "\n",
    "    # Modell definieren \n",
    "    model = keras.Model(inputs=input_layer, outputs=outputs)\n",
    "    \n",
    "    # Hyperparameter 3 60 Trials bestes Modell\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.0003255639325303961), \n",
    "                loss=['mean_absolute_error', 'mean_absolute_error', 'mean_absolute_error'], \n",
    "                metrics={'Verstellweg_X': 'mae', 'Verstellweg_Y': 'mae', 'Verstellweg_Phi': 'mae'})\n",
    "    \n",
    "    # Modell zusammenfassen\n",
    "    #model.summary()\n",
    "\n",
    "    # Hier nur ein Callback um nach drei mal nicht verbessern des Val Loss das Trainieren zu beenden, um Overfitting zu vermeiden\n",
    "    # Zweiter Callback wird nicht benötigt, weil die val_losses ja niedrig sind\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    CNN = model.fit(X_train_scaled, [Y_train[:,0], Y_train[:,1], Y_train[:,2]],\n",
    "                            epochs=20,\n",
    "                            validation_data=(X_val_scaled, [Y_val[:, 0], Y_val[:, 1], Y_val[:, 2]]),\n",
    "                            callbacks=[early_stopping])\n",
    "    \n",
    "    # Vorhersagen für den Testdatensatz erstellen\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    X_p, Y_p, Phi_p = predictions\n",
    "    \n",
    "    # Bestimme die Fehler jeder einzelnen Vorhersage\n",
    "    Fehler_X = Y_test[:,0]-X_p[:,0]\n",
    "    Fehler_Y = Y_test[:,1]-Y_p[:,0]\n",
    "    Fehler_Phi = Y_test[:,2]-Phi_p[:,0]\n",
    "    \n",
    "    # print(Fehler_X.shape)\n",
    "    # print(Fehler_X)\n",
    "      \n",
    "    # Fehler in einen DataFrame konvertieren, für spätere Dichteverteilungen, Labels für tiefere Analysen\n",
    "    df_Fehler = pd.DataFrame({\n",
    "        'Label_X': Y_test[:,0],\n",
    "        'Label_Y': Y_test[:,1],\n",
    "        'Label_Phi': Y_test[:,2],\n",
    "        'Fehler_X': Fehler_X,\n",
    "        'Fehler_Y': Fehler_Y,\n",
    "        'Fehler_Phi': Fehler_Phi})\n",
    "    \n",
    "    # MAEs berechnen\n",
    "    mae_X = mean_absolute_error(Y_test[:, 0], X_p)\n",
    "    mae_Y = mean_absolute_error(Y_test[:, 1], Y_p)\n",
    "    mae_Phi = mean_absolute_error(Y_test[:, 2], Phi_p)\n",
    "\n",
    "    # Folgendes war für den Test mit skalierten Labels, hat zu keiner Verbesserung geführt, deshalb hier nicht verwendet\n",
    "    # X und Y kombinieren da diese zusammen sakliert werden\n",
    "    # XY_p = np.column_stack((X_p, Y_p))\n",
    "\n",
    "    # Rückskalierung der Vorhersagen\n",
    "    # XY_pred = scaler_Y_mm.inverse_transform(XY_p)\n",
    "    # X_pred, Y_pred = XY_pred[:, 0], XY_pred[:, 1]\n",
    "    # Phi_pred = scaler_Y_phi.inverse_transform(Phi_p.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # MAE für den Testdatensatz berechnen\n",
    "    # mae_X = mean_absolute_error(Y_test[:,0], X_p)\n",
    "    # mae_Y = mean_absolute_error(Y_test[:,1], Y_p)\n",
    "    # mae_Phi = mean_absolute_error(Y_test[:,2], Phi_p)\n",
    "\n",
    "    # Printe die MAEs jeden Durchlaufes\n",
    "    print(f\"Mean Absolute Error for Verstellweg_X: {mae_X}\")\n",
    "    print(f\"Mean Absolute Error for Verstellweg_Y: {mae_Y}\")\n",
    "    print(f\"Mean Absolute Error for Verstellweg_Phi: {mae_Phi}\")\n",
    "    \n",
    "    # Returne MAEs und das Dataframe der Fehler\n",
    "    return mae_X, mae_Y, mae_Phi, df_Fehler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufrufen der oben definierten Funktion \n",
    "# Validierung des CNN-Modells ohne interpolierten Daten\n",
    "import random \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from Fensterung_Scaling_DeepLearning import Fensterung_Scale\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Random Seed für die 10 fache Validation definieren\n",
    "random.seed(2)\n",
    "# Generieren einer Liste von 10 eindeutigen zufälligen Ganzzahlen zwischen 0 und 100\n",
    "Random_numbers = random.sample(range(101), 10)\n",
    "print(Random_numbers)\n",
    "\n",
    "# Reinladen der vorverarbeiteten Daten ohne Interpolation\n",
    "data = Get_data(0,1800,0)\n",
    "\n",
    "# Leere Liste der Fehler zum appenden der 10er Validation\n",
    "Liste_Fehler_Blechsplit = []\n",
    "Liste_Fehler_Standardsplit= []\n",
    "\n",
    "# Leere Liste für die MAEs\n",
    "Liste_MAEs_Blechsplit =[]\n",
    "Liste_MAEs_Standardsplit = []\n",
    "\n",
    "# Leere Dataframes, falls nur eine der beiden Schleifen durchlaufen wird (für spätere Excel)\n",
    "MAE_StandardSplit_leer = pd.DataFrame(columns=['CV', 'Datentyp','Error', 'X', 'y', 'phi'])\n",
    "MAE_BlechSplit_leer = pd.DataFrame(columns=['CV', 'Datentyp','Error', 'X', 'y', 'phi'])\n",
    "\n",
    "# Schleife über den Standard Split\n",
    "for n in Random_numbers:\n",
    "    \n",
    "    # Skalierung und Fensterung der Daten\n",
    "    X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels, Angepasste_Blechnummern_test = Fensterung_Scale(data, Validation_data=1, random=n, Train_Test_Split =1, size=0.2)\n",
    "    # Berechnung der MAEs (aufrufen der obigen Funktion)\n",
    "    mae_X, mae_Y, mae_phi, df_Fehler = bestes_model(X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels)\n",
    "    \n",
    "    #Liste appenden\n",
    "    MAE_StandardSplit_df = pd.DataFrame([{'CV':n, 'Datentyp': 'Standardsplit', 'Error' : 'MAE', 'X': mae_X, 'y': mae_Y, 'phi': mae_phi}])\n",
    "    Liste_MAEs_Standardsplit.append(MAE_StandardSplit_df)\n",
    "    \n",
    "    #Fehler liste anpassen und appenden\n",
    "    df_Fehler.insert(loc=0, column='SplitMethode', value='Standardsplit')\n",
    "    df_Fehler.insert(loc=1, column='CV', value=n)\n",
    "    Liste_Fehler_Standardsplit.append(df_Fehler)\n",
    "\n",
    "# Nach 10 maligem Durchlaufen zusammenfügen der Listen\n",
    "MAE_Standardsplit = pd.concat(Liste_MAEs_Standardsplit, ignore_index=True)\n",
    "Fehler_Standardsplit_df = pd.concat(Liste_Fehler_Standardsplit, ignore_index=True)\n",
    "\n",
    "#Schleife für den Blech SPlit\n",
    "for n in Random_numbers:\n",
    "    \n",
    "    X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels, Angepasste_Blechnummern_test   = Fensterung_Scale(data, Validation_data=1, random=n, Train_Test_Split =2, size=0.2, window_size=25)\n",
    "    mae_X, mae_Y, mae_phi, df_Fehler = bestes_model(X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels)\n",
    "    \n",
    "    MAE_BlechSplit_df = pd.DataFrame([{'CV':n, 'Datentyp': 'Blechsplit', 'Error' : 'MAE', 'X': mae_X, 'y': mae_Y, 'phi': mae_phi}])\n",
    "    Liste_MAEs_Blechsplit.append(MAE_BlechSplit_df)\n",
    "    \n",
    "    df_Fehler.insert(loc=0, column='SplitMethode', value='Blechsplit')\n",
    "    df_Fehler.insert(loc=1, column='CV', value=n)\n",
    "    Liste_Fehler_Blechsplit.append(df_Fehler)\n",
    "    \n",
    "MAE_Blechsplit = pd.concat(Liste_MAEs_Blechsplit, ignore_index=True)\n",
    "Fehler_Blechsplit_df = pd.concat(Liste_Fehler_Blechsplit, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# Ausgabe der DataFrames\n",
    "# print(\"MAE Standard Split:\")\n",
    "# print(MAE_StandardSplit_df)\n",
    "print(\"\\nMAE Blech Split:\")\n",
    "print(MAE_Blechsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validierung des CNNs mit interpolierten Daten\n",
    "# Gleiche Vorgehensweise wie oben\n",
    "import random \n",
    "from Fensterung_Scaling_DeepLearning import Fensterung_Scale\n",
    "from Load_Data_for_Modelling import Get_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "random.seed(2)\n",
    "# Generieren einer Liste von 10 eindeutigen zufälligen Ganzzahlen zwischen 0 und 100\n",
    "Random_numbers = random.sample(range(101), 10)\n",
    "print(Random_numbers)\n",
    "\n",
    "Random_numbers = Random_numbers[6:]\n",
    "print(Random_numbers)\n",
    "\n",
    "#data = Data_for_Model(0,1800)\n",
    "df_Int, Interpoliertes_df = Get_data(0,1800,1,2)\n",
    "\n",
    "Liste_Fehler_Blechsplit = []\n",
    "Liste_Fehler_Standardsplit= []\n",
    "\n",
    "Liste_MAEs_Blechsplit =[]\n",
    "Liste_MAEs_Standardsplit = []\n",
    "\n",
    "MAE_StandardSplit_leer = pd.DataFrame(columns=['CV', 'Datentyp','Error', 'X', 'y', 'phi'])\n",
    "MAE_BlechSplit_leer = pd.DataFrame(columns=['CV', 'Datentyp','Error', 'X', 'y', 'phi'])\n",
    "\n",
    "for n in Random_numbers:\n",
    "    \n",
    "    X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels = Fensterung_Scale(df_Int, interpoliertesdf=Interpoliertes_df, Validation_data=1, random=n, Train_Test_Split =1, size=0.2, Interpolation=1)\n",
    "    mae_X, mae_Y, mae_phi, df_Fehler = bestes_model(X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels)\n",
    "    \n",
    "    MAE_StandardSplit_df = pd.DataFrame([{'CV':n, 'Datentyp': 'Standardsplit', 'Error' : 'MAE', 'X': mae_X, 'y': mae_Y, 'phi': mae_phi}])\n",
    "    Liste_MAEs_Standardsplit.append(MAE_StandardSplit_df)\n",
    "    \n",
    "    df_Fehler.insert(loc=0, column='SplitMethode', value='Standardsplit')\n",
    "    df_Fehler.insert(loc=1, column='CV', value=n)\n",
    "    Liste_Fehler_Standardsplit.append(df_Fehler)\n",
    "    \n",
    "MAE_Standardsplit = pd.concat(Liste_MAEs_Standardsplit, ignore_index=True)\n",
    "Fehler_Standardsplit_df = pd.concat(Liste_Fehler_Standardsplit, ignore_index=True)\n",
    "\n",
    "for n in Random_numbers:\n",
    "    \n",
    "    X_train, X_val, X_test, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test, scalers_features, scaler_labels, X_test_scaled_int, Y_test_interpolation, Blechnummern_Test_Int   = Fensterung_Scale(df_Int, interpoliertesdf=Interpoliertes_df, Validation_data=1, random=n, Train_Test_Split =2, size=0.2, Interpolation=1)\n",
    "    mae_X, mae_Y, mae_phi, df_Fehler = bestes_model(X_train, X_val, X_test_scaled_int, Y_train_scaled, Y_val_scaled, Y_test_scaled, Y_train, Y_val, Y_test_interpolation, scalers_features, scaler_labels)\n",
    "    \n",
    "    MAE_BlechSplit_df = pd.DataFrame([{'CV':n, 'Datentyp': 'Blechsplit', 'Error' : 'MAE', 'X': mae_X, 'y': mae_Y, 'phi': mae_phi}])\n",
    "    Liste_MAEs_Blechsplit.append(MAE_BlechSplit_df)\n",
    "    \n",
    "    df_Fehler.insert(loc=0, column='SplitMethode', value='Blechsplit')\n",
    "    df_Fehler.insert(loc=1, column='CV', value=n)\n",
    "    Liste_Fehler_Blechsplit.append(df_Fehler)\n",
    "    \n",
    "MAE_Blechsplit = pd.concat(Liste_MAEs_Blechsplit, ignore_index=True)\n",
    "Fehler_Blechsplit_df = pd.concat(Liste_Fehler_Blechsplit, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# Ausgabe der DataFrames\n",
    "# print(\"MAE Standard Split:\")\n",
    "# print(MAE_StandardSplit_df)\n",
    "print(\"\\nMAE Blech Split:\")\n",
    "print(MAE_Blechsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X    X\n",
      "0  0.005168  NaN\n",
      "1  0.004101  NaN\n",
      "2  0.004261  NaN\n",
      "3  0.005296  NaN\n",
      "4  0.004707  NaN\n",
      "5  0.000613  NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_5492\\2168783429.py:27: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  Errors_for_CSV[Column] = Errors_for_CSV[Column].astype(str).str.replace('.', ',')\n",
      "C:\\Users\\corvi\\AppData\\Local\\Temp\\ipykernel_5492\\2168783429.py:30: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  MAE_BlechSplit_comp[Column] = MAE_BlechSplit_comp[Column].astype(str).str.replace('.', ',')\n"
     ]
    }
   ],
   "source": [
    "Ordner = r'C:\\Users\\corvi\\OneDrive - stud.tu-darmstadt.de\\Desktop\\Masterthesis\\13_ExcelvonDaten_Code\\DeepLearning\\CNN'\n",
    "\n",
    "# Berechne Mittelwert und Std über die 10 Folds\n",
    "Mean_Standard = MAE_Standardsplit[['X','y','phi']].mean() \n",
    "Mean_Blech = MAE_Blechsplit[['X','y','phi']].mean() \n",
    "Std_Standard = MAE_Standardsplit[['X','y','phi']].std()\n",
    "Std_Blech = MAE_Blechsplit[['X','y','phi']].std()\n",
    "\n",
    "# Füge die Mittelwerte und Std den Dataframes hinzu\n",
    "MAE_StandardSplit_df = pd.concat([MAE_Standardsplit, pd.DataFrame([{'CV': 'Mittelwert', 'Datentyp': 'Standardsplit', 'Error' : 'MAE', 'X': Mean_Standard[0], 'y': Mean_Standard[1], 'phi': Mean_Standard[2]}])], ignore_index=True)\n",
    "MAE_BlechSplit_df = pd.concat([MAE_Blechsplit, pd.DataFrame([{'CV': 'Mittelwert', 'Datentyp': 'Blechsplit', 'Error' : 'MAE', 'X': Mean_Blech[0], 'y': Mean_Blech[1], 'phi': Mean_Blech[2]}])], ignore_index=True)\n",
    "MAE_StandardSplit_comp = pd.concat([MAE_StandardSplit_df, pd.DataFrame([{'CV': 'Standardabweichung', 'Datentyp': 'Standardsplit', 'Error' : 'MAE', 'X': Std_Standard[0], 'y': Std_Standard[1], 'phi': Std_Standard[2]}])], ignore_index=True)\n",
    "MAE_BlechSplit_comp = pd.concat([MAE_BlechSplit_df, pd.DataFrame([{'CV': 'Standardabweichung', 'Datentyp': 'Blechsplit', 'Error' : 'MAE', 'X': Std_Blech[0], 'y': Std_Blech[1], 'phi': Std_Blech[2]}])], ignore_index=True)\n",
    "\n",
    "# print(MAE_BlechSplit)\n",
    "# print(MAE_StandardSplit)\n",
    "\n",
    "Errors_for_CSV = pd.concat([MAE_StandardSplit_comp, MAE_BlechSplit_comp], axis=1)\n",
    "print(Errors_for_CSV['X'])\n",
    "\n",
    "# Columns die umgewandelt werden in String\n",
    "Errors_for_CSV.columns = ['CV', 'Datentyp', 'Error', 'X', 'y', 'phi', 'CV1',\n",
    "       'Datentyp1', 'Error1', 'X1', 'y1', 'phi1']\n",
    "\n",
    "# Umwandlung für die Speicherung in CSVs\n",
    "for Column in Errors_for_CSV.columns:\n",
    "        Errors_for_CSV[Column] = Errors_for_CSV[Column].astype(str).str.replace('.', ',')\n",
    "\n",
    "for Column in MAE_BlechSplit_comp.columns:\n",
    "         MAE_BlechSplit_comp[Column] = MAE_BlechSplit_comp[Column].astype(str).str.replace('.', ',')\n",
    "        \n",
    "for Column in Fehler_Blechsplit_df:\n",
    "        Fehler_Blechsplit_df[Column] = Fehler_Blechsplit_df[Column].astype(str).str.replace('.', ',')\n",
    "\n",
    "# Speichern der Fehlerliste in einer CSV\n",
    "Fehler_Blechsplit_df.to_csv(f'{Ordner}\\\\Fehler_CNN_Standardsplit_Interpolationsfaktor2.csv', index=True, sep=';')\n",
    "\n",
    "# Speichern der MAEs und Std in einer CSV\n",
    "Errors_for_CSV.to_csv(f'{Ordner}\\\\CNN_Standardsplit_Interpolationsfaktor2[6-10].csv', index=True, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
