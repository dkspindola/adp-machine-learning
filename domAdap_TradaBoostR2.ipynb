{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from IPython import display  # WICHTIG für Jupyter Live-Update\n",
    "from pandasgui import show\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Lambda, Layer, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "from adapt.instance_based import TrAdaBoostR2\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import importlib\n",
    "import src.execution\n",
    "from src.data import NPY\n",
    "importlib.reload(src.execution)\n",
    "from src.execution import CNNValidationExecution\n",
    "from src.execution import WindowSplittingExecution\n",
    "from src.model import CNN\n",
    "from src.util import timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domainadaptation mit TradaBoostR2 aus der Adapt Libary\n",
    "Erste Version, wenns läuft dann später in den Klassen die einzelnen Methoden ergänzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Trainingsdaten erzeugen**\n",
    "Da die Zuordnung der Datenmengen über die Anzahl der Versuche läuft, wird hier nur der Belchsplit durchgeführt, es wird eine bestimmte Anzahl an Blechen gewählt. \n",
    "\n",
    "- Trainingsdaten aus den Simulationsdaten werden wie gehabt erzeugt\n",
    "- Target Domain Daten werden aus den Raldaten erzeugt.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulationsdaten: Blechsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA: str = \"assets/synthetic-data.csv\"\n",
    "DATA: str = \"assets/data/sim-data.csv\"\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.2\n",
    "BATCH_SPLIT: bool = True # Blechsplit bei True \n",
    "BATCHSIZE: int = 326 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \".\" # Decimal separator of the csv file\n",
    "\n",
    "WindowSplittingExecution.execute(DATA, \n",
    "                                 BATCH_SPLIT, \n",
    "                                 VALIDATION_SPLIT, \n",
    "                                 TEST_SIZE, \n",
    "                                 SEED, \n",
    "                                 BATCHSIZE, \n",
    "                                 INTERPOLATION, \n",
    "                                 WINDOWSIZE, \n",
    "                                 SEP, \n",
    "                                 DECIMAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realdaten Blechsplit (`Train_Test_Split = 2`):\n",
    "- Die Daten trennen in A % Trainingsdaten für den TradaBoostR2 und B % Testdaten\n",
    "--> Solche Auftielung mti den Vorhandneen Methoden leichter umzusetzen als über exakte Anzahl der Bleche\n",
    "- Die Domain Adaptation wird auf dem A % Testdaten durchgeführt\n",
    "- Testen erfolgt auf dem Testdatensatz\n",
    "- Die Größenverhältnisse der Datensätze werden über die Variable  `size` eingestellt\n",
    "- `size = 0.9` so werden 10 % der Bleche in das TradaBoost DaomainAdaptaion gegeben, die restlichen 90 % der Daten werden zum validieren bzw. testen verwendet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA: str = \"assets/data/real-data.csv\"\n",
    "#DATA: str = \"\"\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.99\n",
    "BATCH_SPLIT: bool = True\n",
    "BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "\n",
    "WindowSplittingExecution.execute(DATA, \n",
    "                                 BATCH_SPLIT, \n",
    "                                 VALIDATION_SPLIT, \n",
    "                                 TEST_SIZE, \n",
    "                                 SEED, \n",
    "                                 BATCHSIZE, \n",
    "                                 INTERPOLATION, \n",
    "                                 WINDOWSIZE, \n",
    "                                 SEP, \n",
    "                                 DECIMAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitten der Datensätze - K Fold Crossvalidation**\n",
    " Splitte auf den Inputdaten und dannin den Windo Zeug da so einfacher!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konkardinieren aller Daten zu einem Datensatz\n",
    "dateipfad=\"build\\\\window_split\\\\real-data\\\\Realdaten_Konkardiniert\\\\1744197670\\\\\"\n",
    "input_data = []\n",
    "output_data=[]\n",
    "for datei in os.listdir(dateipfad):\n",
    "    if datei.endswith(\"scaled.npy\"):\n",
    "        pfad = os.path.join(dateipfad, datei)\n",
    "        array=NPY.from_file(pfad).array\n",
    "        print(f\"array shape: {array.shape}\")\n",
    "        if datei.startswith(\"x\"):\n",
    "            input_data.extend(array)\n",
    "        elif datei.startswith(\"y\"):\n",
    "            output_data.extend(array)\n",
    "        print(f\"Datei {datei} geladen\")\n",
    "      \n",
    "print(f\"Anzahl der Input-Daten: {len(input_data)}\")\n",
    "print(f\"Anzahl der Output-Daten: {len(output_data)}\")\n",
    "dataset = [{\"input\": xi, \"output\": yi} for xi, yi in zip(input_data, output_data)]\n",
    "dataset_pd = pd.DataFrame(dataset)\n",
    "unique = []\n",
    "for out in dataset_pd[\"output\"]:\n",
    "    if not any(np.array_equal(out, u) for u in unique):\n",
    "        unique.append(out)\n",
    "\n",
    "print(f\"Anzahl verschiedener Outputs: {len(unique)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random auswählen von Datensätzen, zur statistischen Absicherung des Trainings \n",
    "Nur bezogen auf Realdaten, es werden immer alle SImulationsdaten verwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_data_Mix(N: int, anteil):\n",
    "    for seed in random.randint(0, 32000, N):\n",
    "        DATA: str = \"assets/data/real-data.csv\"\n",
    "        VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "        TEST_SIZE: float = (100-anteil)/100\n",
    "        BATCH_SPLIT: bool = True\n",
    "        BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "        SEED: int = seed # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "        INTERPOLATION: bool = False\n",
    "        WINDOWSIZE: int = 10\n",
    "        SEP: str = \";\" # Separator of the csv file\n",
    "        DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "        \n",
    "        WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                        BATCH_SPLIT, \n",
    "                                        VALIDATION_SPLIT, \n",
    "                                        TEST_SIZE, \n",
    "                                        SEED, \n",
    "                                        BATCHSIZE, \n",
    "                                        INTERPOLATION, \n",
    "                                        WINDOWSIZE, \n",
    "                                        SEP, \n",
    "                                        DECIMAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen von 10 Datensätzen mit jeweils 5% Trainingsdaten \n",
    "validation_data_Mix(N=15, anteil=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klären wie viele Daten in mehreren Dateien vorkommen, um Lekage zu vermeiden. \n",
    "Aufwand um in Pipeline K-Fold einzubauen ist hoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def compute_sample_md5_list(arr):\n",
    "    \"\"\"\n",
    "    Gegeben ein Array arr mit Shape (a, b, c) berechnet diese Funktion\n",
    "    für jeden Sample (Matrix mit Shape (b,c)) den md5-Hash (als Hex-Digest)\n",
    "    und liefert eine Liste der Hashwerte (in der gleichen Reihenfolge wie die Samples).\n",
    "    \"\"\"\n",
    "    hashes = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        # Berechne den md5-Hash des Bytestrings der Matrix\n",
    "        h = hashlib.md5(arr[i].tobytes()).hexdigest()\n",
    "        hashes.append(h)\n",
    "    return hashes\n",
    "\n",
    "def get_all_npy_files(base_dir, target_filename=\"x-train-scaled.npy\"):\n",
    "    \"\"\"\n",
    "    Sucht rekursiv im base_dir nach Dateien mit dem Namen target_filename.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if target_filename in files:\n",
    "            file_paths.append(os.path.join(root, target_filename))\n",
    "    return file_paths\n",
    "\n",
    "# Basisverzeichnis anpassen\n",
    "base_dir = \"build\\\\window_split\\\\real-data\\\\Realdaten_20Prozent_Random\"  # <-- hier Pfad eintragen\n",
    "\n",
    "# 1. Alle Dateien finden\n",
    "file_paths = get_all_npy_files(base_dir)\n",
    "\n",
    "# Dictionaries zur Speicherung:\n",
    "#  - file_hash_lists: für jede Datei die Liste (länge a) der Sample-Hashes (in der Originalreihenfolge)\n",
    "#  - file_counters: für jede Datei den Counter der Hashwerte (Häufigkeiten innerhalb der Datei)\n",
    "file_hash_lists = {}\n",
    "file_counters = {}\n",
    "\n",
    "for fp in file_paths:\n",
    "    arr = np.load(fp)  # Annahme: arr.shape = (a, b, c)\n",
    "    hashes = compute_sample_md5_list(arr)\n",
    "    file_hash_lists[fp] = hashes\n",
    "    file_counters[fp] = Counter(hashes)\n",
    "\n",
    "# 2. Globalen Counter über alle Dateien erstellen\n",
    "global_counter = Counter()\n",
    "for cnt in file_counters.values():\n",
    "    global_counter.update(cnt)\n",
    "\n",
    "# 3. Für jede Datei – für jedes Sample – ermitteln, wie oft der gleiche Sample in anderen Dateien vorkommt.\n",
    "#    Das Ergebnis: für jede Datei eine Liste (Länge a) mit der \"Anzahl Vorkommen in anderen Dateien\".\n",
    "results = {}\n",
    "for fp, hash_list in file_hash_lists.items():\n",
    "    # Hole den Counter für diese Datei\n",
    "    cnt = file_counters[fp]\n",
    "    # Für jeden Sample (bzw. seinen Hash) berechnen:\n",
    "    # global_count[h] - (wie oft h in dieser Datei vorkommt)\n",
    "    # Dies ergibt, wie oft h in allen Dateien außer dieser vorkommt.\n",
    "    occ_list = [global_counter[h] - cnt[h] for h in hash_list]\n",
    "    results[fp] = occ_list\n",
    "\n",
    "# Beispiel: Ausgabe der Summen und Durchschnittswerte pro Datei\n",
    "for fp, occ_list in results.items():\n",
    "    total_occurrences = sum(occ_list)\n",
    "    avg_occurrences = total_occurrences / len(occ_list)\n",
    "    print(f\"Datei {fp}:\")\n",
    "    print(f\"  Insgesamt kommen alle Samples in anderen Dateien insgesamt {total_occurrences} Mal vor.\")\n",
    "    print(f\"  Im Schnitt also {avg_occurrences:.2f} Mal pro Sample.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kontrolle der Verteilungen**\n",
    "Verglichen werden die Verteilungen der Inputs und Outputs vor dem Skalierung und danach\n",
    "Problem: Die Daten der Targetdomain und der Sourcdomain werdne mit unterschiedlichen Scalern skaliert, die skalierten Daten leigen immer übereinander, da mit min max scaliert wird "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad_femDaten=\"build\\\\window_split\\\\sim_data_preprocessed\\\\1743966345\"\n",
    "dateipfad_realDaten=\"build\\\\window_split\\\\experiment-data-subset\\\\1743966491\"\t\n",
    "\n",
    "def plotAllHistOfFIle(file_name):\n",
    "    # Lade die Daten aus beiden Ordnern\n",
    "    fem_path = os.path.join(dateipfad_femDaten, file_name)\n",
    "    real_path = os.path.join(dateipfad_realDaten, file_name)\n",
    "\n",
    "    # Daten als numpy-Array laden\n",
    "    fem_data = np.load(fem_path)\n",
    "    real_data = np.load(real_path,allow_pickle=True)\n",
    "    fem_data = fem_data.reshape(-1, fem_data.shape[2])  # Shape: (287560, 11)\n",
    "    real_data = real_data.reshape(-1, real_data.shape[2])  # Shape: (254180, 11)\n",
    "    # Anzahl der Features (Spalten)\n",
    "    num_features = fem_data.shape[1]\n",
    "    print(fem_data.shape)\n",
    "    print(real_data.shape)\n",
    "    bins = 50  # Anzahl der Bins für die Histogramme\n",
    "\n",
    "\n",
    "    # Anzahl der Spalten pro Zeile\n",
    "    cols = 4\n",
    "    rows = (num_features + cols - 1) // cols  # Rundet auf, falls nicht durch 4 teilbar\n",
    "\n",
    "    # Plots erstellen\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "    axes = axes.flatten()  # 2D Array in 1D umwandeln für einfachere Iteration\n",
    "\n",
    "    for i in range(num_features):\n",
    "        ax = axes[i]\n",
    "        sns.histplot(fem_data[:, i], bins=bins, kde=True, ax=axes[i], color=\"blue\", label=\"Simulation\", stat=\"density\", alpha=0.6)\n",
    "        sns.histplot(real_data[:, i], bins=bins, kde=True, ax=axes[i], color=\"orange\", label=\"Realdaten\", stat=\"density\", alpha=0.6)\n",
    "        ax.set_title(f\"Feature {i+1}\")\n",
    "        ax.set_xlabel(\"Wert\")\n",
    "        ax.set_ylabel(\"Dichte\")\n",
    "        ax.legend()\n",
    "\n",
    "    # Leere Subplots deaktivieren, falls es weniger als 4*n Features gibt\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Datei laden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAllHistOfFIle(\"x-train-scaled.npy\")\n",
    "plotAllHistOfFIle(\"y-train-scaled.npy\")\n",
    "plotAllHistOfFIle(\"y-train.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **DeepCoral Daten**\n",
    "\n",
    "ACHTUNG: Das DeepCORAL für TrAdaBoostR2 ist bisschen unstrukturiert, in Data Analysis müssen hierfür weitere Anpassungen vorgenommen werden, außerdem muss warscheinlich die  Split Funktion von Corvin neu aufgesetzt werden.   \n",
    "\n",
    "- assets/data/stationary_data_250402/sim_data_preprocessed.csv ist gleich zu \\build\\Finale_Data\\stationary_data_unverarbeitet\\sim_data_preprocessed.csv\n",
    "- ->Bereits in TradaBoostR2 getestet\n",
    "- Datensatz -> Immer 20% der Realdaten, also dann 50% davon sind 10% der Gesamtdaten!!!\n",
    "- -> Datensätze sollten gleich sein, aber besser die aus dem jeweiligen Ordner nehmen\n",
    "- sim_data_CORAL... sind nur die Kraftdaten skaliert, erneutes skalieren dürfte bei Standard scaler nichts machen\n",
    "- Datensatz ...used ist komplett in Alignement eingegangen\n",
    "- ->Validierung über Datensatz ...not-used\n",
    "- ->...-used sind Trainingsdaten, Validieurngsdaten dürfen NICHT aus diesem Datensatz kommen\n",
    "- Modell \n",
    "\n",
    "Plan: Erst mal nur CORAL Daten nehmen und damit TradaBostR2 Trainieren\n",
    "- experiment-data-subset_used.csv nehmen davon 99% dann also ~20% der Realdaten\n",
    "- Validierungsdaten und Testdaten aus experiment-data-subset_not_used.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realdata_used=\"build\\\\Finale_Data\\\\stationary_data_offset_labeladaption_Coral\\\\experiment-data-subset_used.csv\"\n",
    "sim_data_CORAL=\"build\\\\Finale_Data\\\\stationary_data_offset_labeladaption_Coral\\\\sim_data_CORAL_preprocessed_offset_labels.csv\"\n",
    "DATA: str = sim_data_CORAL\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.2\n",
    "BATCH_SPLIT: bool = True\n",
    "BATCHSIZE: int = 326 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \".\" # Decimal separator of the csv file\n",
    "\n",
    "WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                BATCH_SPLIT, \n",
    "                                VALIDATION_SPLIT, \n",
    "                                TEST_SIZE, \n",
    "                                SEED, \n",
    "                                BATCHSIZE, \n",
    "                                INTERPOLATION, \n",
    "                                WINDOWSIZE, \n",
    "                                SEP, \n",
    "                                DECIMAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata_NOT_used=\"build\\\\Finale_Data\\\\stationary_data_offset_labeladaption_Coral\\\\experiment-data-subset_not_used.csv\"\n",
    "DATA: str = realdata_NOT_used\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.99\n",
    "BATCH_SPLIT: bool = True\n",
    "BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "\n",
    "WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                BATCH_SPLIT, \n",
    "                                VALIDATION_SPLIT, \n",
    "                                TEST_SIZE, \n",
    "                                SEED, \n",
    "                                BATCHSIZE, \n",
    "                                INTERPOLATION, \n",
    "                                WINDOWSIZE, \n",
    "                                SEP, \n",
    "                                DECIMAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alle Dateien Finden und Bearbeitne für TradaBoostR2\n",
    "def finde_dateipfade(super_ordner):\n",
    "    not_used_pfade = []\n",
    "    used_pfade = []\n",
    "    sim_data_pfade=[]\n",
    "\n",
    "    for root, dirs, files in os.walk(super_ordner):\n",
    "        if 'experiment-data-subset_not_used.csv' in files:\n",
    "            not_used_pfade.append(os.path.join(root, 'experiment-data-subset_not_used.csv'))\n",
    "        if 'experiment-data-subset_used.csv' in files:\n",
    "            used_pfade.append(os.path.join(root, 'experiment-data-subset_used.csv'))\n",
    "        if 'sim_data_CORAL_preprocessed_offset.csv' in files:\n",
    "            sim_data_pfade.append(os.path.join(root, 'sim_data_CORAL_preprocessed_offset.csv'))\n",
    "\n",
    "    return not_used_pfade, used_pfade, sim_data_pfade\n",
    "\n",
    "super_ordner = \"build\\\\stationary_data_coral_offset\"  # <- hier deinen Pfad anpassen\n",
    "not_used_liste, used_liste, sim_data_pfade = finde_dateipfade(super_ordner)\n",
    "print(\"Not Used Dateien:\")\n",
    "for pfad in not_used_liste:\n",
    "    print(pfad)\n",
    "\n",
    "print(\"\\nUsed Dateien:\")\n",
    "for pfad in used_liste:\n",
    "    print(pfad)\n",
    "\n",
    "print(\"\\nSim Dateien:\")\n",
    "for pfad in sim_data_pfade:\n",
    "    print(pfad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulationsdaten CORAL 1 bis 5\n",
    "for dateipfad in sim_data_pfade:\n",
    "    sim_data_CORAL=dateipfad\n",
    "    DATA: str = sim_data_CORAL\n",
    "    VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "    TEST_SIZE: float = 0.2\n",
    "    BATCH_SPLIT: bool = True\n",
    "    BATCHSIZE: int = 326 # real-data: 1800, synthetic-data: 326\n",
    "    SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "    INTERPOLATION: bool = False\n",
    "    WINDOWSIZE: int = 10\n",
    "    SEP: str = \";\" # Separator of the csv file\n",
    "    DECIMAL: str = \".\" # Decimal separator of the csv file\n",
    "\n",
    "    WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                    BATCH_SPLIT, \n",
    "                                    VALIDATION_SPLIT, \n",
    "                                    TEST_SIZE, \n",
    "                                    SEED, \n",
    "                                    BATCHSIZE, \n",
    "                                    INTERPOLATION, \n",
    "                                    WINDOWSIZE, \n",
    "                                    SEP, \n",
    "                                    DECIMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment Data für CORAL subset_used\n",
    "for datei_pfad in used_liste:\n",
    "    DATA: str = datei_pfad\n",
    "    VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "    TEST_SIZE: float = 0.07\n",
    "    BATCH_SPLIT: bool = True\n",
    "    BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "    SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "    INTERPOLATION: bool = False\n",
    "    WINDOWSIZE: int = 10\n",
    "    SEP: str = \";\" # Separator of the csv file\n",
    "    DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "\n",
    "    WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                    BATCH_SPLIT, \n",
    "                                    VALIDATION_SPLIT, \n",
    "                                    TEST_SIZE, \n",
    "                                    SEED, \n",
    "                                    BATCHSIZE, \n",
    "                                    INTERPOLATION, \n",
    "                                    WINDOWSIZE, \n",
    "                                    SEP, \n",
    "                                    DECIMAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment Data für CORAL not_used_liste\n",
    "for datei_pfad in not_used_liste:\n",
    "    DATA: str = datei_pfad\n",
    "    VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "    TEST_SIZE: float = 0.99\n",
    "    BATCH_SPLIT: bool = True\n",
    "    BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "    SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "    INTERPOLATION: bool = False\n",
    "    WINDOWSIZE: int = 10\n",
    "    SEP: str = \";\" # Separator of the csv file\n",
    "    DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "\n",
    "    WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                    BATCH_SPLIT, \n",
    "                                    VALIDATION_SPLIT, \n",
    "                                    TEST_SIZE, \n",
    "                                    SEED, \n",
    "                                    BATCHSIZE, \n",
    "                                    INTERPOLATION, \n",
    "                                    WINDOWSIZE, \n",
    "                                    SEP, \n",
    "                                    DECIMAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"build\\window_split\\experiment-data-subset_used\\1744565138\" ->TEST_SIZE: float = 0.07\n",
    "->Trainingsdatensatz für TradaBoostR2 Realdaten\n",
    "\n",
    "Scaler existieren nun, diese auf die restlichen Daten anwenden\n",
    "\n",
    "->Restliche Daten: Erzeugen mit WindowSplittingExecution \n",
    "\n",
    "Daten Not Used in Training:\n",
    "build\\window_split\\experiment-data-subset_not_used\\1744565885 -> TEST_SIZE: float = 0.99\n",
    "\n",
    "->Diese Scaler und scalierten Daten sind Unbrauchbar!!!\n",
    "\n",
    "Vorgehen:\n",
    "1. WRONGx-test-scaled.npy und WRONGx-validate-scaled.npy mit THIS_scalers_features.pkl zurückscalieren\n",
    "2. x-test.npy und x-validate.npy erzeugen\n",
    "3. x-test.npy und x-validate.npy mit dem Scaler aus experiment-data-subset_used\\1744565138 scalieren und in build\\window_split\\experiment-data-subset_used\\1744565138 abspeichern als x-test-scaled.npy x-validate-scaled.npy\n",
    "4. y-test.npy und y-validate.npy mit dem Scaler aus experiment-data-subset_used\\1744565138 scalieren und in build\\window_split\\experiment-data-subset_used\\1744565138 abspeichern als y-test-scaled.npy y-validate-scaled.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rückskalieren\n",
    "def rückskalieren_WRONG_Daten(ordner_pfad):\n",
    "    \"\"\"\n",
    "    ACHTUNG: Überschriebt ohne Namensänderung!!!!!!\n",
    "    Lädt die gruppierten Scaler aus 'scalers_features.pkl' im angegebenen Ordner,\n",
    "    skaliert die Dateien 'x-test-scaled.npy' und 'x-validate-scaled.npy' gruppenweise zurück \n",
    "    und speichert sie als 'x-test.npy' und 'x-validate.npy' im gleichen Ordner.\n",
    "\n",
    "    Parameter:\n",
    "        ordner_pfad (str): Pfad zum Ordner, der die Dateien enthält.\n",
    "        group_features (list of list of int): Featuregruppen entsprechend der Scaler-Reihenfolge.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scaler laden\n",
    "    scaler_path = os.path.join(ordner_pfad, \"scalers_features.pkl\")\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scalers_features = pickle.load(f)\n",
    "\n",
    "    # Skalierte Test- und Validierungsdaten laden\n",
    "    x_test_scaled_path = os.path.join(ordner_pfad, \"x-test-scaled.npy\")\n",
    "    x_validate_scaled_path = os.path.join(ordner_pfad, \"x-validate-scaled.npy\")\n",
    "\n",
    "    x_test_scaled = np.load(x_test_scaled_path)\n",
    "    x_validate_scaled = np.load(x_validate_scaled_path)\n",
    "\n",
    "    # Rückskalieren gruppenweise\n",
    "    print(scalers_features[0].n_features_in_)\n",
    "    print(x_test_scaled[:,:, [0, 1, 2, 3, 4, 5, 6, 7]].shape)\n",
    "    group_features = [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9], [10]]\n",
    "    x_test_unscaled = np.zeros_like(x_test_scaled)\n",
    "    x_validation_unscaled = np.zeros_like(x_validate_scaled)\n",
    "    \n",
    "    # Rückskalieren für jede Gruppe\n",
    "    for x_unscaled, x_scaled in zip([x_test_unscaled, x_validation_unscaled], [x_test_scaled, x_validate_scaled]):\n",
    "        num_samples, num_timesteps, _ = x_scaled.shape\n",
    "        print(f\"num_samples: {num_samples}\")\n",
    "        print(f\"num_timesteps: {num_timesteps}\")\n",
    "        for scaler, feature_indices in zip(scalers_features, group_features):\n",
    "            print(feature_indices)\n",
    "            subset_scaled = x_scaled[:, :, feature_indices]\n",
    "            subset_scaled_2d = subset_scaled.reshape(-1, len(feature_indices))\n",
    "            subset_unscaled_2d = scaler.inverse_transform(subset_scaled_2d)\n",
    "            subset_unscaled = subset_unscaled_2d.reshape(num_samples, num_timesteps, len(feature_indices))\n",
    "            x_unscaled[:, :, feature_indices] = subset_unscaled\n",
    "            \n",
    "\n",
    "    # Zurückgespeicherte Dateien\n",
    "    x_test_output_path = os.path.join(ordner_pfad, \"x-test.npy\")\n",
    "    x_validate_output_path = os.path.join(ordner_pfad, \"x-validate.npy\")\n",
    "\n",
    "    np.save(x_test_output_path, x_test_unscaled)\n",
    "    np.save(x_validate_output_path, x_validation_unscaled)\n",
    "\n",
    "    print(f\"Zurückskalierte Dateien gespeichert:\\n- {x_test_output_path}\\n- {x_validate_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skalieren in Zielscaler\n",
    "def scale_x_values(folder_training_scaler, folder_test_val_data_unscaled):\n",
    "    \"\"\"\n",
    "    Skaliert Test- und Validierungsdaten gruppenweise mit gespeicherten Scalern \n",
    "    und speichert die skalierten und unskalierten Daten im Trainings-Ordner.\n",
    "\n",
    "    Parameter:\n",
    "        folder_training_scaler (str): Ordnerpfad, in dem scalers_features.pkl und scalers_labels.pkl liegen.\n",
    "        folder_test_val_data_unscaled (str): Ordnerpfad mit unskalierten x- und y-Daten (.npy-Dateien).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Scaler laden ---\n",
    "    scalers_features = load_scaler(os.path.join(folder_training_scaler, \"scalers_features.pkl\"))\n",
    "    scalers_labels = load_scaler(os.path.join(folder_training_scaler, \"scalers_labels.pkl\"))\n",
    "\n",
    "    # --- Unskalierte Daten laden ---\n",
    "    x_test = np.load(os.path.join(folder_test_val_data_unscaled, \"x-test.npy\"))\n",
    "    x_validate = np.load(os.path.join(folder_test_val_data_unscaled, \"x-validate.npy\"))\n",
    "    y_test = np.array(np.load(os.path.join(folder_test_val_data_unscaled, \"y-test.npy\"),allow_pickle=True))\n",
    "    y_validate = np.array(np.load(os.path.join(folder_test_val_data_unscaled, \"y-validate.npy\"),allow_pickle=True))\n",
    "    \n",
    "    # --- Feature-Gruppen-Definition ---\n",
    "    group_features = [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9], [10]]\n",
    "    group_labels = [[0, 1], [2]]\n",
    "\n",
    "    # --- Skaliere x-Daten ---\n",
    "    x_test_scaled = scale_groupwise(x_test, scalers_features, group_features)\n",
    "    x_validate_scaled = scale_groupwise(x_validate, scalers_features, group_features)\n",
    "    \n",
    "    # --- Skaliere y-Daten ---\n",
    "    y_test_scaled = scale_groupwise(y_test, scalers_labels, group_labels)\n",
    "    y_validate_scaled = scale_groupwise(y_validate, scalers_labels, group_labels)\n",
    "    \n",
    "    # --- Speichern ---\n",
    "    save_scaled_data(folder_training_scaler, \"x-test\", x_test, x_test_scaled)\n",
    "    save_scaled_data(folder_training_scaler, \"x-validate\", x_validate, x_validate_scaled)\n",
    "    save_scaled_data(folder_training_scaler, \"y-test\", y_test, y_test_scaled)\n",
    "    save_scaled_data(folder_training_scaler, \"y-validate\", y_validate, y_validate_scaled)\n",
    "\n",
    "    print(\"✅ Skaliert und gespeichert.\")\n",
    "\n",
    "# Hilfsfunktion: Scaler laden\n",
    "def load_scaler(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Hilfsfunktion: Gruppiertes Skalieren\n",
    "def scale_groupwise(data, scalers, groups):\n",
    "    \"\"\"\n",
    "    Skaliert gruppenweise Daten mit einer Liste von Scalers.\n",
    "\n",
    "    Parameter:\n",
    "        data (np.ndarray): Eingabedaten (mindestens 2D oder 3D für x-Daten)\n",
    "        scalers (list): Liste der Scaler-Objekte\n",
    "        groups (list of list): Liste der Gruppen-Feature-Indices\n",
    "        \n",
    "    Rückgabe:\n",
    "        np.ndarray: Skalierte Daten in gleicher Form wie Input\n",
    "    \"\"\"\n",
    "    scaled_data = np.zeros_like(data)\n",
    "\n",
    "    if data.ndim == 3:  # z.B. (samples, timesteps, features)\n",
    "        num_samples, num_timesteps, _ = data.shape\n",
    "        for scaler, feature_indices in zip(scalers, groups):\n",
    "            subset = data[:, :, feature_indices]\n",
    "            subset_2d = subset.reshape(-1, len(feature_indices))\n",
    "            subset_scaled_2d = scaler.transform(subset_2d)\n",
    "            subset_scaled = subset_scaled_2d.reshape(num_samples, num_timesteps, len(feature_indices))\n",
    "            scaled_data[:, :, feature_indices] = subset_scaled\n",
    "\n",
    "    elif data.ndim == 2:  # z.B. (samples, features)\n",
    "        num_samples, _ = data.shape\n",
    "        for scaler, feature_indices in zip(scalers, groups):\n",
    "            subset = data[:, feature_indices]\n",
    "            subset_scaled = scaler.transform(subset)\n",
    "            scaled_data[:, feature_indices] = subset_scaled\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data shape. Expected 2D or 3D array.\")\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "# Hilfsfunktion: Speichern von skalierten und unskalierten Daten\n",
    "def save_scaled_data(folder, base_filename, unscaled_data, scaled_data):\n",
    "    print(f\"unscaled_data in: {os.path.join(folder, f\"{base_filename}.npy\")}\")\n",
    "    print(f\"scaled_data in : {os.path.join(folder, f\"{base_filename}-scaled.npy\")}\")\n",
    "    np.save(os.path.join(folder, f\"{base_filename}.npy\"), unscaled_data)\n",
    "    np.save(os.path.join(folder, f\"{base_filename}-scaled.npy\"), scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACHTUNG: Auskommentiert weil Method ezeug Überschreiben kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der unscalierten daten x-test und x-validate der experiment-data-subset_not_used Daten\n",
    "#for root, dirs, files in os.walk(\"build\\\\window_split\\\\experiment-data-subset_not_used\"):\n",
    "    #if 'metadata.json' in files: # Sonst Fehler um 1\n",
    "        # Zurückscalieren der daten\n",
    "       # rückskalieren_WRONG_Daten(os.path.join(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harter Code notwendig wegen zuordnung von Ordnern mit Daten und den Scalern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACHTUNG: Überschriebt Daten\n",
    "''' \n",
    "scale_x_values(folder_training_scaler=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744580192\",\n",
    "                folder_test_val_data_unscaled=\"build\\\\window_split\\\\experiment-data-subset_not_used\\\\1744580616\")\n",
    "\n",
    "scale_x_values(folder_training_scaler=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744580196\",\n",
    "                folder_test_val_data_unscaled=\"build\\\\window_split\\\\experiment-data-subset_not_used\\\\1744580632\")\n",
    "\n",
    "scale_x_values(folder_training_scaler=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744580201\",\n",
    "                folder_test_val_data_unscaled=\"build\\\\window_split\\\\experiment-data-subset_not_used\\\\1744580652\")\n",
    "\n",
    "scale_x_values(folder_training_scaler=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744580205\",\n",
    "                folder_test_val_data_unscaled=\"build\\\\window_split\\\\experiment-data-subset_not_used\\\\1744580674\")\n",
    "\n",
    "scale_x_values(folder_training_scaler=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744580210\",\n",
    "                folder_test_val_data_unscaled=\"build\\\\window_split\\\\experiment-data-subset_not_used\\\\1744580698\")\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_x_values(folder_training_scaler=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744565138\",\n",
    "                folder_test_val_data_unscaled=\"build\\\\window_split\\\\experiment-data-subset_not_used\\\\1744565885\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Tuning des TradaBoostR2 geeigneten Modells**\n",
    "- TradaBoostR2 erlaubt nur einen Putput, hier ein Array mit 3 Elementen \n",
    "- Tuning muss überarbeitete werden, da Validationloss für Modelle mit drei outputs nicht geeignet ist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.execution import CNNTuningExecution\n",
    "SPLITTED_DATA_FOLDER = \"build\\\\window_split\\\\sim_data_preprocessed\\\\1743966827\\\\\"\n",
    "CNNTuningExecution.execute_tuning_singleOutput(SPLITTED_DATA_FOLDER,max_trials=10,train_on_scled_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **TradaBoostR2**\n",
    "- Early stopping und Valiadation enthalten\n",
    "- Modell umschrieben mit neuer Sparse Layer, eine Layer fasst die Outputs von zuvor zusammen \n",
    "    - Die Letze Layer wird nun nur sparse besetzt und ist nicht trainierbar\n",
    "    - Outputs werden direkt in diese Layer weitergegeben, ist so ne Eigenheit von TradaBoostR2, kann nur auf einem Output arbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(package=\"custom_layers\")\n",
    "class SparseStackLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SparseStackLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Kombiniert Liste von Tensoren (z. B. [batch, 1], [batch, 1], [batch, 1])\n",
    "        # zu einem Tensor mit Shape (batch, 3)\n",
    "        return tf.concat(inputs, axis=-1)\n",
    "    \n",
    "class LivePlotCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = {\"loss\": [], \"val_loss\": []}\n",
    "        #self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\", 0))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\", 0))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\", 0))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\", 0))\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        epochs = list(range(1, len(self.history[\"loss\"]) + 1))\n",
    "\n",
    "        plt.plot(epochs, self.history[\"loss\"], label=\"Trainingsverlust (Loss)\", color=\"blue\", marker=\"o\")\n",
    "        plt.plot(epochs, self.history[\"val_loss\"], label=\"Validierungsverlust (Loss)\", color=\"red\", linestyle=\"dashed\", marker=\"o\")\n",
    "\n",
    "        plt.xlabel(\"Epoche\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training vs. Validierung\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "def domainAdapt_tradaBoostR2_noProcessDoku(cnn_wrapped,n_estimators_tradaBoostR2,learning_rate_tradaBoostR2,\n",
    "                                           X_source_scaled,y_source_scaled,\n",
    "                                           X_target_scaled,y_target_scaled,\n",
    "                                           X_source_scaled_Test,y_source_scaled_Test,\n",
    "                                           save_folder, save_filename                    \n",
    "                                           ):\n",
    "    print(\"Keine Prozessdokumentation der einzelnen Iterationen\")\n",
    "    #Dim Reduktion, weil halt\n",
    "    y_source_scaled=np.squeeze(y_source_scaled)\n",
    "    y_target_scaled=np.squeeze(y_target_scaled)\n",
    "\n",
    "    tradaboost_model = TrAdaBoostR2(\n",
    "        estimator=cnn_wrapped,  # CNN als Basis-Regressor\n",
    "        n_estimators=n_estimators_tradaBoostR2,  # Anzahl der Boosting-Iterationen\n",
    "        verbose=1,\n",
    "        lr=learning_rate_tradaBoostR2,  # Lernrate für den Boosting-Prozess\n",
    "        Xt=X_target_scaled,  # Ziel-Domain-Daten (Realdaten)\n",
    "        yt=y_target_scaled,  # Ziel-Domain-Zielwerte (Realdaten)\n",
    "    )\n",
    "\n",
    "    tradaboost_model.fit(X_source_scaled, y_source_scaled,validation_data=(X_source_scaled_Test,y_source_scaled_Test)) # TODO Realdaten in Validieurn gaufnehmen\n",
    "    \n",
    "    trained_model = tradaboost_model.estimators_[-1]  # Letzter trainierter Estimator\n",
    "    trained_model.model_.save(os.path.join(save_folder, f\"{save_filename}.h5\"))\n",
    "   \n",
    "def domainAdapt_tradaBoostR2_earlystopping(cnn_wrapped,n_estimators_tradaBoostR2,learning_rate_tradaBoostR2,\n",
    "                                           X_source_scaled,y_source_scaled,\n",
    "                                           X_target_scaled,y_target_scaled,\n",
    "                                           X_source_scaled_Test,y_source_scaled_Test,\n",
    "                                           save_folder, save_filename                    \n",
    "                                           ):\n",
    "    \n",
    "    #Dim Reduktion, weil halt\n",
    "    y_source_scaled=np.squeeze(y_source_scaled)\n",
    "    y_target_scaled=np.squeeze(y_target_scaled)\n",
    "\n",
    "    training_progress = []\n",
    "    tradaboost_model = TrAdaBoostR2(\n",
    "        estimator=cnn_wrapped,  # CNN als Basis-Regressor\n",
    "        n_estimators=n_estimators_tradaBoostR2,  # Anzahl der Boosting-Iterationen\n",
    "        verbose=1,\n",
    "        lr=learning_rate_tradaBoostR2,  # Lernrate für den Boosting-Prozess\n",
    "        )\n",
    "    \n",
    "    print(\"\\n Training mit TrAdaBoostR2 gestartet \\n\")\n",
    "    print(f\"save_folder = {save_folder}\")\n",
    "    print(f\"\")\n",
    "    #tqdm mach interen Schlaife von TradaBoostR2 zu externer schleife um Trainingsprozess zu verfolgen\n",
    "    with tqdm(total=tradaboost_model.n_estimators, desc=\"TrAdaBoostR2 Fortschritt\") as pbar:\n",
    "        for i in range(tradaboost_model.n_estimators):\n",
    "            #Fitten einer Iteration von TradaBoostR2\n",
    "            tradaboost_model.fit(X=X_source_scaled,y=y_source_scaled,\n",
    "                                 Xt=X_target_scaled,  # Ziel-Domain-Daten (Realdaten)\n",
    "                                 yt=y_target_scaled,  # Ziel-Domain-Zielwerte (Realdaten)\n",
    "                                 validation_data=(X_source_scaled_Test,y_source_scaled_Test)) # TODO Realdaten in Validieurn gaufnehmen\n",
    "            #Modelle zwischenspeichern für evaluierung des Trainings\n",
    "            print(f\"Modell in interation {i} abspeichern als:{os.path.join(save_folder, f\"tradaboost_model_iter_{i}.h5\")}\")\n",
    "            keras_model = tradaboost_model.estimators_[-1].model_\n",
    "            keras_model.save(os.path.join(save_folder, f\"tradaboost_model_iter_{i}.h5\"))\n",
    "            #Trainingsprocess speichern\n",
    "            checkpoint_data = {\n",
    "                \"iteration\": i + 1,\n",
    "                \"loss\": keras_model.evaluate(X_source_scaled_Test, y_source_scaled_Test, verbose=0),\n",
    "                \"estimator_count\": len(tradaboost_model.estimators_),\n",
    "                \"weights\": tradaboost_model.estimator_weights_,\n",
    "                \"errors\": tradaboost_model.estimator_errors_,\n",
    "            }\n",
    "            training_progress.append(checkpoint_data)\n",
    "            #Updaten des Modellsfortschritts \n",
    "            pbar.update(1)\n",
    "    print(\"\\n Training mit TrAdaBoostR2 beendet \\n\")\n",
    "    \n",
    "    json_path = os.path.join(save_folder, f\"tradaBoostR2_trainingProgress.json\")\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(training_progress, json_file, indent=4, default=str)\n",
    "    \n",
    "    # Extrahiertes Modell nach dem Training speichern\n",
    "    trained_model = tradaboost_model.estimators_[-1]  # Letzter trainierter Estimator\n",
    "\n",
    "    # Modell speichern\n",
    "    trained_model.model_.save(os.path.join(save_folder, f\"{save_filename}.h5\"))\n",
    "\n",
    "def execute_tradaBoostR2_training(dateipfad_femDaten,dateipfad_realDaten,model_file,save_folder,save_filename, learning_rate,\n",
    "                                  epochs,use_scaled_labels=False,with_process_Doku=True,n_estimators_tradaBoostR2=5,learning_rate_tradaBoostR2=1,batch_size=32,patience=3):\n",
    "    \n",
    "    if use_scaled_labels:\n",
    "        label_selcetion=\"-scaled\"\n",
    "    else:\n",
    "        label_selcetion=\"\"\n",
    "    \n",
    "    #Simulationsdaten \n",
    "    pfad_X_source_scaled=dateipfad_femDaten + \"x-train-scaled.npy\"\n",
    "    pfad_y_source_scaled=dateipfad_femDaten + f\"y-train{label_selcetion}.npy\"\n",
    "    pfad_X_source_scaled_Test=dateipfad_femDaten + \"x-validate-scaled.npy\"\n",
    "    pfad_y_source_scaled_Test=dateipfad_femDaten + f\"y-validate{label_selcetion}.npy\"\n",
    "    X_source_scaled = NPY.from_file(pfad_X_source_scaled).array\n",
    "    y_source_scaled = NPY.from_file(pfad_y_source_scaled).array\n",
    "    X_source_scaled_Test = NPY.from_file(pfad_X_source_scaled_Test).array\n",
    "    y_source_scaled_Test = NPY.from_file(pfad_y_source_scaled_Test).array\n",
    "    \n",
    "    # Realdaten\n",
    "    pfad_x_target_scaled=dateipfad_realDaten + \"x-train-scaled.npy\"\n",
    "    pfad_y_target_scaled=dateipfad_realDaten + f\"y-train{label_selcetion}.npy\"\n",
    "    X_target_scaled = NPY.from_file(pfad_x_target_scaled).array\n",
    "    y_target_scaled = NPY.from_file(pfad_y_target_scaled).array\n",
    "\n",
    "    \n",
    "    #Modell erstellen -> Outputs über Spares Layer zu einem Array zusammenfassen\n",
    "    cnn = CNN.from_file(model_file)\n",
    "    outputs = cnn.model.outputs\n",
    "    combined_output = SparseStackLayer(name=\"sparse_output\")(outputs) \n",
    "    new_model = tf.keras.Model(inputs=cnn.model.inputs, outputs=combined_output, name=\"extended_model\")\n",
    "    \n",
    "    #Modell aufbauen mit live plot um Trainingsprozess zu bewerten\n",
    "    live_plot_callback = LivePlotCallback()\n",
    "    #Early Stopping hier erneut einbauen\n",
    "    early_stopping =keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    #Trainignsfortschritt dokumentieren über CSV Daten, Achtung nur letze Tradaboost iteratoon enthalten wegen überschreiben\n",
    "    csv_logger = CSVLogger(filename=os.path.join(save_folder, \"training_log.csv\"), separator=\",\", append=True)\n",
    "    \n",
    "    #Modell aufbauen\n",
    "    new_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                    loss=\"mean_absolute_error\", \n",
    "                    metrics=[\"mae\"])#TODO Bruache ich diese Zeile?\n",
    "    cnn_wrapped = KerasRegressor(model=new_model, epochs=epochs, batch_size=batch_size, verbose=1,callbacks=[early_stopping,csv_logger,live_plot_callback])\n",
    "    \n",
    "    #Dokumentation des Training smit Tradaboost\n",
    "    initial_training_setup = {\n",
    "                \"Speicherorte\": {\n",
    "                \"Modell\": model_file,\n",
    "                \"Simulationsdaten\": dateipfad_femDaten,\n",
    "                \"Realdaten\": dateipfad_realDaten,\n",
    "                \"save_folder\": save_folder,\n",
    "                \"save_filename\": save_filename,\n",
    "                \"Process Dokumentation\": with_process_Doku\n",
    "                },\n",
    "                \"Training Parameter\": {\n",
    "                 \n",
    "                \"Optimizer\": {\n",
    "                    \"name\": \"Adam\",\n",
    "                    \"learning_rate\": learning_rate\n",
    "                },\n",
    "                \"early_stopping\": {\n",
    "                    \"patience\": patience,\n",
    "                    \"restore_best_weights\": True\n",
    "                },\n",
    "                \"KerasRegressor\":{\n",
    "                    \"epochs\": epochs,\n",
    "                    \"batch_size\": batch_size\n",
    "                },\n",
    "                \"loss\": \"mean_absolute_error\",\n",
    "                \"metrics\": [\"mae\"]\n",
    "                },\n",
    "                \"Parameter TradaBoostR2\": {\n",
    "                \"n_estimators\": n_estimators_tradaBoostR2,\n",
    "                \"learning rate\": learning_rate_tradaBoostR2\n",
    "                },\n",
    "                \"Daten\":{\n",
    "                    \"X_source_scaled\": pfad_X_source_scaled,\n",
    "                    \"y_source_scaled\": pfad_y_source_scaled,\n",
    "                    \"X_target_scaled\": pfad_x_target_scaled,\n",
    "                    \"y_target_scaled\": pfad_y_target_scaled,\n",
    "                    \"X_source_scaled_Test\": pfad_X_source_scaled_Test,\n",
    "                    \"y_source_scaled_Test\": pfad_y_source_scaled_Test\n",
    "                },\n",
    "                \"Dimensionen der Daten\": {\n",
    "                    \"X_source_scaled\": X_source_scaled.shape,\n",
    "                    \"y_source_scaled\": y_source_scaled.shape,\n",
    "                    \"X_target_scaled\": X_target_scaled.shape,\n",
    "                    \"y_target_scaled\": y_target_scaled.shape,\n",
    "                    \"X_source_scaled_Test\": X_source_scaled_Test.shape,\n",
    "                    \"y_source_scaled_Test\": y_source_scaled_Test.shape\n",
    "                }\n",
    "                }\n",
    "    \n",
    "    # Speichern der Dokumentation in einer JSON-Datei\n",
    "    json_path = os.path.join(save_folder, f\"tradaBoostR2_training_BasisParameter.json\")\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(initial_training_setup, json_file, indent=4, default=str)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if with_process_Doku==True:\n",
    "        domainAdapt_tradaBoostR2_earlystopping(cnn_wrapped=cnn_wrapped,n_estimators_tradaBoostR2=n_estimators_tradaBoostR2,\n",
    "                                            learning_rate_tradaBoostR2=learning_rate_tradaBoostR2,\n",
    "                                            X_source_scaled=X_source_scaled,y_source_scaled=y_source_scaled,\n",
    "                                            X_target_scaled=X_target_scaled,y_target_scaled=y_target_scaled,\n",
    "                                            X_source_scaled_Test=X_source_scaled_Test,y_source_scaled_Test=y_source_scaled_Test,\n",
    "                                            save_folder=save_folder, save_filename=save_filename)\n",
    "    else:\n",
    "        domainAdapt_tradaBoostR2_noProcessDoku(cnn_wrapped=cnn_wrapped,n_estimators_tradaBoostR2=n_estimators_tradaBoostR2,\n",
    "                                            learning_rate_tradaBoostR2=learning_rate_tradaBoostR2,\n",
    "                                            X_source_scaled=X_source_scaled,y_source_scaled=y_source_scaled,\n",
    "                                            X_target_scaled=X_target_scaled,y_target_scaled=y_target_scaled,\n",
    "                                            X_source_scaled_Test=X_source_scaled_Test,y_source_scaled_Test=y_source_scaled_Test,\n",
    "                                            save_folder=save_folder, save_filename=save_filename)\n",
    "    end_time = time.time()\n",
    "    dauer = {\"Rechendauer \" : end_time - start_time}\n",
    "    json_path = os.path.join(save_folder, f\"Rechendauer.json\")\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(dauer, json_file, indent=4, default=str)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einzelnes TradaBoostR2 Trainign zum Debuggen und ausbessern\n",
    "dateipfad_femDaten=\"build\\\\window_split\\\\sim_data_preprocessed_offset_multiplitiv\\\\1744056915\\\\\"\n",
    "dateipfad_realDaten=\"build\\\\window_split\\\\real-data\\\\1744062620\\\\\"\n",
    "model_file = \"assets\\\\models\\\\untrained\\\\sim_data_preprocessed\\\\1743665223\\\\best-model.h5\"\n",
    "best_learning_rate=0.0005026453831067469\n",
    "save_folder = os.path.join(\"build\", \"tradaboost_model\", \"training_09\")\n",
    "save_filename=\"training_09\"\n",
    "\n",
    "#AUskommentiert weil kann Ergebnisse überschrieben\n",
    "#execute_tradaBoostR2_training(with_process_Doku=False,dateipfad_femDaten=dateipfad_femDaten,dateipfad_realDaten=dateipfad_realDaten,\n",
    "#                              model_file=model_file,save_folder=save_folder,save_filename=save_filename,n_estimators_tradaBoostR2=5,learning_rate_tradaBoostR2=0.7,\n",
    "#                              learning_rate=best_learning_rate,epochs=6,batch_size=32,patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden immer alle Simulationsdaten verwendet: Ein 80/20 Split\n",
    "Training des Modells für unterschiedliche Anteile an Realdaten:\n",
    "- Beginnend bei 80% der Realdaten für das Training und 20% für die Evaluation\n",
    "- Danach Verringern um 10% Schritte auf 10%\n",
    "- Extremtests mit 5% 3% und 1% der Realdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.execution\n",
    "importlib.reload(src.execution)\n",
    "from src.execution import WindowSplittingExecution\n",
    "training_base=[90,80,70,60,50,40,30,20,10,7,5,3,1] # Prozentualer Anteil der Target-Trainings-Daten an den Gesamttargetdaten\n",
    "for anteil in training_base:\n",
    "    test_size=(100-anteil)/100\n",
    "    print(f\"Testsize: {test_size}\")\n",
    "    DATA: str = \"assets/data/real-data.csv\"\n",
    "    VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "    TEST_SIZE: float = (100-anteil)/100\n",
    "    BATCH_SPLIT: bool = True\n",
    "    BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "    SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "    INTERPOLATION: bool = False\n",
    "    WINDOWSIZE: int = 10\n",
    "    SEP: str = \";\" # Separator of the csv file\n",
    "    DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "    WindowSplittingExecution.execute_with_scaler(DATA, \n",
    "                                    BATCH_SPLIT, \n",
    "                                    VALIDATION_SPLIT, \n",
    "                                    TEST_SIZE, \n",
    "                                    SEED, \n",
    "                                    BATCHSIZE, \n",
    "                                    INTERPOLATION, \n",
    "                                    WINDOWSIZE, \n",
    "                                    SEP, \n",
    "                                    DECIMAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TradaBoostR2 Trainings Setup: Erzeugt nur Ordner und Dateinamen um Trainign automatisch ablaufen zu lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugt nur Dateipfadlisten und Ordner falls diese nicht vorhanden sind, kann nicht Überschreiben\n",
    "#Trainieren der Modelle\n",
    "upper_Directory=\"build\\\\window_split\\\\real-data\\\\Realdaten_50Prozent_Random_N15\"\n",
    "subfolder_paths = [\n",
    "    os.path.join(upper_Directory, name) +'\\\\'\n",
    "    for name in os.listdir(upper_Directory)\n",
    "    if os.path.isdir(os.path.join(upper_Directory, name))\n",
    "]\n",
    "print(subfolder_paths)\n",
    "#Ordner für Modelle erstellen\n",
    "base_dir = \"build\\\\tradaboost_model\\\\training_29_50Prozent_Realdaten_15Stueck\"  # <-- hier anpassen\n",
    "start_index = 1\n",
    "end_index = 15\n",
    "prefix = \"tarining_29\"\n",
    "speicherort_modell=[]\n",
    "for i in range(start_index, end_index + 1):\n",
    "    folder_name = f\"{prefix}_{i:02d}\"\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"✅ Erstellt: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"⏭️ Übersprungen (existiert schon): {folder_path}\")\n",
    "    speicherort_modell.append(folder_path+\"\\\\\")\n",
    "print(\"\")\n",
    "print(speicherort_modell)\n",
    "#Trainieren der Modelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training mit Tradaboost durchführen, für die Realdaten in subfolder_paths und abspeichern der Modelle in speicherort_modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad_femDaten=\"build\\\\window_split\\\\sim_data_preprocessed\\\\1744139600\\\\\"\n",
    "#model_file = \"assets\\\\models\\\\untrained\\\\sim_data_preprocessed\\\\1743665223\\\\best-model.h5\"\n",
    "model_file = \"assets\\\\models\\\\untrained\\\\synthetic-model\\\\syn-model.h5\"\n",
    "best_learning_rate=0.0005026453831067469\n",
    "for idx,(data_path,model_save_path) in enumerate(zip(subfolder_paths, speicherort_modell)):\n",
    "    print(\"========================================================================\")\n",
    "    print(f\"Trainingsdaten: {data_path}\")\n",
    "    print(f\"Speicherort: {model_save_path}\")\n",
    "    execute_tradaBoostR2_training(with_process_Doku=False,dateipfad_femDaten=dateipfad_femDaten,dateipfad_realDaten=data_path,use_scaled_labels=True,\n",
    "                              model_file=model_file,save_folder=model_save_path,save_filename=\"best_model\",n_estimators_tradaBoostR2=5,learning_rate_tradaBoostR2=1,\n",
    "                              learning_rate=best_learning_rate,epochs=10,batch_size=32,patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad_femDaten=\"build\\\\window_split\\\\sim_data_CORAL_preprocessed_offset_labels\\\\1744574986\\\\\"\n",
    "dateipfad_realDaten=\"build\\\\window_split\\\\experiment-data-subset_used\\\\1744565138\\\\\"\n",
    "model_file = \"assets\\\\models\\\\untrained\\\\sim_data_preprocessed\\\\1743665223\\\\best-model.h5\"\n",
    "best_learning_rate=0.0005026453831067469\n",
    "save_folder = os.path.join(\"build\", \"tradaboost_model\", \"training_CORAL_01\")\n",
    "save_filename=\"best_model\"\n",
    "\n",
    "#execute_tradaBoostR2_training(with_process_Doku=False,dateipfad_femDaten=dateipfad_femDaten,dateipfad_realDaten=dateipfad_realDaten,\n",
    "#                              model_file=model_file,save_folder=save_folder,save_filename=save_filename,n_estimators_tradaBoostR2=5,learning_rate_tradaBoostR2=1,\n",
    "#                              learning_rate=best_learning_rate,epochs=10,batch_size=32,patience=3,use_scaled_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Training eines TradaBoostR2 Modells mit Daten aus CORAL:\n",
    "\n",
    "- Scaler sind auf den Trainingsdaten erzeugt und gefittet\n",
    "- Diese Scaler müssen aus dem richtigen Ordner genommen werden um doie test und validieurngsdaten aus dem anderen Datensatz damit zu sclaieren und das Modell zu validieren\n",
    "- Die Scalierten Vorhersagen müssen dann zurckscaliert werden um die MAE Werte zu bestimmen\n",
    "- Die Scaler auf den Validierungsdatensätzen sind nutzlos und werden nicht verwendet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training von TradaBoostR2 auf den 5 Datensätzen: build\\window_split\\experiment-data-subset_used\n",
    "\n",
    "ACHTUNG: FEM Daten sind jetzt auch abhängig von Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_Directory=\"build\\\\window_split\\\\experiment-data-subset_used\"\n",
    "subfolder_paths_realdaten = [\n",
    "    os.path.join(upper_Directory, name) +'\\\\'\n",
    "    for name in os.listdir(upper_Directory)\n",
    "    if os.path.isdir(os.path.join(upper_Directory, name))\n",
    "]\n",
    "print(subfolder_paths_realdaten)\n",
    "upper_Directory=\"build\\\\window_split\\\\sim_data_CORAL_preprocessed_offset\"\n",
    "subfolder_paths_FEM = [\n",
    "    os.path.join(upper_Directory, name) +'\\\\'\n",
    "    for name in os.listdir(upper_Directory)\n",
    "    if os.path.isdir(os.path.join(upper_Directory, name))\n",
    "]\n",
    "print(subfolder_paths_FEM)\n",
    "\n",
    "#Ordner für Modelle erstellen\n",
    "base_dir = \"build\\\\tradaboost_model\\\\training_CORAL_03_Daten01Bis05\"  # <-- hier anpassen\n",
    "start_index = 1\n",
    "end_index = 5\n",
    "prefix = \"tarining_CORAL_03\"\n",
    "speicherort_modell=[]\n",
    "for i in range(start_index, end_index + 1):\n",
    "    folder_name = f\"{prefix}_{i:02d}\"\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"✅ Erstellt: {folder_path}\")\n",
    "        speicherort_modell.append(folder_path+\"\\\\\")\n",
    "    else:\n",
    "        print(f\"⏭️ Übersprungen (existiert schon): {folder_path}\")\n",
    "    speicherort_modell.append(folder_path+\"\\\\\")\n",
    "print(\"\")\n",
    "print(speicherort_modell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"assets\\\\models\\\\untrained\\\\sim_data_preprocessed\\\\1743665223\\\\best-model.h5\"\n",
    "best_learning_rate=0.0005026453831067469\n",
    "save_filename=\"best_model\"\n",
    "\n",
    "\n",
    "for idx,(real_daten,fem_daten,speicherort) in enumerate(zip(subfolder_paths_realdaten, subfolder_paths_FEM, speicherort_modell)):\n",
    "    print(\"========================================================================\")\n",
    "    \n",
    "    print(f\"real_daten: {real_daten}\")\n",
    "    print(f\"fem_daten: {fem_daten}\")\n",
    "    print(f\"speicherort: {speicherort}\")\n",
    "    execute_tradaBoostR2_training(with_process_Doku=False,dateipfad_femDaten=fem_daten,dateipfad_realDaten=real_daten,\n",
    "                                  model_file=model_file,save_folder=speicherort,save_filename=save_filename,n_estimators_tradaBoostR2=5,learning_rate_tradaBoostR2=1,\n",
    "                                  learning_rate=best_learning_rate,epochs=10,batch_size=32,patience=3,use_scaled_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### **Evaluation**\n",
    "\n",
    "Code zum erstellen von Plots und Auswerten der Modelle\n",
    "\n",
    "Harter Code als Import der Ergebnisse von Teammitgliedern, aus Zeitgründen kein einheitlicher Standard zum Austausch von Ergebnnissdaten. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsprocess von Tradaboost bewerten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,scalers_labels, X_val, y_val,trained_on_scaled_labels=False,):\n",
    "    \"\"\"\n",
    "    TODO Harter COde, später verbessern\n",
    "    model - Das trainierte Modell, das evaluiert werden soll.\n",
    "    scaler - Scaler des Datensatzes\n",
    "    X_val - Die Eingabedaten für die Validierung, sind skaliert\n",
    "    y_val - Die Zielwerte für die Validierung, sind skaliert \n",
    "    \"\"\"\n",
    "\n",
    "    # Vorhersagen in skalierter Form, z.B. shape=(n_samples, 3)\n",
    "    predictions = model.model.predict(X_val)\n",
    "    \n",
    "    #Ergebnissdaten \n",
    "    results = {\n",
    "        \"scaled\": {},\n",
    "        \"Not scaled\": {}\n",
    "    }\n",
    "\n",
    "    def get_errors(predictions, y_val):\n",
    "        # Berechnung der Fehler in der skalierten Skala\n",
    "        error_X_scaled = np.abs(predictions[:, 0] - y_val[:, 0])\n",
    "        error_Y_scaled = np.abs(predictions[:, 1] - y_val[:, 1])\n",
    "        error_Phi_scaled = np.abs(predictions[:, 2] - y_val[:, 2])\n",
    "        \n",
    "        mae_X = np.mean(error_X_scaled)\n",
    "        mae_Y = np.mean(error_Y_scaled)\n",
    "        mae_Phi = np.mean(error_Phi_scaled)\n",
    "        \n",
    "        mse_X = np.mean((predictions[:, 0] - y_val[:, 0]) ** 2)\n",
    "        mse_Y = np.mean((predictions[:, 1] - y_val[:, 1]) ** 2)\n",
    "        mse_Phi = np.mean((predictions[:, 2] - y_val[:, 2]) ** 2)\n",
    "        \n",
    "        rmse_X = np.sqrt(mse_X)\n",
    "        rmse_Y = np.sqrt(mse_Y)\n",
    "        rmse_Phi = np.sqrt(mse_Phi)\n",
    "\n",
    "        error={'Metrik': ['MAE', 'MSE', 'RMSE'],\n",
    "            'Verstellweg_X': [mae_X, mse_X, rmse_X],\n",
    "            'Verstellweg_Y': [mae_Y, mse_Y, rmse_Y],\n",
    "            'Verstellweg_Phi': [mae_Phi, mse_Phi, rmse_Phi]\n",
    "        }\n",
    "        return error\n",
    "    \n",
    "    #Fallunterscheidung ob Skalierung der Labels vorliegt oder nicht\n",
    "    if trained_on_scaled_labels:\n",
    "        # Rücktransformation der skalieren Vorhersagen und der Zielwerte in die Originalskala.\n",
    "        # Wir gehen davon aus, dass scaler zeilenweise angewendet wird:\n",
    "        #Predictions zurückskalieren\n",
    "        xy_scaled = predictions[:, :2]\n",
    "        phi_scaled = predictions[:, 2:3]\n",
    "        xy_original = scalers_labels[0].inverse_transform(xy_scaled)\n",
    "        phi_original = scalers_labels[1].inverse_transform(phi_scaled)\n",
    "        predictions_original = np.hstack([xy_original, phi_original])\n",
    "\n",
    "        #Labels zurückskalieren\n",
    "        y_xy_scaled = y_val[:, :2]\n",
    "        y_phi_scaled = y_val[:, 2:3]\n",
    "        y_xy_original = scalers_labels[0].inverse_transform(y_xy_scaled)\n",
    "        y_phi_original = scalers_labels[1].inverse_transform(y_phi_scaled)\n",
    "        y_val_original = np.hstack([y_xy_original, y_phi_original])\n",
    "\n",
    "        #Fehler der zurückscalierten Daten, Fehler in Originalskala\n",
    "        results[\"scaled\"] = get_errors(predictions, y_val)\n",
    "        results[\"Not scaled\"] = get_errors(predictions_original, y_val_original)\n",
    "    else:\n",
    "        results[\"scaled\"] ={}\n",
    "        results[\"Not scaled\"] = get_errors(predictions, y_val)\n",
    "\n",
    "    return results\n",
    "\n",
    "@register_keras_serializable(package=\"custom_layers\")# Hier zuvor den Import bei der Klasse ausführen (sihe oben)\n",
    "def process_all_results_Alt(base_dir):\n",
    "    \"\"\"\n",
    "    Durchläuft alle Unterordner im angegebenen Basisverzeichnis und führt die Evaluierung durch.\n",
    "    Für jeden Unterordner wird die JSON-Datei und das Modell geladen, und die Evaluierung wird durchgeführt.\n",
    "    Die Ergebnisse werden in einer Liste gespeichert und zurückgegeben.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []  # Liste zur Speicherung aller Ergebnisse\n",
    "    # Durchlaufe alle Einträge im Basisverzeichnis\n",
    "    for entry in os.listdir(base_dir):\n",
    "        subfolder = os.path.join(base_dir, entry)\n",
    "        print(\"==================================================\")\n",
    "        print(f\"Subfolder: {subfolder}\")\n",
    "        if os.path.isdir(subfolder):\n",
    "            # Pfade zu den benötigten Dateien im aktuellen Unterordner\n",
    "            json_file = os.path.join(subfolder, \"tradaBoostR2_training_BasisParameter.json\")\n",
    "            model_file = os.path.join(subfolder, \"best_model.h5\")\n",
    "            \n",
    "            # Prüfe, ob die erforderlichen Dateien existieren\n",
    "            if os.path.exists(json_file) and os.path.exists(model_file):\n",
    "                # Einlesen der JSON-Datei\n",
    "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    params = json.load(f)\n",
    "\n",
    "                # Einlesen des Modells\n",
    "                print(f\"Einlesen des Modells: {model_file}\")\n",
    "                model = CNN.from_file(model_file)\n",
    "                \n",
    "                # Hole den Pfad zu den Testdaten aus dem JSON (im Feld \"Speicherorte\" -> \"Realdaten\")\n",
    "                # Achte darauf, dass der Pfad eventuell relativ oder absolut vorliegt.\n",
    "                real_data_path = params.get(\"Speicherorte\", {}).get(\"Realdaten\", None)\n",
    "                dimension_trainingdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_source_scaled\", None)\n",
    "                dimension_validationdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_source_scaled_Test\", None)\n",
    "                dimension_realdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_target_scaled\", None)\n",
    "                \n",
    "                #Prüfe, ob auf scaled Labels trainiert wurde\n",
    "                labels_pfad=params.get(\"Daten\",{}).get(\"y_source_scaled\", None)\n",
    "                trained_on_scaled_labels = labels_pfad.endswith(\"-scaled.npy\")\n",
    "\n",
    "\n",
    "                if real_data_path is None:\n",
    "                    print(f\"Warnung: Kein 'Realdaten'-Pfad in {json_file} gefunden.\")\n",
    "                    continue\n",
    "\n",
    "                # Erstelle die Dateipfade zu den Testdaten\n",
    "                x_test_file = os.path.join(real_data_path, \"x-test-scaled.npy\")\n",
    "                y_test_file = os.path.join(real_data_path, \"y-test-scaled.npy\")\n",
    "                #Scaler einlesen\n",
    "                with open(os.path.join(real_data_path, \"scalers_labels.pkl\"), \"rb\") as f:\n",
    "                    scalers_labels = pickle.load(f)\n",
    "                \n",
    "                # Einlesen der Testdaten\n",
    "                if os.path.exists(x_test_file) and os.path.exists(y_test_file):\n",
    "                    # Annahme: NPY.from_file gibt ein Objekt zurück, dessen .array das eigentliche NumPy-Array ist.\n",
    "                    X_target_val_scaled = NPY.from_file(x_test_file).array\n",
    "                    y_target_val_scaled = NPY.from_file(y_test_file).array\n",
    "\n",
    "                    # Evaluieren des Modells auf den Target-Validierungsdaten\n",
    "                    evaluation = eval_model(model,trained_on_scaled_labels=True,scalers_labels=scalers_labels, X_val=X_target_val_scaled, y_val=np.squeeze(y_target_val_scaled))\n",
    "                else:\n",
    "                    print(f\"Testdaten nicht gefunden in: {real_data_path}\")\n",
    "                    evaluation = None\n",
    "\n",
    "                # Speichern der Ergebnisse in einem Dictionary\n",
    "                result_entry = {\n",
    "                    \"subfolder\": entry,         # Name des Unterordners\n",
    "                    #\"parameters\": params,       # Ausgelesene Parameter aus der JSON-Datei\n",
    "                    \"Anzahl Trainingsdaten\": dimension_trainingdaten[0], # Anzahl der Trainingsdaten\n",
    "                    \"Anzahl Validierungsdaten\": dimension_validationdaten[0], # Anzahl der Validierungsdaten\n",
    "                    \"Anzahl Realdaten\": dimension_realdaten[0], # Anzahl der Realdaten\n",
    "                    \"evaluation\": evaluation,   # Ergebnis der Evaluierung\n",
    "                }\n",
    "                \n",
    "                results.append(result_entry)\n",
    "            else:\n",
    "                print(f\"Erforderliche Dateien fehlen in: {subfolder}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "@register_keras_serializable(package=\"custom_layers\")# Hier zuvor den Import bei der Klasse ausführen (sihe oben)\n",
    "def process_all_results(base_dir,trained_on_scaled_Labels=None,tradaboost_model=None):\n",
    "    \"\"\"\n",
    "    Durchläuft alle Unterordner im angegebenen Basisverzeichnis und führt die Evaluierung durch.\n",
    "    Für jeden Unterordner wird die JSON-Datei und das Modell geladen, und die Evaluierung wird durchgeführt.\n",
    "    Die Ergebnisse werden in einer Liste gespeichert und zurückgegeben.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []  # Liste zur Speicherung aller Ergebnisse\n",
    "    # Durchlaufe alle Einträge im Basisverzeichnis\n",
    "    for entry in os.listdir(base_dir):\n",
    "        subfolder = os.path.join(base_dir, entry)\n",
    "        print(\"==================================================\")\n",
    "        print(f\"Subfolder: {subfolder}\")\n",
    "        if os.path.isdir(subfolder):\n",
    "            # Pfade zu den benötigten Dateien im aktuellen Unterordner\n",
    "            json_file = os.path.join(subfolder, \"tradaBoostR2_training_BasisParameter.json\")\n",
    "            model_file = os.path.join(subfolder, \"best_model.h5\")\n",
    "            \n",
    "            # Prüfe, ob die erforderlichen Dateien existieren\n",
    "            if os.path.exists(json_file) and os.path.exists(model_file):\n",
    "                # Einlesen der JSON-Datei\n",
    "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    params = json.load(f)\n",
    "\n",
    "                # Einlesen des Modells\n",
    "                print(f\"Einlesen des Modells: {model_file}\")\n",
    "                model = CNN.from_file(model_file)\n",
    "                \n",
    "                #Fallunterscheidung ob auf skalierten Labels trainiert wurde oder nicht\n",
    "                #Es wurde keine Angabe beim AUfruf gemacht, also über Dokumentation überprüfen\n",
    "                if trained_on_scaled_Labels==None:\n",
    "                    #Prüfe, ob auf scaled Labels trainiert wurde\n",
    "                    labels_pfad=params.get(\"Daten\",{}).get(\"y_source_scaled\", None)\n",
    "                    trained_on_scaled_Labels = labels_pfad.endswith(\"-scaled.npy\")\n",
    "                # else heißt, dass es zuvor sinnvol gesetzt wurde und nicht angepasst werden muss\n",
    "                #-> trained_on_scaled_Labels stimmt nun\n",
    "\n",
    "                #Scaler einlesen, immer machen, weil sinnvoll für Kontrolle\n",
    "                if tradaboost_model==None or tradaboost_model==True:\n",
    "                    real_data_path = params.get(\"Speicherorte\", {}).get(\"Realdaten\", None)\n",
    "                elif tradaboost_model==False:\n",
    "                    real_data_path = params.get(\"Speicherorte\", {}).get(\"Daten\", None)\n",
    "                print(f\"Einlesen der Scaler: {real_data_path}\")\n",
    "                with open(os.path.join(real_data_path, \"scalers_labels.pkl\"), \"rb\") as f:\n",
    "                    scalers_labels = pickle.load(f)\n",
    "\n",
    "                #Fallunterscheidung um Daten für Evaluation zu laden\n",
    "                if trained_on_scaled_Labels:\n",
    "                    x_test_file = os.path.join(real_data_path, \"x-test-scaled.npy\")\n",
    "                    y_test_file = os.path.join(real_data_path, \"y-test-scaled.npy\")\n",
    "                else:\n",
    "                    x_test_file = os.path.join(real_data_path, \"x-test-scaled.npy\")\n",
    "                    y_test_file = os.path.join(real_data_path, \"y-test.npy\")\n",
    "\n",
    "                #Evaluieren des Modells\n",
    "                if os.path.exists(x_test_file) and os.path.exists(y_test_file):\n",
    "                    # Annahme: NPY.from_file gibt ein Objekt zurück, dessen .array das eigentliche NumPy-Array ist.\n",
    "                    X_target_val_scaled = NPY.from_file(x_test_file).array\n",
    "                    \n",
    "                    y_target_val_scaled = NPY.from_file(y_test_file).array\n",
    "                    # Evaluieren des Modells auf den Target-Validierungsdaten\n",
    "                    evaluation = eval_model(model,trained_on_scaled_labels=trained_on_scaled_Labels,scalers_labels=scalers_labels, \n",
    "                                            X_val=X_target_val_scaled, y_val=np.squeeze(y_target_val_scaled))\n",
    "                else:\n",
    "                    print(f\"Testdaten nicht gefunden in: {real_data_path}\")\n",
    "                    evaluation = None\n",
    "\n",
    "                if tradaboost_model==True:\n",
    "                    #real_data_path = params.get(\"Speicherorte\", {}).get(\"Realdaten\", None)\n",
    "                    dimension_trainingdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_source_scaled\", None)\n",
    "                    dimension_validationdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_source_scaled_Test\", None)\n",
    "                    dimension_realdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_target_scaled\", None)\n",
    "                if tradaboost_model==False:\n",
    "                    #real_data_path = params.get(\"Speicherorte\", {}).get(\"Daten\", None)\n",
    "                    dimension_trainingdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_scaled\", None)\n",
    "                    dimension_validationdaten= params.get(\"Dimensionen der Daten\", {}).get(\"X_scaled_Test\", None)\n",
    "                    dimension_realdaten= [\"Kein Tradaboost Modell, diese Feld hat hier keine Bedeutung\"]\n",
    "\n",
    "                result_entry = {\n",
    "                    \"subfolder\": entry,         # Name des Unterordners\n",
    "                    #\"parameters\": params,       # Ausgelesene Parameter aus der JSON-Datei\n",
    "                    \"Anzahl Trainingsdaten\": dimension_trainingdaten[0], # Anzahl der Trainingsdaten\n",
    "                    \"Anzahl Validierungsdaten\": dimension_validationdaten[0], # Anzahl der Validierungsdaten\n",
    "                    \"Anzahl Realdaten\": dimension_realdaten[0], # Anzahl der Realdaten\n",
    "                    \"evaluation\": evaluation,   # Ergebnis der Evaluierung\n",
    "                }\n",
    "                #TODO hier abpeichern von result_entry.json in dem Ordner subfolder\n",
    "\n",
    "                result_entry_path = os.path.join(subfolder, \"result_entry.json\")\n",
    "                with open(result_entry_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(result_entry, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "                \n",
    "                results.append(result_entry)\n",
    "            else:\n",
    "                print(f\"Erforderliche Dateien fehlen in: {subfolder}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_model_on_splits(model_path, data_super_folder, trained_on_scaled_labels, evaluate_scaled_errors):\n",
    "    results = []\n",
    "\n",
    "    # Modell laden\n",
    "    #model = tf.keras.models.load_model(model_path)\n",
    "    model=CNN.from_file(model_path)\n",
    "    # Alle Unterordner durchlaufen\n",
    "    for entry in os.listdir(data_super_folder):\n",
    "        entry_path = os.path.join(data_super_folder, entry)\n",
    "        if not os.path.isdir(entry_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Bearbeite: {entry}\")\n",
    "\n",
    "        # Scaler laden\n",
    "        with open(os.path.join(entry_path, \"scalers_labels.pkl\"), \"rb\") as f:\n",
    "            scaler_labels = pickle.load(f)\n",
    "        \n",
    "        # Daten laden\n",
    "        suffix = \"-scaled.npy\" if trained_on_scaled_labels else \".npy\"\n",
    "        #x_val = np.load(os.path.join(entry_path,\"x-test-scaled.npy\"))\n",
    "        x_val=NPY.from_file(os.path.join(entry_path,\"x-test-scaled.npy\")).array\n",
    "        #y_val = np.load(os.path.join(entry_path, f\"y-test{suffix}\"))\n",
    "        y_val = NPY.from_file(os.path.join(entry_path, f\"y-test{suffix}\")).array\n",
    "                    \n",
    "        # Fehler berechnen\n",
    "        evaluation = eval_model(model=model,scalers_labels=scaler_labels,X_val=x_val, y_val=y_val,trained_on_scaled_labels=trained_on_scaled_labels)\n",
    "\n",
    "        # Datensatzgrößen bestimmen\n",
    "        dimension_validationdaten = x_val.shape\n",
    "        dimension_realdaten = np.load(os.path.join(entry_path, f\"x_real{suffix}\")).shape\n",
    "\n",
    "        # Ergebnisse speichern\n",
    "        result_entry = {\n",
    "            \"subfolder\": entry,\n",
    "            \"Anzahl Trainingsdaten\": 0,\n",
    "            \"Anzahl Validierungsdaten\": dimension_validationdaten[0],\n",
    "            \"Anzahl Realdaten\": 0,\n",
    "            \"evaluation\": evaluation,\n",
    "        }\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    return results\n",
    "\n",
    "def collect_tradaboost_stats(base_dir: str, \n",
    "                             prozent_ordner: list[str] = ['5', '10', '15', '20', '50'],\n",
    "                             output_filename: str = 'ergebnisse.txt'):\n",
    "    \"\"\"\n",
    "    Liest in base_dir die Unterordner 5,10,15,20,50 ein, extrahiert aus jedem\n",
    "    'dataframe.json' die mean_absolute_error-Werte für X, Y, Phi, berechnet\n",
    "    Mittelwert und Standardabweichung (je 20 Versuche) und schreibt alles in\n",
    "    output_filename im base_dir.\n",
    "    \"\"\"\n",
    "    # Initialisiere Ergebnis-Container\n",
    "    ergebnisse = {\n",
    "        'Verstellweg_X': {'Mittelwerte': [], 'Standardabweichungen': []},\n",
    "        'Verstellweg_Y': {'Mittelwerte': [], 'Standardabweichungen': []},\n",
    "        'Verstellweg_Phi': {'Mittelwerte': [], 'Standardabweichungen': []}\n",
    "    }\n",
    "\n",
    "    # Durchlaufe die Prozent-Unterordner\n",
    "    for ordner in prozent_ordner:\n",
    "        ordner_pfad = os.path.join(base_dir, ordner)\n",
    "        json_pfad = os.path.join(ordner_pfad, 'dataframe.json')\n",
    "        if not os.path.isfile(json_pfad):\n",
    "            print(f\"[Warnung] dataframe.json nicht gefunden in {ordner_pfad}\")\n",
    "            # Fülle Platzhalter, damit Längen konsistent bleiben\n",
    "            for key in ergebnisse:\n",
    "                ergebnisse[key]['Mittelwerte'].append(float('nan'))\n",
    "                ergebnisse[key]['Standardabweichungen'].append(float('nan'))\n",
    "            continue\n",
    "\n",
    "        with open(json_pfad, 'r', encoding='utf-8') as f:\n",
    "            daten = json.load(f)\n",
    "\n",
    "        # Für jeden Stellweg X, Y, Phi\n",
    "        for key in ergebnisse:\n",
    "            werte = list(daten[f\"{key}_mean_absolute_error\"].values())\n",
    "            mean = float(np.mean(werte))\n",
    "            std  = float(np.std(werte))\n",
    "            ergebnisse[key]['Mittelwerte'].append(mean)\n",
    "            ergebnisse[key]['Standardabweichungen'].append(std)\n",
    "\n",
    "    # Schreibe Ausgabe\n",
    "    out_path = os.path.join(base_dir, output_filename)\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"TrAdaBoostR2 Modell: Labels gescaled, trainiert auf Modellstruktur getuned auf 100% Sim Daten, Grundlage N=20 Versuche\\n\\n\")\n",
    "        f.write(f\"Prozente = {prozent_ordner}\\n\\n\")\n",
    "        for key, data in ergebnisse.items():\n",
    "            f.write(f\"{key}\\n\")\n",
    "            f.write(f\"Mittelwerte           = {np.round(data['Mittelwerte'], 6).tolist()}\\n\")\n",
    "            f.write(f\"Standardabweichung = {np.round(data['Standardabweichungen'], 6).tolist()}\\n\\n\")\n",
    "\n",
    "    print(f\"Ergebnisse geschrieben nach: {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plottable_data(ergebnissliste):\n",
    "    #Ergebnisse in Liste umwandeln\n",
    "    aufbereitete_ergebnisse = []\n",
    "    for eintrag in ergebnissliste:\n",
    "        anzahl_realdaten = eintrag.get(\"Anzahl Realdaten\", None)\n",
    "        fehler_evaluation = eintrag.get(\"evaluation\", None)\n",
    "        if anzahl_realdaten is not None and fehler_evaluation is not None:\n",
    "            aufbereitete_ergebnisse.append({\n",
    "                \"Anzahl Realdaten\": anzahl_realdaten,\n",
    "                \"evaluation\": fehler_evaluation\n",
    "            })\n",
    "    aufbereitete_ergebnisse = sorted(aufbereitete_ergebnisse, key=lambda x: x[\"Anzahl Realdaten\"])\n",
    "    return aufbereitete_ergebnisse\n",
    "    # Plotten der Ergebnisse    \n",
    "    \n",
    "def plotten_TradaBoostR2_Ergebnisse(ergebnissliste,marker,color,ergebnisart=\"Not scaled\",fig=None,axes=None,overwrite_x_tics=None,label=None):\n",
    "    metrics = ['MAE', 'MSE', 'RMSE']\n",
    "    verstellweg_labels = ['Verstellweg_X', 'Verstellweg_Y', 'Verstellweg_Phi']\n",
    "    if fig is None or axes is None:\n",
    "        print(\"Erstelle neue Figur und Achsen\")\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Durchlaufe die Metriken und Verstellwege\n",
    "    first=True\n",
    "    for row, verstellweg in enumerate(verstellweg_labels):\n",
    "        for col, metric in enumerate(metrics):\n",
    "            # Extrahiere die Daten für die aktuelle Metrik und den Verstellweg\n",
    "            for training,x_tick in zip(ergebnissliste,overwrite_x_tics):\n",
    "                anzahl_realdaten = training[\"Anzahl Realdaten\"]\n",
    "                errors = training[\"evaluation\"][ergebnisart]\n",
    "                # Plotten\n",
    "                ax = axes[row, col]\n",
    "                if overwrite_x_tics is None:\n",
    "                    if first:\n",
    "                        ax.plot(anzahl_realdaten, errors[verstellweg][col], marker=marker,color=color, label=label)\n",
    "                        first=False\n",
    "                    else:\n",
    "                        ax.plot(anzahl_realdaten, errors[verstellweg][col], marker=marker,color=color)\n",
    "                else:\n",
    "                    if first:\n",
    "                        ax.plot(x_tick, errors[verstellweg][col], marker=marker, label=label,color=color)\n",
    "                        first=False\n",
    "                    else:\n",
    "                        ax.plot(x_tick, errors[verstellweg][col], marker=marker, color=color)\n",
    "                ax.set_title(f\"{verstellweg} - {metric}\")\n",
    "                ax.set_xlabel('Anteil der Realdaten in %')\n",
    "                ax.set_ylabel('Fehler')\n",
    "                ax.grid(True)\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right')  # Oder eine andere Position\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    return fig, axes\n",
    "\n",
    "def plotten_TradaBoostR2_Ergebnisse_MAE(ergebnissliste, marker, color, ergebnisart=\"Not scaled\", fig=None, axes=None, overwrite_x_tics=None, label=None):\n",
    "    verstellweg_labels = ['Verstellweg_X', 'Verstellweg_Y', 'Verstellweg_Phi']\n",
    "    \n",
    "    if fig is None or axes is None:\n",
    "        print(\"Erstelle neue Figur und Achsen\")\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # 1 Zeile, 3 Spalten\n",
    "    \n",
    "    first = True\n",
    "    for idx, verstellweg in enumerate(verstellweg_labels):\n",
    "        ax = axes[idx]\n",
    "        for training, x_tick in zip(ergebnissliste, overwrite_x_tics):\n",
    "            anzahl_realdaten = training[\"Anzahl Realdaten\"]\n",
    "            errors = training[\"evaluation\"][ergebnisart]\n",
    "            mae = errors[verstellweg][0]  # MAE liegt an Position 0 im Fehler-Array\n",
    "            \n",
    "            # X-Achsen-Werte bestimmen\n",
    "            x_values = x_tick if overwrite_x_tics is not None else anzahl_realdaten\n",
    "            \n",
    "            # Plotten\n",
    "            if first and label is not None:\n",
    "                ax.scatter(x_values, mae, marker=marker, color=color, label=label)\n",
    "                first = False\n",
    "            else:\n",
    "                ax.scatter(x_values, mae, marker=marker, color=color)\n",
    "        \n",
    "        ax.set_title(f\"{verstellweg} - MAE\")\n",
    "        ax.set_xlabel('Anteil der Realdaten in %')\n",
    "        ax.set_ylabel('MAE')\n",
    "        ax.grid(True)\n",
    "        #if label is not None:\n",
    "        #    ax.legend()\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def plotten_Boxplots_MAE_ALT(averages_path, std_path, dataframe_path,label, color,fig=None, axes=None, x_position=80,show_lines=True,show_box_plot=True):\n",
    "    \"\"\"\n",
    "    Fügt MAE-Boxplots für Verstellweg_X, Verstellweg_Y und Verstellweg_Phi an gegebene x-Positionen ein.\n",
    "    \n",
    "    Parameters:\n",
    "        averages_path (str): Pfad zur averages.json\n",
    "        std_path (str): Pfad zur std.json\n",
    "        dataframe_path (str): Pfad zur dataframe.json\n",
    "        fig (matplotlib.figure.Figure): Vorhandene Figure\n",
    "        axes (array of matplotlib.axes.Axes): Vorhandene Achsen\n",
    "        x_position (float or int): Die x-Position für den Boxplot\n",
    "    \n",
    "    Returns:\n",
    "        fig, axes: Die übergebene bzw. erzeugte Figure und Achsen\n",
    "    \"\"\"\n",
    "    # JSON-Dateien laden\n",
    "    with open(averages_path, 'r') as f:\n",
    "        averages = json.load(f)\n",
    "    with open(std_path, 'r') as f:\n",
    "        stds = json.load(f)\n",
    "    with open(dataframe_path, 'r') as f:\n",
    "        dataframe = json.load(f)\n",
    "\n",
    "    # Verstellwege und zugehörige Achsen\n",
    "    verstellwege = ['Verstellweg_X', 'Verstellweg_Y', 'Verstellweg_Phi']\n",
    "\n",
    "    # Pro Verstellweg den Boxplot erzeugen\n",
    "    for i, verstellweg in enumerate(verstellwege):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Mittelwert und Standardabweichung\n",
    "        mean = averages[f\"{verstellweg}_mean_absolute_error\"]\n",
    "        std = stds[f\"{verstellweg}_mean_absolute_error\"]\n",
    "\n",
    "        # Einzelwerte aus dem DataFrame\n",
    "        werte = []\n",
    "        for k in dataframe[f\"{verstellweg}_mean_absolute_error\"]:\n",
    "            werte.append(dataframe[f\"{verstellweg}_mean_absolute_error\"][k])\n",
    "\n",
    "        # Boxplot bei der angegebenen x-Position\n",
    "        if show_box_plot:\n",
    "            ax.boxplot(werte, positions=[x_position], widths=3, vert=True, patch_artist=True,\n",
    "                   boxprops=dict(facecolor='lightblue'))\n",
    "\n",
    "        if show_lines:\n",
    "            ax.axhline(mean, color=color, linestyle='--', label=f\"{label} +/- Std.\" )\n",
    "            ax.axhline(mean + std, color=color, linestyle=':', label=None)\n",
    "            ax.axhline(mean - std, color=color, linestyle=':', label=None)\n",
    "        ax.grid(True)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "def plotten_Boxplots_MAE(averages=None, stds=None, dataframe=None,averages_path=None, std_path=None, dataframe_path=None,\n",
    "                          label=\"\", color=\"blue\",fig=None, axes=None, x_position=80,show_lines=True, show_box_plot=True):\n",
    "    \"\"\"\n",
    "    Fügt MAE-Boxplots für Verstellweg_X, Verstellweg_Y und Verstellweg_Phi an gegebene x-Positionen ein.\n",
    "    \n",
    "    Entweder übergibst du die averages/stds/dataframe direkt als Dicts ODER die Pfade zu den JSON-Dateien.\n",
    "\n",
    "    Parameters:\n",
    "        averages (dict, optional): Dict mit Mittelwerten\n",
    "        stds (dict, optional): Dict mit Standardabweichungen\n",
    "        dataframe (dict, optional): Dict mit Einzelwerten\n",
    "        averages_path (str, optional): Pfad zur averages.json\n",
    "        std_path (str, optional): Pfad zur std.json\n",
    "        dataframe_path (str, optional): Pfad zur dataframe.json\n",
    "        label (str): Beschriftung für die Legende\n",
    "        color (str): Farbe für Linien und Boxen\n",
    "        fig (matplotlib.figure.Figure, optional): Vorhandene Figure\n",
    "        axes (array of matplotlib.axes.Axes, optional): Vorhandene Achsen\n",
    "        x_position (float or int): Die x-Position für den Boxplot\n",
    "        show_lines (bool): Linien für Mittelwert und Std anzeigen\n",
    "        show_box_plot (bool): Boxplots anzeigen\n",
    "\n",
    "    Returns:\n",
    "        fig, axes: Die übergebene bzw. erzeugte Figure und Achsen\n",
    "    \"\"\"\n",
    "\n",
    "    # Falls keine Dicts übergeben → aus JSON laden\n",
    "    if averages is None and averages_path:\n",
    "        with open(averages_path, 'r') as f:\n",
    "            averages = json.load(f)\n",
    "    if stds is None and std_path:\n",
    "        with open(std_path, 'r') as f:\n",
    "            stds = json.load(f)\n",
    "    if dataframe is None and dataframe_path:\n",
    "        with open(dataframe_path, 'r') as f:\n",
    "            dataframe = json.load(f)\n",
    "\n",
    "    if averages is None or stds is None or dataframe is None:\n",
    "        raise ValueError(\"Entweder alle Pfade ODER alle Dictionaries müssen angegeben werden!\")\n",
    "\n",
    "    # Verstellwege und zugehörige Achsen\n",
    "    verstellwege = ['Verstellweg_X', 'Verstellweg_Y', 'Verstellweg_Phi']\n",
    "\n",
    "    # Falls keine Achsen übergeben → neu erzeugen\n",
    "    if fig is None or axes is None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Pro Verstellweg den Boxplot erzeugen\n",
    "    for i, verstellweg in enumerate(verstellwege):\n",
    "        ax = axes[i]\n",
    "        print(verstellweg)\n",
    "        # Mittelwert und Standardabweichung\n",
    "        mean = averages[f\"{verstellweg}_mean_absolute_error\"]\n",
    "        std = stds[f\"{verstellweg}_mean_absolute_error\"]\n",
    "\n",
    "        # Einzelwerte aus dem DataFrame\n",
    "        werte = list(dataframe[f\"{verstellweg}_mean_absolute_error\"].values())\n",
    "\n",
    "        # Boxplot bei der angegebenen x-Position\n",
    "        if show_box_plot:\n",
    "            ax.boxplot(werte, positions=[x_position], widths=3, vert=True, patch_artist=True,\n",
    "                       boxprops=dict(facecolor='lightblue'))\n",
    "        print(f\"Mean {mean}\")\n",
    "        print(f\"fill between upper: {mean + std}\")\n",
    "        print(f\"fill between lower: {mean - std}\")\n",
    "        if show_lines:\n",
    "            ax.axhline(mean, color=color, linestyle='--', label=f\"{label} +/- Std.\")\n",
    "            ax.axhline(mean + std, color=color, linestyle=':', label=None)\n",
    "            ax.axhline(mean - std, color=color, linestyle=':', label=None)\n",
    "        ax.grid(True)\n",
    "        ax.set_title(verstellweg)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "def box_plot_based_on_MAE_Values(mae_werte_X,mae_werte_Y,mae_werte_Phi,x_position,label=\"\", color=\"blue\",facecolor='lightblue',fig=None, axes=None, show_lines=True, show_box_plot=True):\n",
    "    verstellwege = ['Verstellweg_X', 'Verstellweg_Y', 'Verstellweg_Phi']\n",
    "\n",
    "    # Falls keine Achsen übergeben → neu erzeugen\n",
    "    if fig is None or axes is None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Pro Verstellweg den Boxplot erzeugen\n",
    "    for i, (verstellweg,mae_werte) in enumerate(zip(verstellwege,[mae_werte_X,mae_werte_Y,mae_werte_Phi])):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Mittelwert und Standardabweichung\n",
    "        mean = np.mean(mae_werte)\n",
    "        std = np.std(mae_werte)\n",
    "\n",
    "        # Boxplot bei der angegebenen x-Position\n",
    "        if show_box_plot:\n",
    "            ax.boxplot(mae_werte, positions=[x_position], widths=3, vert=True, patch_artist=True,\n",
    "                       boxprops=dict(facecolor=facecolor))\n",
    "\n",
    "        if show_lines:\n",
    "            ax.axhline(mean, color=color, linestyle='--', label=f\"{label} +/- Std.\")\n",
    "            ax.axhline(mean + std, color=color, linestyle=':', label=None)\n",
    "            ax.axhline(mean - std, color=color, linestyle=':', label=None)\n",
    "        ax.grid(True)\n",
    "        ax.set_title(verstellweg)\n",
    "    return fig, axes\n",
    "\n",
    "def auswertung_mae_kombiniert(ober_ordner, unterordner_list):\n",
    "    \"\"\"\n",
    "    Lädt aus dem angegebenen ober_ordner und einer Liste weiterer Ordnerpfade jeweils die result_entry.json \n",
    "    und berechnet den Mittelwert und die Standardabweichung des MAE (Mean Absolute Error)\n",
    "    aus dem Feld 'evaluation' → 'Not scaled' → 'Verstellweg_X'.\n",
    "\n",
    "    Parameter:\n",
    "        ober_ordner (str): Ordnerpfad, in dem mehrere Unterordner mit result_entry.json liegen.\n",
    "        unterordner_list (list of str): Weitere Ordnerpfade mit result_entry.json.\n",
    "\n",
    "    Rückgabe:\n",
    "        tuple: (Mittelwert, Standardabweichung) des MAE über alle gefundenen JSON-Dateien.\n",
    "    \"\"\"\n",
    "    \n",
    "    mae_werte_X = []\n",
    "    mae_werte_Y=[]\n",
    "    mae_werte_Phi=[]\n",
    "    # 1. Ergebnisse aus ober_ordner\n",
    "    for entry in os.listdir(ober_ordner):\n",
    "        subfolder = os.path.join(ober_ordner, entry)\n",
    "        json_pfad = os.path.join(subfolder, \"result_entry.json\")\n",
    "        if os.path.exists(json_pfad):\n",
    "            mae_wert_X,mae_wert_Y,mae_wert_Phi = lese_mae(json_pfad)\n",
    "            if mae_wert_X is not None and mae_wert_Y is not None and mae_wert_Phi is not None :\n",
    "                mae_werte_X.append(mae_wert_X)\n",
    "                mae_werte_Y.append(mae_wert_Y)\n",
    "                mae_werte_Phi.append(mae_wert_Phi)    \n",
    "        else:\n",
    "            print(f\"Keine result_entry.json gefunden in: {json_pfad}\")\n",
    "\n",
    "    # 2. Ergebnisse aus unterordner_list\n",
    "    for ordner in unterordner_list:\n",
    "        json_pfad = os.path.join(ordner, \"result_entry.json\")\n",
    "        if os.path.exists(json_pfad):\n",
    "            mae_wert_X,mae_wert_Y,mae_wert_Phi = lese_mae(json_pfad)\n",
    "            if mae_wert_X is not None and mae_wert_Y is not None and mae_wert_Phi is not None :\n",
    "                mae_werte_X.append(mae_wert_X)\n",
    "                mae_werte_Y.append(mae_wert_Y)\n",
    "                mae_werte_Phi.append(mae_wert_Phi)\n",
    "        else:\n",
    "            print(f\"Keine result_entry.json gefunden in: {json_pfad}\")\n",
    "\n",
    "    if len(mae_werte_X) ==0 or len(mae_werte_Y) ==0 or len(mae_werte_Phi) == 0:\n",
    "        print(\"Keine MAE-Werte gefunden.\")\n",
    "        return None, None, None\n",
    "\n",
    "    #TODO Abspeichern der ergebnisse\n",
    "\n",
    "\n",
    "    return mae_werte_X, mae_werte_Y, mae_werte_Phi\n",
    "\n",
    "def lese_mae(json_pfad):\n",
    "    \"\"\"\n",
    "    Liest den MAE-Wert aus der angegebenen JSON-Datei.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_pfad, \"r\", encoding=\"utf-8\") as f:\n",
    "            daten = json.load(f)\n",
    "        mae_X = daten[\"evaluation\"][\"Not scaled\"][\"Verstellweg_X\"][0]  # MAE ist 1. Wert\n",
    "        mae_Y = daten[\"evaluation\"][\"Not scaled\"][\"Verstellweg_Y\"][0]  # MAE ist 1. Wert\n",
    "        mae_Phi = daten[\"evaluation\"][\"Not scaled\"][\"Verstellweg_Phi\"][0]  # MAE ist 1. Wert\n",
    "        \n",
    "        return mae_X,mae_Y,mae_Phi\n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Fehler beim Lesen von {json_pfad}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def speichere_mae_ergebnisse(mae_werte_X, mae_werte_Y, mae_werte_Phi,ausgangsordner, ziel_ordner, prozent):\n",
    "    \"\"\"\n",
    "    Speichert MAE-Metriken für Verstellweg_X, Verstellweg_Y, Verstellweg_Phi in einem Unterordner\n",
    "    für den angegebenen Prozentanteil der Trainingsdaten.\n",
    "    \n",
    "    Erzeugt:\n",
    "        - averages.json\n",
    "        - std.json\n",
    "        - dataframe.json\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mittelwerte berechnen\n",
    "    mean_x = np.mean(mae_werte_X)\n",
    "    mean_y = np.mean(mae_werte_Y)\n",
    "    mean_Phi = np.mean(mae_werte_Phi)\n",
    "\n",
    "    averages = {\n",
    "        \"Verstellweg_X_mean_absolute_error\": mean_x,\n",
    "        \"Verstellweg_Y_mean_absolute_error\": mean_y,\n",
    "        \"Verstellweg_Phi_mean_absolute_error\": mean_Phi\n",
    "    }\n",
    "\n",
    "    # Standardabweichungen berechnen\n",
    "    std_x = np.std(mae_werte_X)\n",
    "    std_y = np.std(mae_werte_Y)\n",
    "    std_Phi = np.std(mae_werte_Phi)\n",
    "\n",
    "    stds = {\n",
    "        \"Verstellweg_X_mean_absolute_error\": std_x,\n",
    "        \"Verstellweg_Y_mean_absolute_error\": std_y,\n",
    "        \"Verstellweg_Phi_mean_absolute_error\": std_Phi\n",
    "    }\n",
    "\n",
    "    # Einzelwerte als Dictionary im gewünschten Format\n",
    "    dataframe = {\n",
    "        \"Ausgangsordner Modelle\": ausgangsordner,\n",
    "        \"Verstellweg_X_mean_absolute_error\": {str(i): float(v) for i, v in enumerate(mae_werte_X)},\n",
    "        \"Verstellweg_Y_mean_absolute_error\": {str(i): float(v) for i, v in enumerate(mae_werte_Y)},\n",
    "        \"Verstellweg_Phi_mean_absolute_error\": {str(i): float(v) for i, v in enumerate(mae_werte_Phi)}\n",
    "    }\n",
    "    \n",
    "    unterordner = os.path.join(ziel_ordner, str(prozent))\n",
    "\n",
    "    # Wenn Ordner bereits existiert, hänge _1, _2, ... an\n",
    "    original_unterordner = unterordner\n",
    "    index = 1\n",
    "    while os.path.exists(unterordner):\n",
    "        unterordner = f\"{original_unterordner}_{index}\"\n",
    "        index += 1\n",
    "\n",
    "    # Ordner erstellen\n",
    "    os.makedirs(unterordner)\n",
    "    # JSON-Dateien speichern\n",
    "    with open(os.path.join(unterordner, \"averages.json\"), 'w') as f:\n",
    "        json.dump(averages, f, indent=4)\n",
    "    with open(os.path.join(unterordner, \"std.json\"), 'w') as f:\n",
    "        json.dump(stds, f, indent=4)\n",
    "    with open(os.path.join(unterordner, \"dataframe.json\"), 'w') as f:\n",
    "        json.dump(dataframe, f, indent=4)\n",
    "\n",
    "    print(f\"Ergebnisse für {prozent}% in {unterordner} gespeichert.\")\n",
    "\n",
    "def speichere_mae_ergebnisse_NEU(mae_werte_X, mae_werte_Y, mae_werte_Phi,ausgangsordner, ziel_ordner, prozent):\n",
    "    \"\"\"\n",
    "    Speichert MAE-Metriken für Verstellweg_X, Verstellweg_Y, Verstellweg_Phi in einem Unterordner\n",
    "    für den angegebenen Prozentanteil der Trainingsdaten.\n",
    "    Es werden nur ergebnisse hinzugefügt, die noch nicht vorhanden sind.\n",
    "    Erzeugt:\n",
    "        - averages.json\n",
    "        - std.json\n",
    "        - dataframe.json\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mittelwerte berechnen\n",
    "    mean_x = np.mean(mae_werte_X)\n",
    "    mean_y = np.mean(mae_werte_Y)\n",
    "    mean_Phi = np.mean(mae_werte_Phi)\n",
    "\n",
    "    averages = {\n",
    "        \"Verstellweg_X_mean_absolute_error\": mean_x,\n",
    "        \"Verstellweg_Y_mean_absolute_error\": mean_y,\n",
    "        \"Verstellweg_Phi_mean_absolute_error\": mean_Phi\n",
    "    }\n",
    "\n",
    "    # Standardabweichungen berechnen\n",
    "    std_x = np.std(mae_werte_X)\n",
    "    std_y = np.std(mae_werte_Y)\n",
    "    std_Phi = np.std(mae_werte_Phi)\n",
    "\n",
    "    stds = {\n",
    "        \"Verstellweg_X_mean_absolute_error\": std_x,\n",
    "        \"Verstellweg_Y_mean_absolute_error\": std_y,\n",
    "        \"Verstellweg_Phi_mean_absolute_error\": std_Phi\n",
    "    }\n",
    "\n",
    "    # Einzelwerte als Dictionary im gewünschten Format\n",
    "    dataframe = {\n",
    "        \"Ausgangsordner Modelle\": ausgangsordner,\n",
    "        \"Verstellweg_X_mean_absolute_error\": {str(i): float(v) for i, v in enumerate(mae_werte_X)},\n",
    "        \"Verstellweg_Y_mean_absolute_error\": {str(i): float(v) for i, v in enumerate(mae_werte_Y)},\n",
    "        \"Verstellweg_Phi_mean_absolute_error\": {str(i): float(v) for i, v in enumerate(mae_werte_Phi)}\n",
    "    }\n",
    "    \n",
    "    unterordner = os.path.join(ziel_ordner, str(prozent))\n",
    "\n",
    "    # Wenn Ordner bereits existiert, hänge _1, _2, ... an\n",
    "    original_unterordner = unterordner\n",
    "    index = 1\n",
    "    while os.path.exists(unterordner):\n",
    "        unterordner = f\"{original_unterordner}_{index}\"\n",
    "        index += 1\n",
    "\n",
    "    # Ordner erstellen\n",
    "    os.makedirs(unterordner)\n",
    "    # JSON-Dateien speichern\n",
    "    with open(os.path.join(unterordner, \"averages.json\"), 'w') as f:\n",
    "        json.dump(averages, f, indent=4)\n",
    "    with open(os.path.join(unterordner, \"std.json\"), 'w') as f:\n",
    "        json.dump(stds, f, indent=4)\n",
    "    with open(os.path.join(unterordner, \"dataframe.json\"), 'w') as f:\n",
    "        json.dump(dataframe, f, indent=4)\n",
    "\n",
    "    print(f\"Ergebnisse für {prozent}% in {unterordner} gespeichert.\")\n",
    "\n",
    "def plot_saved_mae_results(base_folder, overwrite_x_tics=None, label_prefix=\"\", color=\"blue\", fig=None, axes=None):\n",
    "    \"\"\"\n",
    "    Durchläuft alle Unterordner in base_folder, in denen jeweils die Dateien averages.json und std.json liegen,\n",
    "    und liest die MAE-Werte für Verstellweg_X, Verstellweg_Y und Verstellweg_Phi aus.\n",
    "    Dann werden in jeweils einem Subplot (3 Spalten) für jeden Verstellweg:\n",
    "      - Die Mittelwerte als Verbindungslinie und Marker\n",
    "      - Ein Fehlerband (Mittelwert ± Standardabweichung)\n",
    "    eingezeichnet.\n",
    "\n",
    "    Falls overwrite_x_tics übergeben wird, werden diese als x-Werte verwendet;\n",
    "    ansonsten wird versucht, der Unterordnername als Zahl zu interpretieren oder der Index verwendet.\n",
    "\n",
    "    Parameters:\n",
    "        base_folder (str): Pfad zum übergeordneten Ordner, der mehrere Unterordner mit averages.json und std.json enthält.\n",
    "        overwrite_x_tics (list, optional): Liste von x-Werten, die den Unterordnern zugeordnet werden.\n",
    "        label_prefix (str): Optionaler Label-Präfix für die Legende.\n",
    "        color (str): Farbe, die für Plot und Fehlerband verwendet wird.\n",
    "        fig (matplotlib.figure.Figure, optional): Vorhandene Figure.\n",
    "        axes (array-like of Axes, optional): Vorhandene Achsen (erwartet 1 Zeile, 3 Spalten).\n",
    "    \n",
    "    Returns:\n",
    "        fig, axes: Die verwendete bzw. erzeugte Figure und Achsen.\n",
    "    \"\"\"\n",
    "    measures = [\"Verstellweg_X_mean_absolute_error\",\n",
    "                \"Verstellweg_Y_mean_absolute_error\",\n",
    "                \"Verstellweg_Phi_mean_absolute_error\"]\n",
    "\n",
    "    # Wenn keine Figure/Achsen übergeben, neue erzeugen\n",
    "    if fig is None or axes is None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Ergebnisse speichern (für jede Messgröße: x-Werte, Mittelwert, Standardabweichung)\n",
    "    results_dict = {m: {\"x\": [], \"mean\": [], \"std\": []} for m in measures}\n",
    "\n",
    "    # Liste aller Unterordner in base_folder (sortiert)\n",
    "    subfolders = sorted([f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f))])\n",
    "    for idx, folder in enumerate(subfolders):\n",
    "        folder_path = os.path.join(base_folder, folder)\n",
    "        averages_path = os.path.join(folder_path, \"averages.json\")\n",
    "        std_path = os.path.join(folder_path, \"std.json\")\n",
    "\n",
    "        # Nur verarbeiten, wenn beide Dateien existieren\n",
    "        if os.path.exists(averages_path) and os.path.exists(std_path):\n",
    "            with open(averages_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                averages = json.load(f)\n",
    "            with open(std_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                stds = json.load(f)\n",
    "\n",
    "            # x-Wert bestimmen: wenn overwrite_x_tics vorhanden, benutze diese, sonst versuche den Ordnernamen zu parsen\n",
    "            if overwrite_x_tics is not None and idx < len(overwrite_x_tics):\n",
    "                x_val = overwrite_x_tics[idx]\n",
    "            else:\n",
    "                try:\n",
    "                    x_val = float(folder)\n",
    "                except ValueError:\n",
    "                    x_val = idx\n",
    "\n",
    "            # Für jeden Verstellweg Werte speichern\n",
    "            for m in measures:\n",
    "                if m in averages and m in stds:\n",
    "                    results_dict[m][\"x\"].append(x_val)\n",
    "                    results_dict[m][\"mean\"].append(averages[m])\n",
    "                    results_dict[m][\"std\"].append(stds[m])\n",
    "                else:\n",
    "                    print(f\"Key {m} fehlt in {folder_path}\")\n",
    "        else:\n",
    "            print(f\"averages.json oder std.json fehlen in: {folder_path}\")\n",
    "\n",
    "    # Jetzt plotten: pro Verstellweg (drei Subplots)\n",
    "    for i, m in enumerate(measures):\n",
    "        ax = axes[i]\n",
    "        xs = np.array(results_dict[m][\"x\"])\n",
    "        means = np.array(results_dict[m][\"mean\"])\n",
    "        stds = np.array(results_dict[m][\"std\"])\n",
    "\n",
    "        # Punkte verbinden\n",
    "        ax.plot(xs, means, marker='o', color=color, label=f\"{label_prefix} Mean\")\n",
    "        # Fehlerband als transparentes Band zeichnen\n",
    "        ax.fill_between(xs, means - stds, means + stds, color=color, alpha=0.3, label=f\"{label_prefix} ± Std\")\n",
    "        # Titel, Achsenbeschriftungen, Grid und Legende\n",
    "        ax.set_title(m.split('_')[0])  # z.B. \"Verstellweg\"\n",
    "        ax.set_xlabel(\"x-Wert\")\n",
    "        ax.set_ylabel(\"MAE\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auswertung Modell training_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_training=\"build\\\\window_split\\\\real-data\\\\Realdaten_90bis1Prozent\\\\\"\n",
    "base_ris_update=\"build\\\\window_split\\\\real-data\\\\Realdaten_90bis1Prozent_Update\\\\\"\n",
    "for base_dir in [base_dir_training,base_ris_update]:\n",
    "    print(\"===================================================================\")\n",
    "    print(f\"Überordner: {base_dir}\")\n",
    "    for entry in os.listdir(base_dir):\n",
    "        subfolder = os.path.join(base_dir, entry)\n",
    "        # Nur Unterordner berücksichtigen\n",
    "        if os.path.isdir(subfolder):\n",
    "            file_path = os.path.join(subfolder, \"x-train-scaled.npy\")\n",
    "            # Prüfen, ob die Datei existiert\n",
    "            if os.path.exists(file_path):\n",
    "                array = np.load(file_path)\n",
    "                print(f\"Unterordner: {entry} - x-train-scaled.npy shape: {array.shape}\")\n",
    "            else:\n",
    "                print(f\"Unterordner: {entry} - x-train-scaled.npy nicht gefunden.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Vergleiche einzelner TradaboostR2 Trainings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validierung des Prozesses\n",
    "- TradaBoostR2 auf Modell für Simdaten getuned (Konservative Wahl) aber Source und Target Domain sind realdaten\n",
    "- Hinzufügen des Trainignserfolges von konventionellem Modell, getuned auf Realdaten, trainiert auf realdeten  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad_training_14=\"build\\\\tradaboost_model\\\\training_14_Realdaten_auf_Realdaten\"\n",
    "process_training_14=process_all_results(dateipfad_training_14,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_14=get_plottable_data(process_training_14)\n",
    "#Training 19\n",
    "dateipfad_training_19=\"build\\\\tradaboost_model\\\\training_19_RealAufReal_5Prozent\"\n",
    "process_training_19=process_all_results(dateipfad_training_19,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_19=get_plottable_data(process_training_19)\n",
    "#Training 20\n",
    "dateipfad_training_20=\"build\\\\tradaboost_model\\\\training_20_RealAufReal_20Prozent\"\n",
    "process_training_20=process_all_results(dateipfad_training_20,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_20=get_plottable_data(process_training_20)\n",
    "#Training 21\n",
    "dateipfad_training_21=\"build\\\\tradaboost_model\\\\training_21_RealAufReal_40Prozent\"\n",
    "process_training_21=process_all_results(dateipfad_training_21,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_21=get_plottable_data(process_training_21)\n",
    "# Training des Baismodells, 80/10/10 SPlit auf den Realdaten und Modell Stuktur für Realdaten\n",
    "dateipfad_referenzmodell_01=\"build\\\\tradaboost_model\\\\referenzmodell_01_80Realdaten\"\n",
    "process_referenzmodell_01=process_all_results(dateipfad_referenzmodell_01,trained_on_scaled_Labels=True,tradaboost_model=False)\n",
    "ergebnisse_refernezmodell_01=get_plottable_data(process_referenzmodell_01)\n",
    "# Training des Baismodells, 80/10/10 SPlit auf den Realdaten und Modell Stuktur für Realdaten, selbst getunten Modellstruktur mit einem Output, tuning auf Sim Daten\n",
    "dateipfad_referenzmodell_03=\"build\\\\tradaboost_model\\\\referenzmodell_03_80Realdaten\"\n",
    "process_referenzmodell_03=process_all_results(dateipfad_referenzmodell_03,trained_on_scaled_Labels=True,tradaboost_model=False)\n",
    "ergebnisse_refernezmodell_03=get_plottable_data(process_referenzmodell_03)\n",
    "dateipfad_referenzmodell_04=\"build\\\\tradaboost_model\\\\referenzmodell_04_80Realdaten\"\n",
    "process_referenzmodell_04=process_all_results(dateipfad_referenzmodell_04,trained_on_scaled_Labels=True,tradaboost_model=False)\n",
    "ergebnisse_refernezmodell_04=get_plottable_data(process_referenzmodell_04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"TradaBoostR2 ausschließlich auf Realdaten\", fontsize=20)\n",
    "speicherort_diagramme=\"build\\\\DIAGRAMME_PRAESI\\\\\"\n",
    "#fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_14,label=\"TradaBoostR2, labels scaled\",ergebnisart=\"Not scaled\",\n",
    "#                                          overwrite_x_tics=[80,60,40,20,10,5][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "#fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_19,label=None,ergebnisart=\"Not scaled\",\n",
    "#                                          overwrite_x_tics=[5,5,5,5,5][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "#fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_20,label=None,ergebnisart=\"Not scaled\",\n",
    "#                                          overwrite_x_tics=[20,20,20,20,20][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "#fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_21,label=None,ergebnisart=\"Not scaled\",\n",
    "#                                          overwrite_x_tics=[40,40,40,40,40,40][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "mean_x_list,mean_y_list,mean_Phi_list=[],[],[]\n",
    "std_x_list,std_y_list,std_Phi_list=[],[],[]\n",
    "prozentwerte=[5,20,40]\n",
    "dateipfad_training_19=\"build\\\\tradaboost_model\\\\training_19_RealAufReal_5Prozent\"\n",
    "dateipfad_training_20=\"build\\\\tradaboost_model\\\\training_20_RealAufReal_20Prozent\"\n",
    "dateipfad_training_21=\"build\\\\tradaboost_model\\\\training_21_RealAufReal_40Prozent\"\n",
    "for i, (dateipfad, prozent) in enumerate(zip(\n",
    "        [dateipfad_training_19, dateipfad_training_20, dateipfad_training_21],\n",
    "        prozentwerte)):\n",
    "    pass\n",
    "    mae_werte_X, mae_werte_Y, mae_werte_Phi = auswertung_mae_kombiniert(ober_ordner=dateipfad, unterordner_list=[])\n",
    "    speichere_mae_ergebnisse(mae_werte_X=mae_werte_X,mae_werte_Y=mae_werte_Y,mae_werte_Phi=mae_werte_Phi,ausgangsordner=dateipfad,ziel_ordner=\"build\\\\results-tradaBoostR2\\\\KeinOfset\",prozent=prozent)\n",
    "    \n",
    "    std_x = np.std(mae_werte_X)\n",
    "    std_y = np.std(mae_werte_Y)\n",
    "    std_Phi = np.std(mae_werte_Phi)\n",
    "    mean_x = np.mean(mae_werte_X)\n",
    "    mean_y = np.mean(mae_werte_Y)\n",
    "    mean_Phi = np.mean(mae_werte_Phi)\n",
    "\n",
    "    mean_x_list.append(mean_x)\n",
    "    mean_y_list.append(mean_y)\n",
    "    mean_Phi_list.append(mean_Phi)\n",
    "\n",
    "    std_x_list.append(std_x)\n",
    "    std_y_list.append(std_y)\n",
    "    std_Phi_list.append(std_Phi)\n",
    "\n",
    "axes[0].plot(prozentwerte, mean_x_list, color='grey', label='TradaBoostR2')\n",
    "axes[0].fill_between(prozentwerte,\n",
    "                     np.array(mean_x_list) - np.array(std_x_list),\n",
    "                     np.array(mean_x_list) + np.array(std_x_list),\n",
    "                     color='grey', alpha=0.2)\n",
    "\n",
    "axes[1].plot(prozentwerte, mean_y_list, color='grey', label='Mean MAE Y')\n",
    "axes[1].fill_between(prozentwerte,\n",
    "                     np.array(mean_y_list) - np.array(std_y_list),\n",
    "                     np.array(mean_y_list) + np.array(std_y_list),\n",
    "                     color='grey', alpha=0.2)\n",
    "\n",
    "axes[2].plot(prozentwerte, mean_Phi_list, color='grey', label='Mean MAE Phi')\n",
    "axes[2].fill_between(prozentwerte,\n",
    "                     np.array(mean_Phi_list) - np.array(std_Phi_list),\n",
    "                     np.array(mean_Phi_list) + np.array(std_Phi_list),\n",
    "                     color='grey', alpha=0.2)\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "#Standard Modell\n",
    "#X MAE\n",
    "prozente = [5, 10, 15, 20, 50]\n",
    "x_mae_upper_values = [0.668879, 0.331212, 0.298660, 0.279682, 0.219200]\n",
    "x_mae_lower_values = [0.240284, 0.171216, 0.185980, 0.109728, 0.055671]\n",
    "x_mae_mean_values  = [0.454582, 0.251214, 0.242320, 0.194705, 0.137435]\n",
    "#Y MAE\n",
    "prozente = [5, 10, 15, 20, 50]\n",
    "y_mae_upper_values = [2.922360, 2.070843, 1.960314, 1.760613, 0.984506]\n",
    "y_mae_lower_values = [1.823473, 1.366487, 1.263816, 1.115037, 0.711599]\n",
    "y_mae_mean_values  = [2.372917, 1.718665, 1.612065, 1.437825, 0.848052] \n",
    "#Phi MAE\n",
    "prozente = [5, 10, 15, 20, 50]\n",
    "phi_mae_upper_values = [3.053156, 2.924598, 2.298257, 2.476446, 1.646387]\n",
    "phi_mae_lower_values = [2.046840, 1.931982, 1.711781, 1.807508, 1.180802]\n",
    "phi_mae_mean_values  = [2.549998, 2.428290, 2.005019, 2.141977, 1.413595]\n",
    "\n",
    "\n",
    "# Farben definieren\n",
    "standard_color = 'green'\n",
    "# X MAE Plot\n",
    "axes[0].plot(prozente, x_mae_mean_values, color=standard_color, label='Base-Model')\n",
    "axes[0].fill_between(prozente, \n",
    "                     x_mae_lower_values, \n",
    "                     x_mae_upper_values, \n",
    "                     color=standard_color, alpha=0.2)\n",
    "\n",
    "# Y MAE Plot\n",
    "axes[1].plot(prozente, y_mae_mean_values, color=standard_color, label='Base-Model')\n",
    "axes[1].fill_between(prozente, \n",
    "                     y_mae_lower_values, \n",
    "                     y_mae_upper_values, \n",
    "                     color=standard_color, alpha=0.2)\n",
    "\n",
    "# Phi MAE Plot\n",
    "axes[2].plot(prozente, phi_mae_mean_values, color=standard_color, label='Base-Model')\n",
    "axes[2].fill_between(prozente, \n",
    "                     phi_mae_lower_values, \n",
    "                     phi_mae_upper_values, \n",
    "                     color=standard_color, alpha=0.2)\n",
    "phi_trend_coeffs = np.polyfit(prozente, phi_mae_mean_values, 2)\n",
    "phi_trend_values = np.polyval(phi_trend_coeffs, prozente)\n",
    "standardmodell_color = 'green'\n",
    "axes[2].plot(prozente, phi_trend_values, linestyle='--', color=standardmodell_color, label='Trendlinie Phi (Standardmodell)')\n",
    " \n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=len(labels), bbox_to_anchor=(0.5, -0.05))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{speicherort_diagramme}TradaBoostR2_RealAufReal_VS_CNN.pdf\")   # als PDF\n",
    "fig.savefig(f\"{speicherort_diagramme}TradaBoostR2_RealAufReal_VS_CNN.svg\")   # als SVG\n",
    "#Plotten der Ergebnisse vom Referenzmodell\n",
    "#print(ergebnisse_training_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ergebniss: Konvergenz für TradaBoostR2, Fehler wird deutlich geringer für erhöhte Anteile an Realdaten\n",
    "- TODO: Konventionelles Modell hinzufügen, was jetzt drin ist kann nicht bleiben\n",
    "- Klären wie die Scaler schwanken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad_5Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_5Prozent_Realdaten\"\n",
    "dateipfad_10Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_10Prozent_Realdaten\"\n",
    "dateipfad_15Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_15Prozent_Realdaten\"\n",
    "dateipfad_20Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_20Prozent_Realdaten\"\n",
    "dateipfad_50Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_50Prozent_Realdaten\"\n",
    "list_dateipfade=[dateipfad_5Prozent,dateipfad_10Prozent,dateipfad_15Prozent,dateipfad_20Prozent,dateipfad_50Prozent]\n",
    "for dateipfad in list_dateipfade:\n",
    "    process=process_all_results(dateipfad,trained_on_scaled_Labels=True,tradaboost_model=True)\n",
    "    ergebnisse=get_plottable_data(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"TradaBoostR2\", fontsize=20)\n",
    "speicherort_diagramme=\"build\\\\DIAGRAMME_PRAESI\\\\\"\n",
    "#========================================================================\n",
    "#Standard Modell\n",
    "#X MAE\n",
    "prozente = [5, 10, 15, 20, 50]\n",
    "x_mae_upper_values = [0.668879, 0.331212, 0.298660, 0.279682, 0.219200]\n",
    "x_mae_lower_values = [0.240284, 0.171216, 0.185980, 0.109728, 0.055671]\n",
    "x_mae_mean_values  = [0.454582, 0.251214, 0.242320, 0.194705, 0.137435]\n",
    "#Y MAE\n",
    "prozente = [5, 10, 15, 20, 50]\n",
    "y_mae_upper_values = [2.922360, 2.070843, 1.960314, 1.760613, 0.984506]\n",
    "y_mae_lower_values = [1.823473, 1.366487, 1.263816, 1.115037, 0.711599]\n",
    "y_mae_mean_values  = [2.372917, 1.718665, 1.612065, 1.437825, 0.848052] \n",
    "#Phi MAE\n",
    "prozente = [5, 10, 15, 20, 50]\n",
    "phi_mae_upper_values = [3.053156, 2.924598, 2.298257, 2.476446, 1.646387]\n",
    "phi_mae_lower_values = [2.046840, 1.931982, 1.711781, 1.807508, 1.180802]\n",
    "phi_mae_mean_values  = [2.549998, 2.428290, 2.005019, 2.141977, 1.413595]\n",
    "\n",
    "\n",
    "# Farben definieren\n",
    "standard_color = 'green'\n",
    "# X MAE Plot\n",
    "axes[0].plot(prozente, x_mae_mean_values, color=standard_color, label='Base-Model')\n",
    "axes[0].fill_between(prozente, \n",
    "                     x_mae_lower_values, \n",
    "                     x_mae_upper_values, \n",
    "                     color=standard_color, alpha=0.2)\n",
    "\n",
    "# Y MAE Plot\n",
    "axes[1].plot(prozente, y_mae_mean_values, color=standard_color, label='Base-Model')\n",
    "axes[1].fill_between(prozente, \n",
    "                     y_mae_lower_values, \n",
    "                     y_mae_upper_values, \n",
    "                     color=standard_color, alpha=0.2)\n",
    "\n",
    "# Phi MAE Plot\n",
    "axes[2].plot(prozente, phi_mae_mean_values, color=standard_color, label='Base-Model')\n",
    "axes[2].fill_between(prozente, \n",
    "                     phi_mae_lower_values, \n",
    "                     phi_mae_upper_values, \n",
    "                     color=standard_color, alpha=0.2)\n",
    "\n",
    "#========================================================================\n",
    "#Plotten der Ergebnisse für TradaBoostR2\n",
    "# Beispielhafte Prozentangaben (x-Achse)\n",
    "#prozentwerte = [5,5, 10,10, 15,15, 20,50]\n",
    "prozentwerte = [5, 10,15,20,50]\n",
    "\n",
    "# Leere Listen für Mittelwerte und Standardabweichungen\n",
    "mean_x_list, mean_y_list, mean_Phi_list = [], [], []\n",
    "std_x_list, std_y_list, std_Phi_list = [], [], []\n",
    "#dateipfad_training_17=\"build\\\\tradaboost_model\\\\training_17_5Prozent_Realdaten\"\n",
    "#dateipfad_training_25=\"build\\\\tradaboost_model\\\\training_25_5Prozent_Realdaten_15Stueck\"\n",
    "#dateipfad_training_22=\"build\\\\tradaboost_model\\\\training_22_10Prozent_Realdaten\"\n",
    "#dateipfad_training_26=\"build\\\\tradaboost_model\\\\training_26_10Prozent_Realdaten_15Stueck\"\n",
    "#dateipfad_training_23=\"build\\\\tradaboost_model\\\\training_23_15Prozent_Realdaten\"\n",
    "#dateipfad_training_27=\"build\\\\tradaboost_model\\\\training_27_15Prozent_Realdaten_15Stueck\"\n",
    "#dateipfad_training_18=\"build\\\\tradaboost_model\\\\training_18_20Prozent_Realdaten\"\n",
    "#dateipfad_training_24=\"build\\\\tradaboost_model\\\\training_24_50Prozent_Realdaten\"\n",
    "\n",
    "dateipfad_5Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_5Prozent_Realdaten\"\n",
    "dateipfad_10Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_10Prozent_Realdaten\"\n",
    "dateipfad_15Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_15Prozent_Realdaten\"\n",
    "dateipfad_20Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_20Prozent_Realdaten\"\n",
    "dateipfad_50Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_50Prozent_Realdaten\"\n",
    "\n",
    "# Werte sammeln\n",
    "for i, (dateipfad, prozent) in enumerate(zip(\n",
    "        #[dateipfad_training_17,dateipfad_training_25,dateipfad_training_22,dateipfad_training_26, dateipfad_training_23,dateipfad_training_27, dateipfad_training_18,dateipfad_training_24],\n",
    "        [dateipfad_5Prozent,dateipfad_10Prozent,dateipfad_15Prozent, dateipfad_20Prozent,dateipfad_50Prozent],\n",
    "        prozentwerte)):\n",
    "    pass\n",
    "    mae_werte_X, mae_werte_Y, mae_werte_Phi = auswertung_mae_kombiniert(ober_ordner=dateipfad, unterordner_list=[])\n",
    "    speichere_mae_ergebnisse(mae_werte_X=mae_werte_X,mae_werte_Y=mae_werte_Y,mae_werte_Phi=mae_werte_Phi,ausgangsordner=dateipfad,\n",
    "                             ziel_ordner=\"build\\\\results-tradaBoostR2\\\\KeinOffset_N20\",prozent=prozent)\n",
    "    \n",
    "    std_x = np.std(mae_werte_X)\n",
    "    std_y = np.std(mae_werte_Y)\n",
    "    std_Phi = np.std(mae_werte_Phi)\n",
    "    mean_x = np.mean(mae_werte_X)\n",
    "    mean_y = np.mean(mae_werte_Y)\n",
    "    mean_Phi = np.mean(mae_werte_Phi)\n",
    "\n",
    "    mean_x_list.append(mean_x)\n",
    "    mean_y_list.append(mean_y)\n",
    "    mean_Phi_list.append(mean_Phi)\n",
    "\n",
    "    std_x_list.append(std_x)\n",
    "    std_y_list.append(std_y)\n",
    "    std_Phi_list.append(std_Phi)\n",
    "\n",
    "# Plotten\n",
    "axes[0].plot(prozentwerte, mean_x_list, color='red', label='TradaBoostR2')\n",
    "axes[0].fill_between(prozentwerte,\n",
    "                     np.array(mean_x_list) - np.array(std_x_list),\n",
    "                     np.array(mean_x_list) + np.array(std_x_list),\n",
    "                     color='red', alpha=0.2)\n",
    "axes[1].plot(prozentwerte, mean_y_list, color='red', label='Mean MAE Y')\n",
    "axes[1].fill_between(prozentwerte,\n",
    "                     np.array(mean_y_list) - np.array(std_y_list),\n",
    "                     np.array(mean_y_list) + np.array(std_y_list),\n",
    "                     color='red', alpha=0.2)\n",
    "axes[2].plot(prozentwerte, mean_Phi_list, color='red', label='Mean MAE Phi')\n",
    "axes[2].fill_between(prozentwerte,\n",
    "                     np.array(mean_Phi_list) - np.array(std_Phi_list),\n",
    "                     np.array(mean_Phi_list) + np.array(std_Phi_list),\n",
    "                     color='red', alpha=0.2)\n",
    " \n",
    "for ax in axes:\n",
    "    ax.grid(True, which='both', axis='y')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.3))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f\"{speicherort_diagramme}TradaBoostR2_vs_CNN.pdf\")   # als PDF\n",
    "fig.savefig(f\"{speicherort_diagramme}TradaBoostR2_vs_CNN.svg\")   # als SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ergebnisse zusammenfassen:\n",
    "base_folder = r\"build\\\\results-tradaBoostR2\\\\KeinOffset_N20\"\n",
    "collect_tradaboost_stats(base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vergleich der Ergebnisse für unterschiedliche Ausgangsdatensätze**\n",
    "TradaboostR2 durchführen für unterschiedliche Anteile von Realdaten in der Target Domain und unterschiedliche  \n",
    "1. Source Realdaten, Target Realdaten (Training 14, 19, 20)\n",
    "2. Source Sim Daten, Target Realdaten (Training 11, 17, 18)\n",
    "3. Source Sim Daten offset, Target Realdaten (Training 10, 16, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "dateipfad_training_11=\"build\\\\tradaboost_model\\\\training_11_90bis1ProzentRealdaten\"\n",
    "process_training_11=process_all_results(dateipfad_training_11,trained_on_scaled_Labels=True,tradaboost_model=True)\n",
    "ergebnisse_training_11=get_plottable_data(process_training_11)\n",
    "#Experiment 18\n",
    "dateipfad_training_18=\"build\\\\tradaboost_model\\\\training_18_20Prozent_Realdaten\"\n",
    "process_training_18=process_all_results(dateipfad_training_18,trained_on_scaled_Labels=True,tradaboost_model=True)\n",
    "ergebnisse_training_18=get_plottable_data(process_training_18)\n",
    "#Training 17\n",
    "dateipfad_training_17=\"build\\\\tradaboost_model\\\\training_17_5Prozent_Realdaten\"\n",
    "process_training_17=process_all_results(dateipfad_training_17,trained_on_scaled_Labels=True,tradaboost_model=True)\n",
    "ergebnisse_training_17=get_plottable_data(process_training_17)\n",
    "#3\n",
    "dateipfad_training_10=\"build\\\\tradaboost_model\\\\training_10_90bis1ProzentRealdaten\"\n",
    "process_training_10=process_all_results(dateipfad_training_10,trained_on_scaled_Labels=True,tradaboost_model=True)\n",
    "ergebnisse_training_10=get_plottable_data(process_training_10)\n",
    "#Experiment 16\n",
    "dateipfad_training_16=\"build\\\\tradaboost_model\\\\training_16_20Prozent_Realdaten\"\n",
    "process_training_16=process_all_results(dateipfad_training_16,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_16=get_plottable_data(process_training_16)\n",
    "#Experiment 15\n",
    "dateipfad_training_15=\"build\\\\tradaboost_model\\\\Training_15_5Prozent_Realdaten\"\n",
    "process_training_15=process_all_results(dateipfad_training_15,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_15=get_plottable_data(process_training_15)\n",
    "#Experiment 22\n",
    "dateipfad_training_22=\"build\\\\tradaboost_model\\\\training_22_10Prozent_Realdaten\"\n",
    "process_training_22=process_all_results(dateipfad_training_22,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_22=get_plottable_data(process_training_22)\n",
    "\n",
    "# CORAL \n",
    "dateipfad_training_CORAL_03=\"build\\\\tradaboost_model\\\\training_CORAL_03_Daten01Bis05\"\n",
    "process_training_CORAL_03=process_all_results(dateipfad_training_CORAL_03,trained_on_scaled_Labels=True,tradaboost_model=True)\n",
    "ergebnisse_training_CORAL_03=get_plottable_data(process_training_CORAL_03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 22\n",
    "dateipfad_training_22=\"build\\\\tradaboost_model\\\\training_22_10Prozent_Realdaten\"\n",
    "process_training_22=process_all_results(dateipfad_training_22,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_22=get_plottable_data(process_training_22)\n",
    "#Experiment 23\n",
    "dateipfad_training_23=\"build\\\\tradaboost_model\\\\training_23_15Prozent_Realdaten\"\n",
    "process_training_23=process_all_results(dateipfad_training_23,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_23=get_plottable_data(process_training_23)\n",
    "#Experiment 24\n",
    "dateipfad_training_24=\"build\\\\tradaboost_model\\\\training_24_50Prozent_Realdaten\"\n",
    "process_training_24=process_all_results(dateipfad_training_24,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_24=get_plottable_data(process_training_24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 25\n",
    "dateipfad_training_25=\"build\\\\tradaboost_model\\\\training_25_5Prozent_Realdaten_15Stueck\"\n",
    "process_training_25=process_all_results(dateipfad_training_25,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_25=get_plottable_data(process_training_25)\n",
    "#Experiment 26\n",
    "dateipfad_training_26=\"build\\\\tradaboost_model\\\\training_26_10Prozent_Realdaten_15Stueck\"\n",
    "process_training_26=process_all_results(dateipfad_training_26,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_26=get_plottable_data(process_training_26)\n",
    "#Experiment 27\n",
    "dateipfad_training_27=\"build\\\\tradaboost_model\\\\training_27_15Prozent_Realdaten_15Stueck\"\n",
    "process_training_27=process_all_results(dateipfad_training_27,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_27=get_plottable_data(process_training_27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5%\n",
    "dateipfad_training_5Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_5Prozent_Realdaten\"\n",
    "process_training_5Prozent=process_all_results(dateipfad_training_5Prozent,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_5Prozent=get_plottable_data(process_training_5Prozent)\n",
    "#Experiment 10%\n",
    "dateipfad_training_10Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_10Prozent_Realdaten\"\n",
    "process_training_10Prozent=process_all_results(dateipfad_training_10Prozent,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_10Prozent=get_plottable_data(process_training_10Prozent)\n",
    "#Experiment 15%\n",
    "dateipfad_training_15Prozent=\"build\\\\tradaboost_model\\\\Combined_Models_Update\\\\training_15Prozent_Realdaten\"\n",
    "process_training_15Prozent=process_all_results(dateipfad_training_15Prozent,trained_on_scaled_Labels=None,tradaboost_model=True)\n",
    "ergebnisse_training_15Prozent=get_plottable_data(process_training_15Prozent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotten\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"TradaBoostR2\", fontsize=20)\n",
    "\n",
    "#Basismodelle \n",
    "daten_cnn_80='build\\\\results-base-model\\\\80\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_80+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"grey\",std_path=daten_cnn_80+\"std.json\",x_position=80,\n",
    "                                 dataframe_path=daten_cnn_80+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "daten_cnn_50='build\\\\results-base-model\\\\50\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_50+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"grey\",std_path=daten_cnn_50+\"std.json\",x_position=50,\n",
    "                                 dataframe_path=daten_cnn_50+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "daten_cnn_20='build\\\\results-base-model\\\\20\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_20+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"black\",std_path=daten_cnn_20+\"std.json\",x_position=20,\n",
    "                                 dataframe_path=daten_cnn_20+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "daten_cnn_10='build\\\\results-base-model\\\\10\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_10+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"black\",std_path=daten_cnn_10+\"std.json\",x_position=10,\n",
    "                                 dataframe_path=daten_cnn_10+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "daten_cnn_5='build\\\\results-base-model\\\\5\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_5+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"black\",std_path=daten_cnn_5+\"std.json\",x_position=5,\n",
    "                                 dataframe_path=daten_cnn_5+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "\n",
    "#Referenz TradaBoostR2\n",
    "fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_14,label=\"Source und Targetdaomin Realdaten\",ergebnisart=\"Not scaled\",\n",
    "                                          overwrite_x_tics=[80,60,40,20,10,5][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_19,label=None,ergebnisart=\"Not scaled\",\n",
    "                                          overwrite_x_tics=[5,5,5,5,5][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_20,label=None,ergebnisart=\"Not scaled\",\n",
    "                                          overwrite_x_tics=[20,20,20,20,20][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_21,label=None,ergebnisart=\"Not scaled\",\n",
    "                                          overwrite_x_tics=[40,40,40,40,40,40][::-1],color=\"black\",marker=\"*\",fig=fig,axes=axes)\n",
    "\n",
    "for i,(ergebnis,prozente) in enumerate(zip([ergebnisse_training_11, ergebnisse_training_18,ergebnisse_training_17,ergebnisse_training_22,ergebnisse_training_23],\n",
    "                             [[90,80,70,60,50,40,30,20,10,7,5,3,1][::-1],[20,20,20,20,20],[5,5,5,5,5,],[10,10,10,10,10],[15,15,15,15,15]])):\n",
    "    if not i==0:\n",
    "       fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnis,label=\"Kein Ofset, scaled labels\" if i==1 else None,ergebnisart=\"Not scaled\",\n",
    "                                                overwrite_x_tics=prozente,color=\"blue\",marker=\"4\",fig=fig,axes=axes)\n",
    "    else:\n",
    "        fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnis,label=\"Kein Ofset, scaled labels, Einzelversuche\" if i==0 else None,ergebnisart=\"Not scaled\",\n",
    "                                            overwrite_x_tics=prozente,color=\"green\",marker=\"4\",fig=fig,axes=axes)\n",
    "\n",
    "'''\n",
    "for i, (ergebnis,prozente) in enumerate(zip([ergebnisse_training_10, ergebnisse_training_16,ergebnisse_training_15],\n",
    "                             [[90,80,70,60,50,40,30,20,10,7,5,3,1][::-1],[20,20,20,20,20],[5,5,5,5,5,]])):\n",
    "    fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnis,label=\"Ofset multiplitiv, scaled labels\" if i==0 else None,ergebnisart=\"Not scaled\",\n",
    "                                              overwrite_x_tics=[90,80,70,60,50,40,30,20,10,7,5,3,1][::-1],color=\"red\",marker=\"2\",fig=fig,axes=axes)\n",
    "\n",
    "#TradaBoostR2 mit CORAL\n",
    "fig, axes=plotten_TradaBoostR2_Ergebnisse_MAE(ergebnisse_training_CORAL_03,label=\"CORAL Sim Daten, Lbels Scaled\",ergebnisart=\"Not scaled\",\n",
    "                                          overwrite_x_tics=[20,20,20,20,20],color=\"red\",marker=\"D\",fig=fig,axes=axes)\n",
    "'''\n",
    "for ax in axes:\n",
    "    ax.grid(True, which='both', axis='y')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konfidenzintervalle Bei 5 bis 20%\n",
    "-> Standardabweichung und Mittelwert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"TradaBoostR2\", fontsize=20)\n",
    "\n",
    "#Basismodelle \n",
    "print(\"Basismodell\")\n",
    "print(\"Einzelergebnis: 80%\")\n",
    "daten_cnn_80='build\\\\results-base-model\\\\80\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_80+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"grey\",std_path=daten_cnn_80+\"std.json\",x_position=80,\n",
    "                                 dataframe_path=daten_cnn_80+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "print(\"Einzelergebnis: 50%\")\n",
    "daten_cnn_50='build\\\\results-base-model\\\\50\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_50+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"grey\",std_path=daten_cnn_50+\"std.json\",x_position=50,\n",
    "                                 dataframe_path=daten_cnn_50+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "print(\"Einzelergebnis: 20%\")\n",
    "daten_cnn_20='build\\\\results-base-model\\\\20\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_20+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"black\",std_path=daten_cnn_20+\"std.json\",x_position=20,\n",
    "                                 dataframe_path=daten_cnn_20+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "print(\"Einzelergebnis: 10%\")\n",
    "daten_cnn_10='build\\\\results-base-model\\\\10\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_10+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"black\",std_path=daten_cnn_10+\"std.json\",x_position=10,\n",
    "                                 dataframe_path=daten_cnn_10+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "print(\"Einzelergebnis: 5%\")\n",
    "daten_cnn_5='build\\\\results-base-model\\\\5\\\\'\n",
    "fig, axes = plotten_Boxplots_MAE(averages_path=daten_cnn_5+\"averages.json\",label=\"CNN 80% Realdaten\", color=\"black\",std_path=daten_cnn_5+\"std.json\",x_position=5,\n",
    "                              dataframe_path=daten_cnn_5+\"dataframe.json\",fig=fig,axes=axes,show_lines=False,show_box_plot=True)\n",
    "\n",
    "# Beispielhafte Prozentangaben (x-Achse)\n",
    "prozentwerte = [5, 10, 15, 20,50]\n",
    "\n",
    "# Leere Listen für Mittelwerte und Standardabweichungen\n",
    "mean_x_list, mean_y_list, mean_Phi_list = [], [], []\n",
    "std_x_list, std_y_list, std_Phi_list = [], [], []\n",
    "\n",
    "# Werte sammeln\n",
    "for i, (dateipfad, prozent) in enumerate(zip(\n",
    "        [dateipfad_training_17, dateipfad_training_22, dateipfad_training_23, dateipfad_training_18,dateipfad_training_24],\n",
    "        prozentwerte)):\n",
    "    pass\n",
    "    mae_werte_X, mae_werte_Y, mae_werte_Phi = auswertung_mae_kombiniert(ober_ordner=dateipfad, unterordner_list=[])\n",
    "    speichere_mae_ergebnisse(mae_werte_X=mae_werte_X,mae_werte_Y=mae_werte_Y,mae_werte_Phi=mae_werte_Phi,ausgangsordner=dateipfad,ziel_ordner=\"build\\\\results-tradaBoostR2\\\\KeinOfset\",prozent=prozent)\n",
    "    \n",
    "    std_x = np.std(mae_werte_X)\n",
    "    std_y = np.std(mae_werte_Y)\n",
    "    std_Phi = np.std(mae_werte_Phi)\n",
    "    mean_x = np.mean(mae_werte_X)\n",
    "    mean_y = np.mean(mae_werte_Y)\n",
    "    mean_Phi = np.mean(mae_werte_Phi)\n",
    "\n",
    "    mean_x_list.append(mean_x)\n",
    "    mean_y_list.append(mean_y)\n",
    "    mean_Phi_list.append(mean_Phi)\n",
    "\n",
    "    std_x_list.append(std_x)\n",
    "    std_y_list.append(std_y)\n",
    "    std_Phi_list.append(std_Phi)\n",
    "\n",
    "# Plotten\n",
    "#fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "#fig.suptitle(\"TradaBoostR2\", fontsize=20)\n",
    "print(\"TradaBostR2 Modell\")\n",
    "print(\"Verstellweg_X\")\n",
    "print(f\"Prozente = {prozentwerte}\")\n",
    "print(f\"Mittelwerte {mean_x_list}\")\n",
    "print(f\"Standardabweichung ={std_x_list}\")\n",
    "axes[0].plot(prozentwerte, mean_x_list, color='red', label='TradaBoostR2')\n",
    "axes[0].fill_between(prozentwerte,\n",
    "                     np.array(mean_x_list) - np.array(std_x_list),\n",
    "                     np.array(mean_x_list) + np.array(std_x_list),\n",
    "                     color='red', alpha=0.2)\n",
    "\n",
    "print(\"Verstellweg_Y\")\n",
    "print(f\"Prozente = {prozentwerte}\")\n",
    "print(f\"Mittelwerte {mean_y_list}\")\n",
    "print(f\"Standardabweichung ={std_y_list}\")\n",
    "axes[1].plot(prozentwerte, mean_y_list, color='red', label='Mean MAE Y')\n",
    "axes[1].fill_between(prozentwerte,\n",
    "                     np.array(mean_y_list) - np.array(std_y_list),\n",
    "                     np.array(mean_y_list) + np.array(std_y_list),\n",
    "                     color='red', alpha=0.2)\n",
    "\n",
    "# Verstellweg_Phi\n",
    "print(\"Verstellweg_Phi\")\n",
    "print(f\"Prozente = {prozentwerte}\")\n",
    "print(f\"Mittelwerte {mean_Phi_list}\")\n",
    "print(f\"Standardabweichung ={std_Phi_list}\")\n",
    "axes[2].plot(prozentwerte, mean_Phi_list, color='red', label='Mean MAE Phi')\n",
    "axes[2].fill_between(prozentwerte,\n",
    "                     np.array(mean_Phi_list) - np.array(std_Phi_list),\n",
    "                     np.array(mean_Phi_list) + np.array(std_Phi_list),\n",
    "                     color='red', alpha=0.2)\n",
    " \n",
    "for ax in axes:\n",
    "    ax.grid(True, which='both', axis='y')\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.3))\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleich von Basismodellen:\n",
    "- Target und Source Realdaten auf der Modellstruktur von Sim Daten Optimierung\n",
    "- Fehler eintragen von konventieonellem Training auf Realen Daten mit der Modellstruktur von Sim Daten Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Plots: Vergleich von Tradaboost bei verscheidenen Sim Daten Offsets und CORAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Plot für 20% Realdaten: \n",
    "- Basis Modell \n",
    "- Tradaboost auf Ausgangssimdaten \n",
    "- Tradaboost auf CORAL\n",
    "- Harter Code weil Ergebnisse von Team mit eingebaut wurde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Realdten\n",
    "werte_cnn={\"Verstellweg_Phi_mean_absolute_error\":{\n",
    "        \"0\":1.9067816734,\n",
    "        \"1\":2.1337795258,\n",
    "        \"2\":1.9199428558,\n",
    "        \"3\":1.9067816734,\n",
    "        \"4\":3.005964756,\n",
    "        \"5\":1.9067816734,\n",
    "        \"6\":2.2692170143\n",
    "    },\"Verstellweg_X_mean_absolute_error\":{\n",
    "        \"0\":0.209063977,\n",
    "        \"1\":0.171160996,\n",
    "        \"2\":0.1484249532,\n",
    "        \"3\":0.209063977,\n",
    "        \"4\":0.0717340857,\n",
    "        \"5\":0.209063977,\n",
    "        \"6\":0.3262635767\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\":{\n",
    "        \"0\":1.9697440863,\n",
    "        \"1\":1.351219058,\n",
    "        \"2\":1.1623004675,\n",
    "        \"3\":1.9697440863,\n",
    "        \"4\":1.1553333998,\n",
    "        \"5\":1.9697440863,\n",
    "        \"6\":1.4088890553\n",
    "    }}\n",
    "#Daten Tradaboost 20%\n",
    "tradaBoostR2={\"Verstellweg_X_mean_absolute_error\": {\n",
    "        \"0\": 0.08288468032251943,\n",
    "        \"1\": 0.21524366282545698,\n",
    "        \"2\": 0.14586098820897023,\n",
    "        \"3\": 0.053280332398058364,\n",
    "        \"4\": 0.20650519108802687,\n",
    "        \"5\": 0.18184938013202243,\n",
    "        \"6\": 0.11713021944887886,\n",
    "        \"7\": 0.025617252271489258,\n",
    "        \"8\": 0.03641682240428067,\n",
    "        \"9\": 0.3018832174692429,\n",
    "        \"10\": 0.1763029739464692,\n",
    "        \"11\": 0.14635767279158332,\n",
    "        \"12\": 0.22023966136671375,\n",
    "        \"13\": 0.16828045354301863,\n",
    "        \"14\": 0.0661374278086979,\n",
    "        \"15\": 0.12250337163710164,\n",
    "        \"16\": 0.06145791206108678,\n",
    "        \"17\": 0.15484200206590248,\n",
    "        \"18\": 0.0786290246397835,\n",
    "        \"19\": 0.06858963713414828\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\": {\n",
    "        \"0\": 1.1751330641804827,\n",
    "        \"1\": 1.2576812388273626,\n",
    "        \"2\": 1.144125397215789,\n",
    "        \"3\": 1.0024489317667418,\n",
    "        \"4\": 1.0969184767343503,\n",
    "        \"5\": 1.1895605895330212,\n",
    "        \"6\": 1.2619899140763817,\n",
    "        \"7\": 1.1220895204792756,\n",
    "        \"8\": 1.7612019871672981,\n",
    "        \"9\": 1.6243900821727244,\n",
    "        \"10\": 1.3416488552260777,\n",
    "        \"11\": 1.4666800538890026,\n",
    "        \"12\": 1.468327278666995,\n",
    "        \"13\": 1.004181134725511,\n",
    "        \"14\": 0.9282207859888456,\n",
    "        \"15\": 0.8795664960824864,\n",
    "        \"16\": 1.6466492937146229,\n",
    "        \"17\": 1.2280335819845427,\n",
    "        \"18\": 1.2285310542835002,\n",
    "        \"19\": 1.0620621863674233\n",
    "    },\n",
    "    \"Verstellweg_Phi_mean_absolute_error\": {\n",
    "        \"0\": 2.073779472686649,\n",
    "        \"1\": 2.1627584662752493,\n",
    "        \"2\": 1.965387472274034,\n",
    "        \"3\": 1.592234019012553,\n",
    "        \"4\": 1.5366292233258196,\n",
    "        \"5\": 1.850415948121691,\n",
    "        \"6\": 1.881056591763831,\n",
    "        \"7\": 1.9150726655759753,\n",
    "        \"8\": 1.984376253563216,\n",
    "        \"9\": 2.5019066327108095,\n",
    "        \"10\": 2.0318410159699605,\n",
    "        \"11\": 2.0814779434465933,\n",
    "        \"12\": 1.9225910475300676,\n",
    "        \"13\": 1.7005844771876635,\n",
    "        \"14\": 1.8245534361956879,\n",
    "        \"15\": 1.6013948748018372,\n",
    "        \"16\": 1.5687989227375518,\n",
    "        \"17\": 1.7783990885369756,\n",
    "        \"18\": 1.76735358534929,\n",
    "        \"19\": 1.735422802396724\n",
    "    }\n",
    "}\n",
    "#Werte von Coral\n",
    "tradaBoostR2_CORAL={\n",
    "    \"Verstellweg_X_mean_absolute_error\": {\n",
    "        \"0\": 0.08916666363060587,\n",
    "        \"1\": 0.01441970658478481,\n",
    "        \"2\": 0.2845787332813527,\n",
    "        \"3\": 0.058659423202674664,\n",
    "        \"4\": 0.025058066611856136\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\": {\n",
    "        \"0\": 1.2076558100259316,\n",
    "        \"1\": 1.152666684863061,\n",
    "        \"2\": 1.191035593494967,\n",
    "        \"3\": 1.1321671400178104,\n",
    "        \"4\": 1.0151379487677\n",
    "    },\n",
    "    \"Verstellweg_Phi_mean_absolute_error\": {\n",
    "        \"0\": 2.741374029036908,\n",
    "        \"1\": 2.361319938769061,\n",
    "        \"2\": 1.9479067650002413,\n",
    "        \"3\": 2.0183503363440707,\n",
    "        \"4\": 1.6194910031975571\n",
    "    }\n",
    "}\n",
    "#Daten zu Sorft Start, 20% Realdaten\n",
    "'''soft_start_daten={\n",
    "        \"Verstellweg_Phi_mean_absolute_error\":{\n",
    "        \"0\":1.8479813337,\n",
    "        \"1\":1.9301689863,\n",
    "        \"2\":1.4882190228,\n",
    "        \"3\":1.4696961641,\n",
    "        \"4\":1.4983427525\n",
    "    },\n",
    "    \"Verstellweg_X_mean_absolute_error\":{\n",
    "        \"0\":0.063426502,\n",
    "        \"1\":0.0344519727,\n",
    "        \"2\":0.1150183976,\n",
    "        \"3\":0.1496070921,\n",
    "        \"4\":0.0980957523\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\":{\n",
    "        \"0\":1.1159968376,\n",
    "        \"1\":1.424552083,\n",
    "        \"2\":0.8680146337,\n",
    "        \"3\":1.6187688112,\n",
    "        \"4\":1.1865266562\n",
    "    }\n",
    "}'''\n",
    "soft_start_daten={\n",
    "    \"Verstellweg_X_mean_absolute_error\":{\n",
    "        \"0\":0.2152078748,\n",
    "        \"1\":0.0647058561,\n",
    "        \"2\":0.1846295446,\n",
    "        \"3\":0.1305526644,\n",
    "        \"4\":0.174933672,\n",
    "        \"5\":0.1969616562,\n",
    "        \"6\":0.1165737212,\n",
    "        \"7\":0.1123874038,\n",
    "        \"8\":0.1278404891,\n",
    "        \"9\":0.2110052258\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\":{\n",
    "        \"0\":1.3802608252,\n",
    "        \"1\":1.0254011154,\n",
    "        \"2\":1.3772754669,\n",
    "        \"3\":1.378138423,\n",
    "        \"4\":1.6563072205,\n",
    "        \"5\":1.5655932426,\n",
    "        \"6\":1.5238282681,\n",
    "        \"7\":1.2637146711,\n",
    "        \"8\":1.7235040665,\n",
    "        \"9\":1.1684662104\n",
    "    },\n",
    "    \"Verstellweg_Phi_mean_absolute_error\":{\n",
    "        \"0\":1.9285458326,\n",
    "        \"1\":1.5551062822,\n",
    "        \"2\":1.8586724997,\n",
    "        \"3\":1.7134574652,\n",
    "        \"4\":1.7493059635,\n",
    "        \"5\":1.9976165295,\n",
    "        \"6\":2.2492103577,\n",
    "        \"7\":1.7626707554,\n",
    "        \"8\":1.6296473742,\n",
    "        \"9\":1.7448952198\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "def extract_values(data, key):\n",
    "    return list(data[key].values())\n",
    "\n",
    "# Verstellwege und Labels\n",
    "verstellwege = [\n",
    "    (\"Verstellweg_X_mean_absolute_error\", \"Verstellweg X\"),\n",
    "    (\"Verstellweg_Y_mean_absolute_error\", \"Verstellweg Y\"),\n",
    "    (\"Verstellweg_Phi_mean_absolute_error\", \"Verstellweg Phi\")\n",
    "]\n",
    "from matplotlib.patches import Patch\n",
    "# Plot Setup\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "#fig.suptitle(\"MAE der Verstellwege bei 20% der Realdaten\", fontsize=20)\n",
    "\n",
    "titel_formeln = [r\"$x$\", r\"$y$\", r\"$\\varphi$\"]\n",
    "# Farbliche Gestaltung\n",
    "farben = ['#1f77b4', '#2ca02c', '#ff7f0e']\n",
    "farben=['#005AA9','#005AA9','#005AA9']\n",
    "farben=[\"green\",\"red\",\"orange\",\"blue\"]# TODO hier ergänzen \n",
    "farben=[\"grey\",\"grey\",\"grey\",\"grey\"]# TODO hier ergänzen \n",
    "schraffuren = ['///', '\\\\\\\\\\\\', '|||', 'xxx']\n",
    "methoden = [\"Base-Model\", \"TrAdaBoostR2\", \"TrAdaBoostR2-CORAL\", \"Fine-Tuning\"]\n",
    "# Für jeden Verstellweg die Boxplots einzeichnen\n",
    "for i,(ax, (verstellweg_key, verstellweg_label)) in enumerate(zip(axes, verstellwege)):\n",
    "    # Werte extrahieren\n",
    "    cnn_values = extract_values(werte_cnn, verstellweg_key)\n",
    "    trada_values = extract_values(tradaBoostR2, verstellweg_key)\n",
    "    coral_values = extract_values(tradaBoostR2_CORAL, verstellweg_key)\n",
    "    soft_start_values= extract_values(soft_start_daten, verstellweg_key)\n",
    "\n",
    "    # Boxplots zeichnen\n",
    "    bp = ax.boxplot([cnn_values, trada_values, coral_values,soft_start_values],\n",
    "                    #labels=[\"CNN\", \"TrAdaBoostR2\", \"TrAdaBoostR2_CORAL\",\"Softstart\"],\n",
    "                    patch_artist=True,\n",
    "                    widths=0.6)\n",
    "    \n",
    "    for patch, hatch in zip(bp['boxes'], schraffuren):\n",
    "        patch.set(hatch=hatch, facecolor='white', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    \n",
    "    y_min = min(min(cnn_values), min(trada_values), min(coral_values), min(soft_start_values))\n",
    "    ax.set_ylim(bottom=y_min - 0.05)  # Optional: Ein kleiner Offset nach unten, um Platz zu schaffen\n",
    "\n",
    "\n",
    "    # Farben zuweisen\n",
    "    for patch, color in zip(bp['boxes'], farben):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    # Mediane hervorheben\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='black', linewidth=2)\n",
    "    mae_addon=[\"x\",\"y\",\"\\varphi\"]\n",
    "    if i==0:\n",
    "        ax.set_ylabel(f\"$MAE_x$ [mm]\",fontsize=20)\n",
    "    elif i==1:\n",
    "        ax.set_ylabel(f\"$MAE_y$ [mm]\",fontsize=20)\n",
    "    else:\n",
    "        ax.set_ylabel(\"$MAE_{\\\\varphi}$ [°]\",fontsize=20)\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.set_xticks([])\n",
    "    #ax.set_title(f\"Diviation $\\Delta${titel_formeln[i]}\", fontsize=18)\n",
    "    #ax.set_ylabel(\"Mean Absolute Error\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    #ax.set_ylim(bottom=0)  # Damit die Boxen immer bei 0 starten\n",
    "\n",
    "legenden_patches = [Patch(facecolor='grey', hatch=h, edgecolor='black', label=label) \n",
    "                    for h, label in zip(schraffuren, methoden)]\n",
    "fig.legend(handles=legenden_patches, loc='lower center', ncol=4, fontsize=24, frameon=False, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.9])\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"{speicherort_diagramme}BoxPlot_3Modelle.pdf\")   # als PDF\n",
    "fig.savefig(f\"{speicherort_diagramme}BoxPlot_3Modelle.svg\")   # als SVG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def extract_values(data, key):\n",
    "    return list(data[key].values())\n",
    "\n",
    "# Verstellwege und Labels\n",
    "verstellwege = [\n",
    "    (\"Verstellweg_X_mean_absolute_error\", \"Verstellweg X\", r\"$MAE_x$ [mm]\"),\n",
    "    (\"Verstellweg_Y_mean_absolute_error\", \"Verstellweg Y\", r\"$MAE_y$ [mm]\"),\n",
    "    (\"Verstellweg_Phi_mean_absolute_error\", \"Verstellweg Phi\", r\"$MAE_{\\varphi}$ [°]\")\n",
    "]\n",
    "\n",
    "# Farben und Schraffuren\n",
    "farben = [\"white\", \"white\", \"white\", \"white\"]\n",
    "schraffuren = ['///', '\\\\\\\\\\\\', '|||', 'xxx']\n",
    "methoden = [\"Base-Model\", \"TrAdaBoostR2\", \"TrAdaBoostR2-CORAL\", \"Fine-Tuning\"]\n",
    "\n",
    "speicherort_diagramme=\"build\\\\DIAGRAMME_PRAESI\\\\\"\n",
    "\n",
    "\n",
    "# Boxplots zeichnen ohne Legenden\n",
    "for verstellweg_key, verstellweg_label, ylabel in verstellwege:\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "    # Werte extrahieren\n",
    "    cnn_values = extract_values(werte_cnn, verstellweg_key)\n",
    "    trada_values = extract_values(tradaBoostR2, verstellweg_key)\n",
    "    coral_values = extract_values(tradaBoostR2_CORAL, verstellweg_key)\n",
    "    soft_start_values = extract_values(soft_start_daten, verstellweg_key)\n",
    "\n",
    "    # Boxplot zeichnen\n",
    "    bp = ax.boxplot([cnn_values, trada_values, coral_values, soft_start_values],\n",
    "                    patch_artist=True, widths=0.6)\n",
    "\n",
    "    for patch, hatch, color in zip(bp['boxes'], schraffuren, farben):\n",
    "        patch.set(hatch=hatch, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    # Mediane\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='black', linewidth=2)\n",
    "\n",
    "    # Y-Achse\n",
    "    y_min = min(min(cnn_values), min(trada_values), min(coral_values), min(soft_start_values))\n",
    "    ax.set_ylim(bottom=y_min - 0.05)\n",
    "    ax.set_ylabel(ylabel, fontsize=18)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_box_aspect(3/4)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Dateiname bauen\n",
    "    name_suffix = verstellweg_label.replace(\" \", \"_\")\n",
    "    fig.savefig(f\"{speicherort_diagramme}BoxPlot_{name_suffix}.pdf\")\n",
    "    fig.savefig(f\"{speicherort_diagramme}BoxPlot_{name_suffix}.svg\")\n",
    "\n",
    "    #plt.close(fig)\n",
    "\n",
    "\n",
    "# Neue Figure für Legende\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(4, 4))  # Etwas größer für den Rand\n",
    "\n",
    "# Dummy-Patches für Legende\n",
    "legenden_patches = [Patch(facecolor='white', hatch=h, edgecolor='black', label=label) \n",
    "                    for h, label in zip(schraffuren, methoden)]\n",
    "\n",
    "# Legende zentriert im Plot platzieren\n",
    "ax_legend.legend(handles=legenden_patches, \n",
    "                 loc='center', \n",
    "                 fontsize=18, \n",
    "                 frameon=False)\n",
    "\n",
    "# Achsen ausblenden\n",
    "ax_legend.axis('off')\n",
    "\n",
    "# 0.5 cm (~0.2 inch) Rand um die Figure herum\n",
    "plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)\n",
    "\n",
    "# Speichern\n",
    "print(f\"{speicherort_diagramme}BoxPlot_Legende_Mitte.pdf\")\n",
    "fig_legend.savefig(f\"{speicherort_diagramme}BoxPlot_Legende_Mitte.pdf\", bbox_inches='tight', pad_inches=0.5/2.54)\n",
    "fig_legend.savefig(f\"{speicherort_diagramme}BoxPlot_Legende_Mitte.svg\", bbox_inches='tight', pad_inches=0.5/2.54)\n",
    "\n",
    "#plt.close(fig_legend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Realdten\n",
    "werte_cnn={\"Verstellweg_Phi_mean_absolute_error\":{\n",
    "        \"0\":1.9067816734,\n",
    "        \"1\":2.1337795258,\n",
    "        \"2\":1.9199428558,\n",
    "        \"3\":1.9067816734,\n",
    "        \"4\":3.005964756,\n",
    "        \"5\":1.9067816734,\n",
    "        \"6\":2.2692170143\n",
    "    },\"Verstellweg_X_mean_absolute_error\":{\n",
    "        \"0\":0.209063977,\n",
    "        \"1\":0.171160996,\n",
    "        \"2\":0.1484249532,\n",
    "        \"3\":0.209063977,\n",
    "        \"4\":0.0717340857,\n",
    "        \"5\":0.209063977,\n",
    "        \"6\":0.3262635767\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\":{\n",
    "        \"0\":1.9697440863,\n",
    "        \"1\":1.351219058,\n",
    "        \"2\":1.1623004675,\n",
    "        \"3\":1.9697440863,\n",
    "        \"4\":1.1553333998,\n",
    "        \"5\":1.9697440863,\n",
    "        \"6\":1.4088890553\n",
    "    }}\n",
    "#Daten Tradaboost 20%\n",
    "tradaBoostR2={\"Verstellweg_X_mean_absolute_error\": {\n",
    "        \"0\": 0.12250337163710164,\n",
    "        \"1\": 0.06145791206108678,\n",
    "        \"2\": 0.15484200206590248,\n",
    "        \"3\": 0.0786290246397835,\n",
    "        \"4\": 0.06858963713414828\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\": {\n",
    "        \"0\": 0.8795664960824864,\n",
    "        \"1\": 1.6466492937146229,\n",
    "        \"2\": 1.2280335819845427,\n",
    "        \"3\": 1.2285310542835002,\n",
    "        \"4\": 1.0620621863674233\n",
    "    },\n",
    "    \"Verstellweg_Phi_mean_absolute_error\": {\n",
    "        \"0\": 1.6013948748018372,\n",
    "        \"1\": 1.5687989227375518,\n",
    "        \"2\": 1.7783990885369756,\n",
    "        \"3\": 1.76735358534929,\n",
    "        \"4\": 1.735422802396724\n",
    "    }\n",
    "}\n",
    "#Werte von Coral\n",
    "tradaBoostR2_CORAL={\n",
    "    \"Verstellweg_X_mean_absolute_error\": {\n",
    "        \"0\": 0.08916666363060587,\n",
    "        \"1\": 0.01441970658478481,\n",
    "        \"2\": 0.2845787332813527,\n",
    "        \"3\": 0.058659423202674664,\n",
    "        \"4\": 0.025058066611856136\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\": {\n",
    "        \"0\": 1.2076558100259316,\n",
    "        \"1\": 1.152666684863061,\n",
    "        \"2\": 1.191035593494967,\n",
    "        \"3\": 1.1321671400178104,\n",
    "        \"4\": 1.0151379487677\n",
    "    },\n",
    "    \"Verstellweg_Phi_mean_absolute_error\": {\n",
    "        \"0\": 2.741374029036908,\n",
    "        \"1\": 2.361319938769061,\n",
    "        \"2\": 1.9479067650002413,\n",
    "        \"3\": 2.0183503363440707,\n",
    "        \"4\": 1.6194910031975571\n",
    "    }\n",
    "}\n",
    "#Daten zu Sorft Start, 20% Realdaten\n",
    "soft_start_daten={\n",
    "        \"Verstellweg_Phi_mean_absolute_error\":{\n",
    "        \"0\":1.8479813337,\n",
    "        \"1\":1.9301689863,\n",
    "        \"2\":1.4882190228,\n",
    "        \"3\":1.4696961641,\n",
    "        \"4\":1.4983427525\n",
    "    },\n",
    "    \"Verstellweg_X_mean_absolute_error\":{\n",
    "        \"0\":0.063426502,\n",
    "        \"1\":0.0344519727,\n",
    "        \"2\":0.1150183976,\n",
    "        \"3\":0.1496070921,\n",
    "        \"4\":0.0980957523\n",
    "    },\n",
    "    \"Verstellweg_Y_mean_absolute_error\":{\n",
    "        \"0\":1.1159968376,\n",
    "        \"1\":1.424552083,\n",
    "        \"2\":0.8680146337,\n",
    "        \"3\":1.6187688112,\n",
    "        \"4\":1.1865266562\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_values(data, key):\n",
    "    return list(data[key].values())\n",
    "\n",
    "# Verstellwege und Labels\n",
    "verstellwege = [\n",
    "    (\"Verstellweg_X_mean_absolute_error\", \"Verstellweg X\"),\n",
    "    (\"Verstellweg_Y_mean_absolute_error\", \"Verstellweg Y\"),\n",
    "    (\"Verstellweg_Phi_mean_absolute_error\", \"Verstellweg Phi\")\n",
    "]\n",
    "from matplotlib.patches import Patch\n",
    "# Plot Setup\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "#fig.suptitle(\"MAE der Verstellwege bei 20% der Realdaten\", fontsize=20)\n",
    "\n",
    "titel_formeln = [r\"$x$\", r\"$y$\", r\"$\\varphi$\"]\n",
    "# Farbliche Gestaltung\n",
    "farben=[\"green\",\"red\",\"orange\",\"blue\"]# TODO hier ergänzen \n",
    "methoden = [\"Base-Model\", \"TrAdaBoostR2\", \"TrAdaBoostR2-CORAL\", \"Fine-Tuning\"]\n",
    "# Für jeden Verstellweg die Boxplots einzeichnen\n",
    "for i,(ax, (verstellweg_key, verstellweg_label)) in enumerate(zip(axes, verstellwege)):\n",
    "    # Werte extrahieren\n",
    "    cnn_values = extract_values(werte_cnn, verstellweg_key)\n",
    "    trada_values = extract_values(tradaBoostR2, verstellweg_key)\n",
    "    coral_values = extract_values(tradaBoostR2_CORAL, verstellweg_key)\n",
    "    soft_start_values= extract_values(soft_start_daten, verstellweg_key)\n",
    "\n",
    "    # Boxplots zeichnen\n",
    "    bp = ax.boxplot([cnn_values, trada_values, coral_values,soft_start_values],\n",
    "                    labels=[\"CNN\", \"TrAdaBoostR2\", \"TrAdaBoostR2_CORAL\",\"Softstart\"],\n",
    "                    patch_artist=True,\n",
    "                    widths=0.6)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_min = min(min(cnn_values), min(trada_values), min(coral_values), min(soft_start_values))\n",
    "    ax.set_ylim(bottom=y_min - 0.05)  # Optional: Ein kleiner Offset nach unten, um Platz zu schaffen\n",
    "\n",
    "\n",
    "    # Farben zuweisen\n",
    "    for patch, color in zip(bp['boxes'], farben):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    # Mediane hervorheben\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='black', linewidth=2)\n",
    "\n",
    "    if i < 2:\n",
    "        ax.set_ylabel(\"MAE [mm]\",fontsize=14)\n",
    "    else:\n",
    "        ax.set_ylabel(\"MAE [°]\",fontsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=0)\n",
    "    \n",
    "    ax.set_title(f\"Verstellweg $\\Delta${titel_formeln[i]}\", fontsize=18)\n",
    "    #ax.set_ylabel(\"Mean Absolute Error\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    #ax.set_ylim(bottom=0)  # Damit die Boxen immer bei 0 starten\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.9])\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"{speicherort_diagramme}BoxPlot_3Modelle_bunt.pdf\")   # als PDF\n",
    "fig.savefig(f\"{speicherort_diagramme}BoxPlot_3Modelle_bunt.svg\")   # als SVG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (dateipfad, prozent) in enumerate(zip(\n",
    "        [dateipfad_training_CORAL_03],\n",
    "        [20])):\n",
    "    pass\n",
    "    mae_werte_X, mae_werte_Y, mae_werte_Phi = auswertung_mae_kombiniert(ober_ordner=dateipfad, unterordner_list=[])\n",
    "    speichere_mae_ergebnisse(mae_werte_X=mae_werte_X,mae_werte_Y=mae_werte_Y,mae_werte_Phi=mae_werte_Phi,ausgangsordner=dateipfad,\n",
    "                             ziel_ordner=\"build\\\\results-tradaBoostR2\\\\CORAL_03\",prozent=prozent)\n",
    "\n",
    "#ergebnisse_training_CORAL_03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
