{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from adapt.instance_based import TrAdaBoostR2\n",
    "import json\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from src.model import CNN\n",
    "from src.util import timestamp\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domainadaptation mit TradaBoostR2 aus der Adapt Libary\n",
    "Erste Version, wenns läuft dann später in den Klassen die einzelnen Methoden ergänzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Trainingsdaten erzeugen**\n",
    "Da die Zuordnung der Datenmengen über die Anzahl der Versuche läuft, wird hier nur der Belchsplit durchgeführt, es wird eine bestimmte Anzahl an Blechen gewählt. \n",
    "\n",
    "- Trainingsdaten aus den Simulationsdaten werden wie gehabt erzeugt\n",
    "- Target Domain Daten werden aus den Raldaten erzeugt.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.execution import WindowSplittingExecution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulationsdaten: Blechsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA: str = \"assets/synthetic-data.csv\"\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.5\n",
    "BATCH_SPLIT: bool = False\n",
    "BATCHSIZE: int = 326 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \".\" # Decimal separator of the csv file\n",
    "\n",
    "\n",
    "WindowSplittingExecution.execute(DATA, \n",
    "                                 BATCH_SPLIT, \n",
    "                                 VALIDATION_SPLIT, \n",
    "                                 TEST_SIZE, \n",
    "                                 SEED, \n",
    "                                 BATCHSIZE, \n",
    "                                 INTERPOLATION, \n",
    "                                 WINDOWSIZE, \n",
    "                                 SEP, \n",
    "                                 DECIMAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realdaten Blechsplit (`Train_Test_Split = 2`):\n",
    "- Die Daten trennen in A % Trainingsdaten für den TradaBoostR2 und B % Testdaten\n",
    "--> Solche Auftielung mti den Vorhandneen Methoden leichter umzusetzen als über exakte Anzahl der Bleche\n",
    "- Die Domain Adaptation wird auf dem A % Testdaten durchgeführt\n",
    "- Testen erfolgt auf dem Testdatensatz\n",
    "- Die Größenverhältnisse der Datensätze werden über die Variable  `size` eingestellt\n",
    "- `size = 0.9` so werden 10 % der Bleche in das TradaBoost DaomainAdaptaion gegeben, die restlichen 90 % der Daten werden zum Kontrollieren verwndet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA: str = \"assets/real-data.csv\"\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.9\n",
    "BATCH_SPLIT: bool = False\n",
    "BATCHSIZE: int = 1800 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \",\" # Decimal separator of the csv file\n",
    "\n",
    "\n",
    "WindowSplittingExecution.execute(DATA, \n",
    "                                 BATCH_SPLIT, \n",
    "                                 VALIDATION_SPLIT, \n",
    "                                 TEST_SIZE, \n",
    "                                 SEED, \n",
    "                                 BATCHSIZE, \n",
    "                                 INTERPOLATION, \n",
    "                                 WINDOWSIZE, \n",
    "                                 SEP, \n",
    "                                 DECIMAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kontrolle der Verteilungen**\n",
    "Verglichen werden die Verteilungen der Inputs und Outputs vor dem Skalierung und danach\n",
    "Problem: Die Daten der Targetdomain und der Sourcdomain werdne mit unterschiedlichen Scalern skaliert, die skalierten Daten leigen immer übereinander, da mit min max scaliert wird "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dateipfad_femDaten=\"build\\\\window_split\\\\synthetic-data\\\\1742837410\"\n",
    "dateipfad_realDaten=\"build\\\\window_split\\\\real-data\\\\1742838313\"\n",
    "\n",
    "def plotAllHistOfFIle(file_name):\n",
    "    # Lade die Daten aus beiden Ordnern\n",
    "    fem_path = os.path.join(dateipfad_femDaten, file_name)\n",
    "    real_path = os.path.join(dateipfad_realDaten, file_name)\n",
    "\n",
    "    # Daten als numpy-Array laden\n",
    "    fem_data = np.load(fem_path)\n",
    "    real_data = np.load(real_path)\n",
    "    fem_data = fem_data.reshape(-1, fem_data.shape[2])  # Shape: (287560, 11)\n",
    "    real_data = real_data.reshape(-1, real_data.shape[2])  # Shape: (254180, 11)\n",
    "    # Anzahl der Features (Spalten)\n",
    "    num_features = fem_data.shape[1]\n",
    "    print(fem_data.shape)\n",
    "    print(real_data.shape)\n",
    "    bins = 50  # Anzahl der Bins für die Histogramme\n",
    "\n",
    "\n",
    "    # Anzahl der Spalten pro Zeile\n",
    "    cols = 4\n",
    "    rows = (num_features + cols - 1) // cols  # Rundet auf, falls nicht durch 4 teilbar\n",
    "\n",
    "    # Plots erstellen\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "    axes = axes.flatten()  # 2D Array in 1D umwandeln für einfachere Iteration\n",
    "\n",
    "    for i in range(num_features):\n",
    "        ax = axes[i]\n",
    "        sns.histplot(fem_data[:, i], bins=bins, kde=True, ax=axes[i], color=\"blue\", label=\"Simulation\", stat=\"density\", alpha=0.6)\n",
    "        sns.histplot(real_data[:, i], bins=bins, kde=True, ax=axes[i], color=\"orange\", label=\"Realdaten\", stat=\"density\", alpha=0.6)\n",
    "        ax.set_title(f\"Feature {i+1}\")\n",
    "        ax.set_xlabel(\"Wert\")\n",
    "        ax.set_ylabel(\"Dichte\")\n",
    "        ax.legend()\n",
    "\n",
    "    # Leere Subplots deaktivieren, falls es weniger als 4*n Features gibt\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Datei laden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAllHistOfFIle(\"x-train-scaled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAllHistOfFIle(\"y-train-scaled.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TradaBoostR2**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domainAdapt_tradaBoostR2_saving(model_file, save_folder, save_filename, learning_rate,X_source_scaled,y_source_scaled,X_target_scaled,y_target_scaled):\n",
    "    print(\"Modell laden\")\n",
    "    cnn = CNN.from_file(model_file)\n",
    "    base_output = cnn.model.get_layer(\"dense_4\").output  # Falls 'dense_4' die letzte gemeinsame Schicht ist\n",
    "    new_output = Dense(3, activation=\"linear\", name=\"output\")(base_output)\n",
    "    cnn.model = Model(inputs=cnn.model.input, outputs=new_output)\n",
    "\n",
    "    cnn.model.summary()\n",
    "\n",
    "    #Basis Modell erzeugen\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = ['mean_absolute_error', 'mean_absolute_error', 'mean_absolute_error']\n",
    "    metrics={'Verstellweg_X': 'mae', 'Verstellweg_Y': 'mae', 'Verstellweg_Phi': 'mae'}\n",
    "    loss_one_output=['mean_absolute_error']\n",
    "    metrics_one_output=['mae']\n",
    "    cnn.model.compile(optimizer=optimizer, loss=loss_one_output,metrics=metrics_one_output)\n",
    "    \n",
    "    \n",
    "    #Wrappen des MOdells vom Tensorflow in in Keras\n",
    "    cnn_wrapped = KerasRegressor(model=cnn.model, epochs=2, batch_size=32, verbose=1)#,target_type=\"continuous-multioutput\")#, multi_output=True)#,validate=False)\n",
    "    tradaboost_model = TrAdaBoostR2(\n",
    "        estimator=cnn_wrapped,  # CNN als Basis-Regressor\n",
    "        n_estimators=1  # Anzahl der Boosting-Iterationen\n",
    "    )\n",
    "    \n",
    "    y_source_scaled=np.squeeze(y_source_scaled)\n",
    "    y_target_scaled=np.squeeze(y_target_scaled)\n",
    "    print(\"\\n Training mit TrAdaBoostR2 gestartet \\n\")\n",
    "    print(f\"save_folder = {save_folder}\")\n",
    "    print(f\"\")\n",
    "    with tqdm(total=tradaboost_model.n_estimators, desc=\"TrAdaBoostR2 Fortschritt\") as pbar:\n",
    "        for i in range(tradaboost_model.n_estimators):\n",
    "            tradaboost_model.fit(X_source_scaled, y_source_scaled,\n",
    "                                    X_target_scaled, y_target_scaled\n",
    "                                    )\n",
    "                \n",
    "                # Speichern des Zwischenstands nach jeder Iteration\n",
    "            joblib.dump(tradaboost_model, os.path.join(save_folder, f\"tradaboost_model_iter_{i}\"))\n",
    "\n",
    "            # Optional: Wichtige Variablen separat speichern\n",
    "            checkpoint_data = {\n",
    "                \"iteration\": i + 1,\n",
    "                \"estimator_count\": len(tradaboost_model.estimators_),\n",
    "                \"weights\": tradaboost_model.estimator_weights_,\n",
    "                \"errors\": tradaboost_model.estimator_errors_,\n",
    "            }\n",
    "            joblib.dump(checkpoint_data, os.path.join(save_folder, f\"checkpoint_data_{i}.pkl\"))\n",
    "\n",
    "            pbar.update(1)\n",
    "    print(\"\\n Training mit TrAdaBoostR2 beendet \\n\")\n",
    "    \n",
    "    # Extrahiertes Modell nach dem Training speichern\n",
    "    trained_model = tradaboost_model.estimators_[-1]  # Letzter trainierter Estimator\n",
    "\n",
    "    # Modell speichern\n",
    "    trained_model.model_.save(os.path.join(save_folder, f\"{save_filename}.h5\"))\n",
    "        \n",
    "    metadata = {\n",
    "        \"model\": model_file,\n",
    "        \"data\": \"Einzeln übergeben weil TradaBoost!\",\n",
    "        \"optimizer\": {\n",
    "            \"name\": optimizer.name,\n",
    "            \"learning_rate\": learning_rate\n",
    "        },\n",
    "        \"loss\": loss,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    #TODO: Den Speicher Zeug noch hinzufügen um das Training zu dokumentieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapt.instance_based import TrAdaBoostR2\n",
    "import json\n",
    "from src.model import CNN\n",
    "from src.util import timestamp\n",
    "import keras\n",
    "from src.data import NPY\n",
    "\n",
    "dateipfad_femDaten=\"build\\\\window_split\\\\synthetic-data\\\\1742837410\\\\\"\n",
    "dateipfad_realDaten=\"build\\\\window_split\\\\real-data\\\\1742838313\\\\\"\n",
    "'''\n",
    "#Simulationsdaten      \n",
    "X_source_scaled=np.load(dateipfad_femDaten+\"x-train-scaled.npy\")\n",
    "y_source_scaled=np.load(dateipfad_femDaten+\"y-train-scaled.npy\")\n",
    "# Realdaten\n",
    "X_target_scaled=np.load(dateipfad_realDaten+\"x-train-scaled.npy\")\n",
    "y_target_scaled=np.load(dateipfad_realDaten+\"y-train-scaled.npy\")\n",
    "'''\n",
    "X_source_scaled = NPY.from_file(dateipfad_femDaten + \"x-train-scaled.npy\").array\n",
    "y_source_scaled = NPY.from_file(dateipfad_femDaten + \"y-train-scaled.npy\").array\n",
    "\n",
    "# Realdaten\n",
    "X_target_scaled = NPY.from_file(dateipfad_realDaten + \"x-train-scaled.npy\").array\n",
    "y_target_scaled = NPY.from_file(dateipfad_realDaten + \"y-train-scaled.npy\").array\n",
    "\n",
    "\n",
    "\n",
    "#Modelldaten\n",
    "model_file = \"assets\\\\tuned-synthetic-data-offset\\\\1742400473\\\\best-model.h5\"\n",
    "save_folder=\"build\\\\tradaboost_model\\\\\"\n",
    "save_filename=\"test_training\"\n",
    "\n",
    "# Daomain Adaptation Training mit TradaBoostR2\n",
    "domainAdapt_tradaBoostR2_saving(model_file=model_file,save_folder=save_folder,save_filename=save_filename,learning_rate=0.00011858185678151421,\n",
    "                         X_source_scaled=X_source_scaled,y_source_scaled=y_source_scaled,X_target_scaled=X_target_scaled,y_target_scaled=y_target_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping und Valiadation hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data muss mit erzeugt werden\n",
    "DATA: str = \"assets/synthetic-data.csv\"\n",
    "VALIDATION_SPLIT: bool = True # If the data should also be splitted into a validation set?\n",
    "TEST_SIZE: float = 0.5\n",
    "BATCH_SPLIT: bool = False\n",
    "BATCHSIZE: int = 326 # real-data: 1800, synthetic-data: 326\n",
    "SEED: int = 69 # Seed for random state -> Split with same seed and data will always result in the same split\n",
    "INTERPOLATION: bool = False\n",
    "WINDOWSIZE: int = 10\n",
    "SEP: str = \";\" # Separator of the csv file\n",
    "DECIMAL: str = \".\" # Decimal separator of the csv file\n",
    "\n",
    "\n",
    "WindowSplittingExecution.execute(DATA, \n",
    "                                 BATCH_SPLIT, \n",
    "                                 VALIDATION_SPLIT, \n",
    "                                 TEST_SIZE, \n",
    "                                 SEED, \n",
    "                                 BATCHSIZE, \n",
    "                                 INTERPOLATION, \n",
    "                                 WINDOWSIZE, \n",
    "                                 SEP, \n",
    "                                 DECIMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def domainAdapt_tradaBoostR2_earlystopping(model_file, save_folder, save_filename, learning_rate,X_source_scaled,y_source_scaled,X_target_scaled,y_target_scaled):\n",
    "    print(\"Modell laden\")\n",
    "    cnn = CNN.from_file(model_file)\n",
    "    base_output = cnn.model.get_layer(\"dense_4\").output  # Falls 'dense_4' die letzte gemeinsame Schicht ist\n",
    "    new_output = Dense(3, activation=\"linear\", name=\"output\")(base_output)\n",
    "    cnn.model = Model(inputs=cnn.model.input, outputs=new_output)\n",
    "\n",
    "    cnn.model.summary()\n",
    "\n",
    "    #Basis Modell erzeugen\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = ['mean_absolute_error', 'mean_absolute_error', 'mean_absolute_error']\n",
    "    metrics={'Verstellweg_X': 'mae', 'Verstellweg_Y': 'mae', 'Verstellweg_Phi': 'mae'}\n",
    "    loss_one_output=['mean_absolute_error']\n",
    "    metrics_one_output=['mae']\n",
    "    cnn.model.compile(optimizer=optimizer, loss=loss_one_output,metrics=metrics_one_output)\n",
    "    \n",
    "    early_stopping =keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    \n",
    "    #Wrappen des MOdells vom Tensorflow in in Keras\n",
    "    cnn_wrapped = KerasRegressor(model=cnn.model, epochs=2, batch_size=32, verbose=1,callbacks=[early_stopping])#,target_type=\"continuous-multioutput\")#, multi_output=True)#,validate=False)\n",
    "    tradaboost_model = TrAdaBoostR2(\n",
    "        estimator=cnn_wrapped,  # CNN als Basis-Regressor\n",
    "        n_estimators=1  # Anzahl der Boosting-Iterationen\n",
    "    )\n",
    "    \n",
    "    y_source_scaled=np.squeeze(y_source_scaled)\n",
    "    y_target_scaled=np.squeeze(y_target_scaled)\n",
    "    print(\"\\n Training mit TrAdaBoostR2 gestartet \\n\")\n",
    "    print(f\"save_folder = {save_folder}\")\n",
    "    print(f\"\")\n",
    "    with tqdm(total=tradaboost_model.n_estimators, desc=\"TrAdaBoostR2 Fortschritt\") as pbar:\n",
    "        for i in range(tradaboost_model.n_estimators):\n",
    "            tradaboost_model.fit(X_source_scaled, y_source_scaled,\n",
    "                                 X_target_scaled, y_target_scaled,\n",
    "                                 eval_set)\n",
    "                \n",
    "                # Speichern des Zwischenstands nach jeder Iteration\n",
    "            joblib.dump(tradaboost_model, os.path.join(save_folder, f\"tradaboost_model_iter_{i}\"))\n",
    "\n",
    "            # Optional: Wichtige Variablen separat speichern\n",
    "            checkpoint_data = {\n",
    "                \"iteration\": i + 1,\n",
    "                \"estimator_count\": len(tradaboost_model.estimators_),\n",
    "                \"weights\": tradaboost_model.estimator_weights_,\n",
    "                \"errors\": tradaboost_model.estimator_errors_,\n",
    "            }\n",
    "            joblib.dump(checkpoint_data, os.path.join(save_folder, f\"checkpoint_data_{i}.pkl\"))\n",
    "\n",
    "            pbar.update(1)\n",
    "    print(\"\\n Training mit TrAdaBoostR2 beendet \\n\")\n",
    "    \n",
    "    # Extrahiertes Modell nach dem Training speichern\n",
    "    trained_model = tradaboost_model.estimators_[-1]  # Letzter trainierter Estimator\n",
    "\n",
    "    # Modell speichern\n",
    "    trained_model.model_.save(os.path.join(save_folder, f\"{save_filename}.h5\"))\n",
    "        \n",
    "\n",
    "    metadata = {\n",
    "        \"model\": model_file,\n",
    "        \"data\": \"Einzeln übergeben weil TradaBoost!\",\n",
    "        \"optimizer\": {\n",
    "            \"name\": optimizer.name,\n",
    "            \"learning_rate\": learning_rate\n",
    "        },\n",
    "        \"loss\": loss,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
